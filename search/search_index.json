{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Tufts Research Technology Bioinformatics","text":"<p>Research Technology Bioinformatics provides consultations to Tufts students, faculty and researchers.  In addition we maintain bioinformatics tools on the Tufts HPC Cluster and the Tufts Galaxy Platform.  We also lead in-class sessions, partner on grants, and develop workshops.</p>"},{"location":"#coming-soon","title":"Coming Soon","text":"<p>Omics Tutorials</p> <ul> <li>Metagenomics</li> <li>Proteomics</li> <li>Metabolomics</li> <li>GWAS</li> <li>DNA Methylation Analysis</li> <li>CHIP-seq Analysis</li> <li>Multi-omics</li> </ul> <p>Biostatistics:</p> <ul> <li>Two-Way ANOVA</li> <li>Bayes Rule</li> <li>Bayes Alternatives to Frequentist Statistics</li> </ul> <p>Machine Learning:</p> <ul> <li>Model Performance</li> <li>Regularization</li> <li>Decision Trees/Ensemble Methods</li> <li>Neural Networks</li> <li>Deep Learning</li> <li>Bayesian Modelling</li> <li>Bayesian Networks</li> </ul> <p>Acknowledgement</p> <p>Much of this content has been adapted from great tutorials originally created by:</p> <ul> <li>Jason Laird, Bioinformatics Scientist, TTS Research Technology, Tufts University</li> <li>Rebecca Batorsky, Data Scientist, DISC,  Tufts University</li> <li>Adelaide Rhodes, Senior Bioinformatics Scientist, TTS Research Technology, Tufts University</li> </ul> <p>We would also like to thank Kyle Monahan, Klara Chura, Christina Divoll, Kayla Sansevere, and Uku Uustalu for their review of this content!</p>"},{"location":"about/about/","title":"About Tufts Research Technology Bioinformatics","text":"<p>Research Technology Bioinformatics provides consultations to Tufts students, faculty and researchers.  In addition we maintain bioinformatics tools on the Tufts HPC Cluster and the Tufts Galaxy Platform.  We also lead in-class sessions, partner on grants, and develop workshops.</p>"},{"location":"about/about/#contributers","title":"Contributers","text":""},{"location":"about/about/#coming-soon","title":"Coming Soon","text":""},{"location":"about/about/#acknowledgemets","title":"Acknowledgemets","text":""},{"location":"basic/unix_r_python/","title":"Introduction","text":""},{"location":"basic/unix_r_python/#unix-tutorials","title":"Unix Tutorials","text":"<ul> <li>Intro To Unix</li> </ul>"},{"location":"basic/unix_r_python/#r-tutorials","title":"R Tutorials","text":"<ul> <li>Intro To R</li> </ul>"},{"location":"basic/unix_r_python/#python-tutorials","title":"Python Tutorials","text":"<ul> <li>Intro To Python</li> </ul>"},{"location":"basic/intro-to-python/functions-scope/","title":"Functions & Scope","text":""},{"location":"basic/intro-to-python/functions-scope/#functions","title":"Functions","text":"<p>So far we have used functions in base python or python modules. But what if we want to create our own? Well here is the general formula to do so:</p> <pre><code># load the module we need\nimport numpy as np \n\n# define a function to return geometric mean\ndef geometric_mean(values):\n    return np.exp(np.mean(np.log(values)))\n\n# call our function\ngeometric_mean(coverage)                 \n</code></pre> <pre><code>209.88855396892262\n</code></pre> <p>Here we see that we use <code>def</code> to define the function and <code>return</code> to specify what value you'd like to return. We then call our function and use our <code>coverage</code> variable as our set of values. The geometric mean of the set of values in <code>coverage</code> are then returned.</p>"},{"location":"basic/intro-to-python/functions-scope/#function-documentation","title":"Function Documentation","text":"<p>To clarify the purpose of your function you can add a multiline string to your function using three quotes <code>'''</code>:</p> <pre><code># define a function to return geometric mean\ndef geometric_mean(values):\n''' This function takes a list of\n    values and returns the geometric mean \n    of those values'''\n    return np.exp(np.mean(np.log(values)))\n</code></pre> <p>This multiline string is also accessible when we run our function through the <code>help</code> function:</p> <pre><code>help(geometric_mean)\n</code></pre> <pre><code>Help on function geometric_mean in module __main__:\n\ngeometric_mean(values)\n    This function takes a list of\n    values and returns the geometric mean \n    of those values\n</code></pre>"},{"location":"basic/intro-to-python/functions-scope/#variable-scope","title":"Variable Scope","text":"<p>Variable naming can be difficult and sometimes variable names might need to be reused. Normally, when we use a variable name over again, we change the value of that variable. However, if we assign the same variable in and outside a function the values do not get overwritten:</p> <pre><code>x = 45\n\ndef print_x():\n    x = 30\n    return x\n</code></pre> <pre><code>x\n</code></pre> <pre><code>45\n</code></pre> <pre><code>print_x()\n</code></pre> <pre><code>30\n</code></pre>"},{"location":"basic/intro-to-python/libraries-data-frames/","title":"Libraries","text":"<p>Libraries are collections of functions called modules that can be imported and used in your script. Let's use the <code>math</code> library to grab constants:</p> <pre><code>import math\nmath.e\n</code></pre> <pre><code>2.718281828459045\n</code></pre> <p>Now how about functions:</p> <pre><code>math.log2(25)\n</code></pre> <p><pre><code>4.643856189774724\n</code></pre> We want to point out that here we call the value <code>e</code> after <code>math</code>. This is called a member value. On the contrary we see that <code>.log2()</code> comes after <code>math</code>. The parentheses make this a method or function in a given package. </p> <p>Tip</p> <p>If you ever need assistance with a library, try using the <code>help()</code> function to grab more information (e.g. <code>help(math)</code>).</p>"},{"location":"basic/intro-to-python/libraries-data-frames/#importing-parts-of-libraries-using-aliases","title":"Importing Parts of Libraries &amp; Using Aliases","text":"<p>Sometimes you'll only need a few things from a library. To grab just those few things use the following approach:</p> <pre><code>from math import log2, e\nmath.log2(25)\n</code></pre> <pre><code>4.643856189774724\n</code></pre> <p>Now sometimes the name of a library is just too long to continuously type out. For this we can use an alias</p> <pre><code>from math import log2 as l2\nmath.l2(25)\n</code></pre> <pre><code>4.643856189774724\n</code></pre> <p>Here we abbreviate <code>log2</code> from the <code>math</code> package to <code>l2</code>.</p>"},{"location":"basic/intro-to-python/libraries-data-frames/#importing-and-inspecting-data-frames","title":"Importing and Inspecting Data Frames","text":"<p>In data analysis we often work with tabular data, or two dimensional data with columns and rows. Columns will typically contain the same type of data and rows will be one sample with different observations. We commonly read in tabular data using the <code>pandas</code> module:</p> <pre><code>import pandas as pd\nimport csv\ndf = pd.read_csv('/cluster/tufts/bio/tools/training/intro-to-r/metadata.tsv' , sep = '/t', engine = 'python')\nprint(df)\n</code></pre> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452\n</code></pre> <p>If we want to inspect this data frame we can use a few useful commands. To get a quick summary of the data frame we can use:</p> <pre><code>df.info() # reveals that we have 6 columns, 9 rows, uses 504.0+ bytes of memory, and has one integer column\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 9 entries, 1 to 9\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   SampleID                 9 non-null      object\n 1   AntibioticUsage          9 non-null      object\n 2   DaySinceExperimentStart  9 non-null      object\n 3   Genotype                 9 non-null      object\n 4   Description              9 non-null      object\n 5   OtuCount                 9 non-null      int64 \ndtypes: int64(1), object(5)\nmemory usage: 504.0+ bytes\n</code></pre> <p>To get column names:</p> <pre><code>df.columns\n</code></pre> <pre><code>Index(['SampleID', 'AntibioticUsage', 'DaySinceExperimentStart', 'Genotype',\n       'Description', 'OtuCount'],\n      dtype='object')\n</code></pre> <p>To get row names:</p> <pre><code>df.index\n</code></pre> <pre><code>Int64Index([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n</code></pre> <p>To transpose (flip the columns and rows) the data frame:</p> <pre><code>df.T\n</code></pre> <pre><code>                                                 1                          2  \nSampleID                                  WT.unt.1                   WT.unt.2   \nAntibioticUsage                               None                       None   \nDaySinceExperimentStart                       DAY0                       DAY0   \nGenotype                                        WT                         WT   \nDescription              16S_WT_unt_1_SRR2627457_1  16S_WT_unt_2_SRR2627461_1   \nOtuCount                                      1174                       1474   \n</code></pre> <p>If we wanted a numeric summary we can use:</p> <pre><code>df.describe()\n</code></pre> <pre><code>          OtuCount\ncount     9.000000\nmean    777.777778\nstd     600.526806\nmin     175.000000\n25%     279.000000\n50%     452.000000\n75%    1451.000000\nmax    1492.000000\n</code></pre> <p>Note</p> <p>the <code>.describe()</code> method will only summarizes numeric data. So here we only have one column of numeric data that get's summarized.</p>"},{"location":"basic/intro-to-python/libraries-data-frames/#data-manipulation","title":"Data Manipulation","text":"<p>Say you want to grab certain values in a data frame using the number location. So the value in the second row and third column:</p> <pre><code>df.iloc[2,3]\n</code></pre> <p><pre><code>'WT'\n</code></pre> Here we see that the formula to grab values is <code>[row, column]</code>. If we wanted to use row/column names to specify the value:</p> <pre><code>df.loc[1,\"OtuCount\"]\n</code></pre> <pre><code>1174\n</code></pre> <p>To grab all values in a row or column we use <code>:</code> to specify every value:</p> <pre><code>df.loc[:,\"OtuCount\"]\n</code></pre> <pre><code>1    1174\n2    1474\n3    1492\n4    1451\n5     314\n6     189\n7     279\n8     175\n9     452\nName: OtuCount, dtype: int64\n</code></pre> <p>We can also subset our data with a few operators:</p> Operator Description &gt; greater than &gt;= greater than or equal &lt; less than &lt;= less than or equal == equals != not equal &amp; and | or <p>Let's go through a few of these:</p> <pre><code>df[df[\"AntibioticUsage\"] == \"None\"]    # select samples with no antibiotic useage\n</code></pre> <pre><code>SampleID    AntibioticUsage DaySinceExperimentStart Genotype    Description OtuCount\n1   WT.unt.1    None    DAY0    WT  16S_WT_unt_1_SRR2627457_1   1174\n2   WT.unt.2    None    DAY0    WT  16S_WT_unt_2_SRR2627461_1   1474\n3   WT.unt.3    None    DAY0    WT  16S_WT_unt_3_SRR2627463_1   1492\n4   WT.unt.7    None    DAY0    WT  16S_WT_unt_7_SRR2627465_1   1451\n</code></pre> <pre><code>df[df[\"OtuCount\"] &gt; 400]   # select samples with an otu count over 400\n</code></pre> <pre><code>SampleID    AntibioticUsage DaySinceExperimentStart Genotype    Description OtuCount\n1   WT.unt.1    None    DAY0    WT  16S_WT_unt_1_SRR2627457_1   1174\n2   WT.unt.2    None    DAY0    WT  16S_WT_unt_2_SRR2627461_1   1474\n3   WT.unt.3    None    DAY0    WT  16S_WT_unt_3_SRR2627463_1   1492\n4   WT.unt.7    None    DAY0    WT  16S_WT_unt_7_SRR2627465_1   1451\n9   WT.day3.9   Streptomycin    DAY3    WT  16S_WT_day3_9_SRR2628504_1  452\n</code></pre>"},{"location":"basic/intro-to-python/lists/","title":"Lists","text":"<p>A data frame is not the only way to store data, we can also create lists of values which can be the same data type or different data types. Here is an example:</p> <pre><code>coverage = [200, 34, 900, 423, 98, 789]\n</code></pre>"},{"location":"basic/intro-to-python/lists/#grabbing-list-values","title":"Grabbing List Values","text":"<p>We can also grab these values by their index, which again are zero-indexed (meaning they start at zero). Here is an example of grabbing the 3rd item in the list:</p> <pre><code>coverage[2]\n</code></pre> <pre><code>900\n</code></pre>"},{"location":"basic/intro-to-python/lists/#addingdeleting-values","title":"Adding/Deleting Values","text":"<p>To add values we can use the <code>.append()</code> method to add items to the end of a list:</p> <pre><code>coverage.append(542)\ncoverage\n</code></pre> <pre><code>[200, 34, 300, 423, 98, 789, 542]\n</code></pre> <p>Additionally, we can also remove items from a list as well with the <code>del</code> statement:</p> <pre><code>del coverage[3]\ncoverage\n</code></pre> <pre><code>[200, 34, 300, 98, 789, 542]\n</code></pre>"},{"location":"basic/intro-to-python/loops-conditionals/","title":"Loops & Conditionals","text":""},{"location":"basic/intro-to-python/loops-conditionals/#loops","title":"Loops","text":"<p>Loops perform some operation on a value in a set of values. Let's go through an example using our <code>coverage</code> list from the previous note:</p> <pre><code>for i in coverage:\n    print(i)\n</code></pre> <pre><code>200\n34\n300\n98\n789\n542\n</code></pre> <p>Here we see that <code>i</code> is a substitute for some value in the sequence provided - in this case 200, <code>34, 300, 98, 789, 542</code>. </p>"},{"location":"basic/intro-to-python/loops-conditionals/#nested-loops","title":"Nested Loops","text":"<p>Loops can also nested where a loop is placed inside a loop:</p> <pre><code>for i in [1,2]:\n    for j in coverage:\n        print(j*2)\n</code></pre> <pre><code>200\n34\n300\n98\n789\n542\n400\n68\n600\n196\n1578\n1084\n</code></pre> <p>Here we move through the loop and for every value in the first list (<code>[1,2]</code>), Then for each pass of the first loop we move through values the second list (<code>[200, 34, 300, 98, 789, 542]</code>). Finally for each value <code>i</code> we then multiply by each value <code>j</code>. </p>"},{"location":"basic/intro-to-python/loops-conditionals/#pass-statement","title":"Pass Statement","text":"<p>If you want a placeholder for your loop, meaning no operation is performed, use the <code>pass</code> statement:</p> <pre><code>for i in coverage:\n    pass\n</code></pre>"},{"location":"basic/intro-to-python/loops-conditionals/#conditionals","title":"Conditionals","text":"<p>If we were interested in performing some operation on a value only if a condition is met, we can use an <code>if</code> statement:</p> <pre><code>for i in coverage:\n    if i &gt; 500:\n        print(i)\n    else:\n        pass\n</code></pre> <pre><code>789\n542\n</code></pre> <p>Here we use the comparison operators we mentioned in the Libraries &amp; Data Frames Topic Note to only print values in <code>coverage</code> if they are larger than <code>500</code>.</p>"},{"location":"basic/intro-to-python/loops-conditionals/#multiple-conditionals","title":"Multiple Conditionals","text":"<p>To perform operations based on multiple conditions you can add in <code>elif</code> statements:</p> <pre><code>for i in coverage:\n    if i &gt; 500:\n        print(i)\n    elif i &lt; 500:\n        print('This value is less than 500')\n</code></pre> <pre><code>This value is less than 500\nThis value is less than 500\nThis value is less than 500\nThis value is less than 500\n789\n542\n</code></pre>"},{"location":"basic/intro-to-python/plotting-plotly/","title":"Plotting with Plotly","text":""},{"location":"basic/intro-to-python/plotting-plotly/#plotting-with-plotly","title":"Plotting with Plotly","text":"<p>While there are other plotting libraries, we will focus on <code>plotly</code> for the following reasons:</p> <ul> <li>has the ability to zoom </li> <li>images can be downloaded as <code>png</code> files</li> <li>select features can highlight features of the plot</li> </ul>"},{"location":"basic/intro-to-python/plotting-plotly/#basic-plot","title":"Basic Plot","text":"<p>Let's make a scatterplot:</p> <pre><code>import plotly.express as px\nfig = px.scatter(df,                      # the data we are using\n                 x=\"Day\",                 # x axis data\n                 y=\"OtuCount\",            # y axis data\n                 color='Day',             # how to color our data\n                 template=\"simple_white\") # what theme we would like\nfig.show()\n</code></pre> <p></p>"},{"location":"basic/intro-to-python/plotting-plotly/#adding-a-trendline","title":"Adding A TrendLine","text":"<p>We can add a trend line as well:</p> <pre><code>import plotly.express as px\nfig = px.scatter(df,\n                 x=\"Day\",\n                 y=\"OtuCount\",\n                 color='Day',\n                 template=\"simple_white\",\n                 trendline=\"ols\")         # add in a trend line\nfig.show()\n</code></pre> <p></p>"},{"location":"basic/intro-to-python/plotting-plotly/#scaling","title":"Scaling","text":"<p>Now if one of your axes spans multiple magnitudes you can scale your data using the <code>log_x</code> or <code>log_y</code> arguements:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 trendline=\"ols\",\n                 log_y = True)             # scale y axis\nfig.show()\n</code></pre> <p></p>"},{"location":"basic/intro-to-python/plotting-plotly/#panels","title":"Panels","text":"<p>Sometimes it is useful to separate data by some variable and create panels. We can easily do this by specifying the <code>facet_row</code> or <code>facet_col</code> arguements - where plots are stacked one on top of the other or side-by-side, respectively:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 facet_col = \"DaySinceExperimentStart\") # split plots by variable\nfig.show()\n</code></pre> <p></p>"},{"location":"basic/intro-to-python/plotting-plotly/#modifying-text","title":"Modifying Text","text":"<p>To modify text we can use the <code>labels</code> and <code>title</code> option:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 labels={                        \n                     \"OtuCount\": \"OTU count\"     # add in a space and capitalize\n                 },\n                 title = \"Figure 1\")             # add in figure title\nfig.show()\n</code></pre> <p></p> <p>Tip</p> <p>For more plots and plot customization options, checkout the Plotly Graphing Library Page for more information</p>"},{"location":"basic/intro-to-python/python-ondemand/","title":"Introduction To Python OnDemand","text":""},{"location":"basic/intro-to-python/python-ondemand/#setup","title":"Setup","text":"<p>Before getting started you will need:</p> <ul> <li>Account on Tufts HPC</li> <li>VPN if accessing the HPC from off campus</li> </ul>"},{"location":"basic/intro-to-python/python-ondemand/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; JupyterLab</code> and you will see a form to fill out to request compute resources to use JupyterLab on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>32GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Default</code></li> <li><code>Load Supporting Modules</code>: <code>curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6</code></li> </ul> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To JupyterLab Server</code>, and you will notice a new window will pop up with JupyterLab. </p>"},{"location":"basic/intro-to-python/python-ondemand/#introduction-to-jupyterlab","title":"Introduction to JupyterLab","text":"<p>Jupyterlab is a web-based user interface to run Python code and is not a traditional Integrated Development Environment (IDE) where you create scripts via some text editor and then submit directly to command line. JupyterLab has several advantages, including being able to run code in chunks, annotating code with links, and displaying figures right next to code! For this reason, JupyterLab is a robust tool for script development/data analysis. When you open JupyterLab you will notice:</p> <ul> <li>Left Sidebar: containing your file browser, list of running kernels/terminals, table of contents, extension manager</li> <li>Main Work Area: containing options for file/window types to open (ipykernels, terminal environments, text files, markdown files, and python files)</li> </ul> <p></p> <p>We are going to start by opening up a <code>.ipynb</code> file by clicking <code>Notebook Python 3 (ipykernel)</code>. These are not python scripts, but notebook files that contain code but also text, links and images. These files can easily be converted to a python script (file ending in <code>.py</code>) by going to:</p> <ul> <li><code>File</code></li> <li><code>Download as</code></li> <li><code>Python (.py)</code></li> </ul> <p>For now let's work in the Jupyter notebook (<code>.ipynb</code> file)!</p>"},{"location":"basic/intro-to-python/python-ondemand/#code-vs-markdown","title":"Code Vs. Markdown","text":"<p>You will notice when you open up your notebook that you are working in blocks:</p> <p></p> <p>These blocks can either be:</p> <ul> <li>raw blocks: raw data that can be converted into HTML/Latex formats</li> <li>code blocks: python code that can be run in chunks</li> <li>markdown blocks: a plain text format that can render links, lists, and images like what you might find on a website</li> </ul> <p>Here we will focus on code blocks to run chunks of python code, and markdown blocks which can add in images, links, etc. to annotate our code.</p>"},{"location":"basic/intro-to-python/python-ondemand/#markdown-basics","title":"Markdown Basics","text":"<p>markdown code:</p> <p><pre><code>- list item 1\n- list item 2\n</code></pre> output: - list item 1 - list item 2</p> <p>markdown code:</p> <p><pre><code>1. numbered list item 1\n2. numbered list item 2\n</code></pre> output: 1. numbered list item 1 2. numbered list item 2</p> <p>markdown code:</p> <p><pre><code># Level 1 Heading\n## Level 2 Heading\n</code></pre> output:</p>"},{"location":"basic/intro-to-python/python-ondemand/#level-1-heading","title":"Level 1 Heading","text":""},{"location":"basic/intro-to-python/python-ondemand/#level-2-heading","title":"Level 2 Heading","text":"<p>markdown code:</p> <p><pre><code>[google link](https://www.google.com/)\n</code></pre> output: google link</p> <p>Now that we have a basic understanding of markdown, let's create some annotations. In your first code block change the type to markdown and enter:</p> <pre><code># Introduction to Python \n\nHere are a few helpful links to get started:\n\n- [Python Cheatsheet](https://www.pythoncheatsheet.org/cheatsheet/basics)\n- [JupyterLab Documentation](https://jupyterlab.readthedocs.io/en/stable/)\n</code></pre> <p>Now hit either the play button at the top of the screen or hit <code>Shift + Enter</code> to run the block:</p> <p></p>"},{"location":"basic/intro-to-python/variables-data-types/","title":"Variables & Data Types","text":""},{"location":"basic/intro-to-python/variables-data-types/#variables","title":"Variables","text":"<p>In Python, we store values using names called variables. We can assign a variable with an <code>=</code> sign:</p> <p><pre><code>max_coverage = 6000\nminCoverage = 35\nantibiotic = 'Streptomycin'\nantibiotic2 = 'Penicillin'\n</code></pre> You will note a few things about variables:</p> <ul> <li>can incorporate letters, digits and underscores</li> <li>cannot start with a digit</li> <li>these are case sensitive</li> </ul> <p>Variables, once we assign them to some value, can be passed into functions to accomplish certain tasks. Functions, generally speaking, take in some input and spit out some output. Let's use the simplist use case, the <code>print()</code> function:</p> <p><pre><code>print('The maximum coverage is ', max_coverage)\n</code></pre> <pre><code>The maximum coverage is  6000\n</code></pre></p> <p>Here the function <code>print()</code> took in two character values and printed a combined string of words.</p> <p>Note</p> <p>variables are available to use between blocks. However, the order in which you run blocks matters so make sure to run your code blocks in order!</p>"},{"location":"basic/intro-to-python/variables-data-types/#data-types","title":"Data Types","text":"<ul> <li><code>integer</code>: a positive/negative whole number (34, -675)</li> <li><code>float</code>: a floating point number (4.67, -2034.67)</li> <li><code>string</code>: a character string written with either single or double quotes ('Streptomycin', \"antibiotic\")</li> <li><code>bool</code>: a TRUE/FALSE value</li> </ul> <p>So you have a variable, how do you determine the type? Well we can use the <code>type()</code> function:</p> <pre><code>type(max_coverage)\n</code></pre> <pre><code>int\n</code></pre> <p>If you want to convert between data types you can specify with the following functions:</p> <ul> <li><code>int()</code>: to convert to an integer</li> <li><code>float()</code>: to convert to a floating point number</li> <li><code>str()</code>: to convert to a string</li> </ul>"},{"location":"basic/intro-to-python/variables-data-types/#calculations","title":"Calculations","text":"<p>You can use Python like a calculator using the following symbols:</p> Operator Name Example + Addition x + y - Subtraction x - y * Multiplication x * y / Division x / y % Modulus x % y ** Exponentiation x ** y // Floor division x // y <p>Let's try an few example:</p> <pre><code>35 / 7 - 5 + 4 * 4 + 2**2\n</code></pre> <pre><code>20.0\n</code></pre> <p>We note that Python calculations follow the order of operations when performing a calculation. We should also bring up two non-standard operations that you may or may not be familiar with: Modulus and Floor division. Modulus is the remainder after division so:</p> <pre><code>7 % 2\n</code></pre> <pre><code>1\n</code></pre> <p>Floor division is a division operation for which you round the result down to a whole number:</p> <pre><code>7 // 2\n</code></pre> <pre><code>3\n</code></pre>"},{"location":"basic/intro-to-python/variables-data-types/#strings-operators","title":"Strings &amp; Operators","text":"<p>You can use <code>+</code> and <code>*</code> with string data as well to add and multiply, take for instance:</p> <pre><code>antibiotic + antibiotic\n</code></pre> <pre><code>'StreptomycinStreptomycin'\n</code></pre> <pre><code>antibiotic * 4\n</code></pre> <pre><code>'StreptomycinStreptomycinStreptomycinStreptomycin'\n</code></pre>"},{"location":"basic/intro-to-python/variables-data-types/#indexing","title":"Indexing","text":"<p>Unlike the other data types, strings have lengths. We can use the <code>len()</code> function to  check how long  a string is:</p> <pre><code>print(antibiotic)\nlen(antibiotic)\n</code></pre> <pre><code>'Streptomycin'\n12\n</code></pre> <p>We can slice strings if needed to! However, the letters you are grabbing are zero-indexed meaning that the first letter is letter 0, the second letter is letter 1, and so on:</p> <pre><code>antibiotic[0]\n</code></pre> <pre><code>'S'\n</code></pre> <pre><code>antibiotic[1]\n</code></pre> <pre><code>'t'\n</code></pre> <p>We can grab more letters using the format <code>[start:stop]</code>:</p> <pre><code>antibiotic[1:5]\n</code></pre> <pre><code>'trep'\n</code></pre>"},{"location":"basic/intro-to-python/variables-data-types/#comments","title":"Comments","text":"<p>When assigning variables we can add descriptions to our code to give our code context. We do this by writing our description after a <code>#</code> symbol:</p> <pre><code># creating a variable for time of day\ntime_of_day = 'Morning'\n</code></pre> <p>Everything after the <code>#</code> is not processed as Python code even within a code block in a Jupyter notebook.</p>"},{"location":"basic/intro-to-r/basics/","title":"R Basics","text":""},{"location":"basic/intro-to-r/basics/#new-r-script","title":"New R script","text":"<p>Now we will create an R script. R commands can be entered into the console, but saving these commands in a script will allow us to rerun these commands at a later date. To create an R script we will need to either:</p> <ul> <li>Go to <code>File &gt; New File &gt; R script</code></li> <li>Click the <code>New File</code> icon and select R script</li> </ul> <p></p>"},{"location":"basic/intro-to-r/basics/#running-r-code","title":"Running R Code","text":"<p>When running R code you have a few options:</p> <p>Running One Line/Chunk:</p> <ul> <li> <p>Put your cursor at the beginning of the line of code and hit <code>Ctrl + Enter</code> on Windows or  \u2318 + <code>Enter</code> on MacOSX.</p> </li> <li> <p>Highlight the line/chunk of code and hit <code>Ctrl + Enter</code> or \u2318 + <code>Enter</code>.</p> </li> </ul> <p>Running The Entire Script:</p> <ul> <li>Clicking <code>Source</code> at the top of the script window.</li> </ul>"},{"location":"basic/intro-to-r/basics/#calculations","title":"Calculations","text":"<p>Let's try running some R code! R can be used to run all sorts of calculations just like a calculator:</p> <p></p> <p>You will notice that we ran this code in the script window but you can see the output in the console.  When we work with calculations it is useful to remember the order of operations - here are their equivalents in R:</p> <ul> <li>Parentheses: <code>(</code>, <code>)</code></li> <li>Exponents: <code>^</code> or <code>**</code></li> <li>Multiply: <code>*</code></li> <li>Divide: <code>/</code></li> <li>Add: <code>+</code></li> <li>Subtract: <code>-</code></li> </ul> <p>Let's look at some examples:</p> <pre><code>10 * 3^3\n</code></pre> <pre><code>[1] 270\n</code></pre> <p><pre><code>(400 / 10) * (4e2) # 4e2 is the same as 4^2\n</code></pre> <pre><code>[1] 16000\n</code></pre></p> <p>You'll notice that in the last equation we added words after a <code>#</code> and the equation still ran. This is what is known as a comment, where everything after the <code>#</code> is not registered as R code. Commenting is immensely valuable for giving your code context so that you and whoever else reads it knows the purpose of a given chunk of code.</p> <p>Additionally there are functions built in R to perform mathematical calculations:</p> <pre><code>abs(10) # absolute value\n</code></pre> <pre><code>[1] 10\n</code></pre> More Examples <pre><code>sqrt(25) # square root\n</code></pre> <pre><code>[1] 5\n</code></pre> <pre><code>log(10) # natural logarithm\n</code></pre> <pre><code>[1] 2.302585\n</code></pre> <pre><code>log10(10) # log base 10\n</code></pre> <pre><code>[1] 1\n</code></pre>"},{"location":"basic/intro-to-r/basics/#comparisons","title":"Comparisons","text":"<p>R can also be used to make comparisons. Here we note the operators used to do so:</p> <ul> <li>Equals: <code>==</code></li> <li>Does Not Equal: <code>!=</code></li> <li>Less Than Or Equal <code>&lt;=</code></li> <li>Greater Than Or Equal <code>&gt;=</code></li> <li>Greater Than <code>&gt;</code></li> <li>Less Than <code>&lt;</code></li> </ul> <pre><code>2 == 2\n</code></pre> <pre><code>[1] TRUE\n</code></pre> More Examples <pre><code>2 != 2\n</code></pre> <pre><code>[1] FALSE\n</code></pre> <pre><code>3 &lt;= 10\n</code></pre> <pre><code>[1] TRUE\n</code></pre> <p>Note</p> <p>Unless the number is an integer, do not use <code>==</code> to compare. This is due to the fact that the decimal value may appear the same  in R but from a machine level the two values can be very different.</p>"},{"location":"basic/intro-to-r/basics/#variables-vectors","title":"Variables &amp; Vectors","text":"<p>Dealing with values can be cumbersome. In R, values can be assigned to words using the <code>&lt;-</code> operator:</p> <pre><code>x &lt;- 35 # assigning a value of 35\nx\nx &lt;- 40 # changing value to 40\nx\n</code></pre> <p><pre><code>[1] 35\n</code></pre> <pre><code>[1] 40\n</code></pre> You'll notice that we initially assigned <code>x</code> to a value of <code>35</code> and then updated value to <code>40</code>. This is important to keep in mind because the last value assigned to <code>x</code> will be kept. Variables can I have a combination lowercase letters, uppercase letters, underscores and periods:</p> <p><pre><code>value &lt;- 40\nbiggerValue &lt;- 45\neven_bigger_value &lt;- 50\nbiggest.value &lt;- 55\n</code></pre> <pre><code>value\nbiggerValue\neven_bigger_value\nbiggest.value\n</code></pre></p> <pre><code>[1] 40\n[1] 45\n[1] 50\n[1] 55\n</code></pre> <p>Note</p> <p>Take note that the spelling needs to be consistent to call the variable correctly.</p> <p>We can also assign a series of values in a specific order to a variable to create what is called a vector:</p> <pre><code>someVector &lt;- 5:10\nsomeVector\n</code></pre> <pre><code>[1]  5  6  7  8  9 10\n</code></pre>"},{"location":"basic/intro-to-r/basics/#environment","title":"Environment","text":"<p>As you may have noticed we have been assigning variables and they have been added to your <code>Environment</code> window:</p> <p></p> <p>If you would like to declutter your environment, you have a few options:</p> <ul> <li>You can use the <code>rm()</code> function to remove which ever variables you'd like. To remove more than one just put a comma between variable names.</li> <li>You can clear all variables by clicking the broom icon:</li> </ul> <p></p> <p>Warning</p> <p>Be careful when removing variables, especially if these values took a long time to generate!</p>"},{"location":"basic/intro-to-r/basics/#r-packages","title":"R Packages","text":"<p>Aside from the base functions there are thousands of custom fuctions which are bundled in R packages. We can access these functions by loading the package that contains them. On the Tufts HPC, groups of packages are available. To access them you will need to specify the path where these packages are held. To identify the base group of packages, we can use the <code>libPaths()</code> function:</p> <pre><code>.libPaths()\n</code></pre> <pre><code>[1] \"/opt/shared/R/4.0.0/lib64/R/library\"\n</code></pre> <p>This is the base R library for OnDemand and it is rather limited. We will pull in a more complete library by pointing to it:</p> <pre><code>.libPaths(c('/cluster/tufts/hpc/tools/R/4.0.0'))\n.libPaths()\n</code></pre> <pre><code>[1] \"/cluster/tufts/hpc/tools/R/4.0.0\"    \"/opt/shared/R/4.0.0/lib64/R/library\"\n</code></pre> <p>Now you'll note we are first pointing to the <code>/cluster/tufts/hpc/tools/R/4.0.0</code> library first for packages! You'll can see what packagews are available in the<code>Packages</code> window:</p> <p></p> <p>To load a package you can use the <code>library()</code> function:</p> <pre><code>library(ggplot2)\n</code></pre> <p>Note</p> <p>If you need a package installed in this shared library, reach out to TTS Reasarch Technology, at tts-research@tufts.edu</p>"},{"location":"basic/intro-to-r/data-structures/","title":"Data Structures","text":""},{"location":"basic/intro-to-r/data-structures/#data-types","title":"Data Types","text":"<p>So far we have only dealt with numeric values. However, we are not limited to just numbers we can store:</p> <ul> <li><code>numeric</code> - numeric values that can contain whole numbers and decimals</li> <li><code>character</code> - text value that is made a text value by adding quotes. So for example <code>1 2 3</code> is a numeric data, but <code>\"1\" \"2\" \"3\"</code> is character data</li> <li><code>integer</code> - limited to just whole numbers, but will take up less memory than numeric data. We can specify an integer by adding <code>L</code> to a number (e.g. <code>1L</code> or <code>3L</code>)</li> <li><code>logical</code> - These are boolean values so <code>TRUE</code>/<code>T</code> or <code>FALSE</code>/<code>F</code>.</li> <li><code>complex</code> - complex number such as <code>1+6i</code></li> </ul>"},{"location":"basic/intro-to-r/data-structures/#data-structures_1","title":"Data Structures","text":"<p>So we have all this lovely data to play with and in R we typically organize in a few ways:</p>"},{"location":"basic/intro-to-r/data-structures/#vectors","title":"Vectors","text":"<p>Vectors are collections of data, like a:</p> <p>collection of numbers - <code>c(1,2,3)</code>   collection of characters -  <code>c(\"1\",\"2\",\"3\")</code>   collection of logical values - <code>c(TRUE,FALSE,TRUE)</code></p> <p>Note</p> <p>It should be noted that a vector needs to be a collection of the same type of data. You will also note that each list is separated by commas and surrounded by <code>c()</code>. This is necessary to create vectors so make sure to remember the <code>c()</code>!</p>"},{"location":"basic/intro-to-r/data-structures/#factors","title":"Factors","text":"<p>Factors can be used to store categorical data and can be created like this:</p> <pre><code>size &lt;- c(\"small\", \"medium\", \"small\", \"large\", \"medium\")\nsize\n</code></pre> <pre><code>[1] \"small\"  \"medium\" \"small\"  \"large\"  \"medium\"\n</code></pre> <pre><code>size &lt;- factor(size,levels = c(\"small\",\"medium\",\"large\"))\nsize\n</code></pre> <pre><code>[1] small  medium small  large  medium\n\nLevels: small medium large\n</code></pre> <p>Now we have turned this character vector into a factor vector! These will come in handy when we start breaking down data by category.</p>"},{"location":"basic/intro-to-r/data-structures/#matrices","title":"Matrices","text":"<p>A matrix can be created by combining vectors of the same length and same data type. They are used frequently when performing operations on numeric data but can include other data types. In R we can create a matrix with the <code>matrix()</code> function:</p> <p><pre><code>matrix(data=1:9,nrow = 3,ncol=3)\n</code></pre> <pre><code>     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n</code></pre></p> <p>Here we take a vector and specify how many columns and how many rows we'd like. </p>"},{"location":"basic/intro-to-r/data-structures/#data-frames","title":"Data Frames","text":"<p>Data frames are also collections of vectors of the same length. However, they do not need to be the same data type. Here we create a data.frame with the <code>data.frame()</code> function:</p> <p><pre><code>data.frame(\ncharacters=c(\"past\",\"present\",\"future\"),\nnumbers=c(1,2,3),\nlogical=c(TRUE,FALSE,TRUE),\ninteger=c(1L,2L,3L)\n)\n</code></pre> <pre><code>  characters numbers logical integer\n1       past       1    TRUE       1\n2    present       2   FALSE       2\n3     future       3    TRUE       3\n</code></pre></p>"},{"location":"basic/intro-to-r/data-structures/#lists","title":"Lists","text":"<p>Lists are collections of data that do not need to be the same type or length. We can create lists with the <code>list()</code> function:</p> <p><pre><code>list(\ndata.frame=data.frame(numbers=1:3,characters=c(\"past\",\"present\",\"future\")),\nnumbers=1:5,\ncharacters=c(\"past\",\"present\",\"future\")\n)\n</code></pre> <pre><code>$data.frame\n  numbers characters\n1       1       past\n2       2    present\n3       3     future\n\n$numbers\n[1] 1 2 3 4 5\n\n$characters\n[1] \"past\"    \"present\" \"future\" \n</code></pre></p>"},{"location":"basic/intro-to-r/functions-flow/","title":"Functions and Flow","text":""},{"location":"basic/intro-to-r/functions-flow/#functions","title":"Functions","text":"<p>Functions are operations we can perform on our various data structures to get some result. We typically like to make functions modular so they perform one specific task and not whole pipelines. Here is the general format for a function:</p> <pre><code>functionName &lt;- function(x){\nresult &lt;- operation(x)\nreturn(result)\n}\n</code></pre> <p>So here we see that we assign some operation to a name, here it we just call it <code>functionName</code>. Then the function takes an input, <code>x</code>. Inside the function our result is obtained by doing some operation on our input. Finally we then use <code>return()</code> to return that result. Let's try making a function that will square the input:</p> <pre><code>squareInput &lt;- function(x){\nresult &lt;- x * x\nreturn(result)\n}\n\nsquareInput(5)\n&gt; 25\n</code></pre> <p>Note</p> <p>Without <code>return()</code> in the function, R will return the last variable in the function. So you can leave out <code>return()</code>, however it is best to specify what you are returning for clarity.</p>"},{"location":"basic/intro-to-r/functions-flow/#additional-arguements","title":"Additional Arguements","text":"<p>Functions can have as little or as many arguements as needed. So in the example above we used one arguement, <code>x</code>. Let's try using more than one arguement:</p> <pre><code>item_in_vector_func &lt;- function(vector, item){\nitem_in_vector &lt;- item %in% vector\nreturn(item_in_vector)\n}\nitem_in_vector_func(vector = c(1,2,3), item=2)\n</code></pre> <pre><code>[1] TRUE\n</code></pre> <p>Here we used a new operator, <code>%in%</code>, which does exactly what it sounds like - it checks whether some value is in another set of values. We should also note, you can specify values in functions:</p> <pre><code>item_in_vector_func &lt;- function(vector=1:10, item=NULL){\nitem_in_vector &lt;- item %in% vector\nreturn(item_in_vector)\n}\n</code></pre> <p>Here we specify default values for the <code>item_in_vector()</code> function. Unless we change them when we call the function, these values will remain. So when you call a function from a package it's a good idea to check what the default values are.</p>"},{"location":"basic/intro-to-r/functions-flow/#control-flow","title":"Control Flow","text":"<p>Now what if we don't want to perform an operation until a condition is met? For this we need an if/else statement:</p> <pre><code>x &lt;- 3\n\nif (x == 10){\nprint(\"x equals 10\")\n} else{\nprint(\"x does not equal 10\")\n}\n</code></pre> <pre><code>[1] \"x does not equal 10\"\n</code></pre> <p>Now what if we wanted to include multiple conditions?</p> <pre><code>x &lt;- 3\n\nif (x == 10){\nprint(\"x equals 10\")\n} else if (x &gt; 2){\nprint(\"x is greater than 2\")\n} else if (x &gt; 1){\nprint(\"x is greater than 1\")\n} else{\nprint(\"x does not equal  10\")\n}\n</code></pre> <pre><code>[1] \"x is greater than 2\"\n</code></pre> <p>Here notice something, x meets 2 of the conditions:</p> <ul> <li><code>x &gt; 2</code></li> <li><code>x &gt; 1</code></li> </ul> <p>However, we note that the conditional statement is broken when <code>x</code> meets the first condition in the order above. </p>"},{"location":"basic/intro-to-r/functions-flow/#loops","title":"Loops","text":"<p>Operations can be repeated with a <code>for</code> loop:</p> <pre><code>for (i in 1:5){\nprint(i*i)\n}\n</code></pre> <p><pre><code>[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n</code></pre> Here we see that <code>i</code> is a substitute for some value in the sequence provided - in this case <code>1,2,3,4,5</code>. We can also nest a loop inside a loop like so:</p> <p><pre><code>for (i in 1:3){\nfor(j in 3:5){\nprint(i*j)\n}\n}\n</code></pre> <pre><code>[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 8\n[1] 10\n[1] 9\n[1] 12\n[1] 15\n</code></pre></p> <p>You'll notice that for each value i was multiplied by each value j. So:</p> <pre><code>1*3\n1*4\n1*5\n2*3\n2*5\n2*6\n3*3\n3*4\n3*5\n</code></pre>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/","title":"Inspecting/Manipulating Data","text":""},{"location":"basic/intro-to-r/inspecting-manipulating-data/#importing-data","title":"Importing Data","text":"<p>When importing data we use a few common functions:</p> <ul> <li><code>read.csv()</code> - to read in .csv files or files separated by commas</li> <li><code>read.table()</code> - to read files separated by delimiters other than commas - like spaces, tabs, semicolons, etc.</li> <li><code>openxlsx::read.xlsx()</code> - to read excel files</li> </ul> <p>You'll note that <code>read.xlsx()</code> has the prefix <code>openxlsx::</code>. This is because the <code>read. xlsx()</code> function is not avaiable with base R. To get this function you will need either need to:</p> <ul> <li>specify the package that the function comes from:</li> </ul> <pre><code>openxlsx::read.xlsx()\n</code></pre> <ul> <li>load the library with the package:</li> </ul> <pre><code>library(openxlsx)\nread.xlsx()\n</code></pre> <p>We will now practice inspecting data frames that we will copy over from a shared location. In the <code>Terminal</code> tab enter the following command:</p> <pre><code>cp /cluster/tufts/bio/tools/training/intro-to-r/data/* data/\n</code></pre>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#readcsv","title":"read.csv()","text":"<p>When importing <code>.csv</code> files you'll need to specify the path to where you're file is located. So if your <code>.csv</code> file is in <code>data/test.csv</code>, you can download it like so:</p> <pre><code>read.csv(\"data/test.csv\")\n</code></pre> <p>We can also extend this to URL's as well:</p> <pre><code>read.csv(url(\"https://zenodo.org/api/files/739025d8-5111-476a-9bb9-7f28a200ce8e/linked-ee-dataset-v20220524-QT_2022-07-13-sdev.csv\"))\n</code></pre>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#readtable","title":"read.table()","text":"<p>Like <code>read.csv()</code>, <code>read.table()</code> can also import data. The latter function is very useful in that it can download files not delimted (a.k.a separated) by commas. So to open a \".tsv\" file (a.k.a a file delimeted by a tab <code>\"\\t\"</code>):</p> <pre><code>meta &lt;- read.table(\"data/metadata.tsv\",sep=\"\\t\",stringsAsFactors=FALSE)\n</code></pre> <p>You'll notice in the code above that we include the option, <code>stringsAsFactors=FALSE</code>. If this was set to <code>TRUE</code> it would coerce your character columns into factor columns and this isn't always desired. So here we explicitly say <code>stringsAsFactors=FALSE</code> to be safe.</p>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#readxlsx","title":"read.xlsx()","text":"<p>While files like the ones mentioned above are popular, so are excel spreadsheets. So it is worth mentioning how to read in excel data as well:</p> <pre><code>library(openxlsx)      read.xlsx(\"data/test.xlsx\")\n</code></pre> <p>Now in excel spreadsheets you may only want to pull out one page or start from a row that isn't the first. To do so you can use:</p> <pre><code>library(openxlsx)\nread.xlsx(\"data/test.xlsx\",sheet=1,startRow = 1,colNames = TRUE,rowNames = FALSE)\n</code></pre> <p>So here we are pulling: the document \"/Documents/test.xlsx\", the second sheet, starting from the fifth row, specifying we do have column names, specifying we do not have row names. </p>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#inspecting-data","title":"Inspecting Data","text":"<p>You might have noticed that the only data frame we saved to a variable was the <code>metadata.tsv</code> file. We are going to now examine this file:</p> <p>To get a summary of each column:</p> <pre><code>summary(meta)\n</code></pre> <pre><code>   SampleID         AntibioticUsage    DaySinceExperimentStart   Genotype         Description           OtuCount     \n Length:9           Length:9           Length:9                Length:9           Length:9           Min.   : 175.0  \n Class :character   Class :character   Class :character        Class :character   Class :character   1st Qu.: 279.0  \n Mode  :character   Mode  :character   Mode  :character        Mode  :character   Mode  :character   Median : 452.0  \n                                                                                                     Mean   : 777.8  \n                                                                                                     3rd Qu.:1451.0  \n                                                                                                     Max.   :1492.0 \n</code></pre> <p>To get the data's class:</p> <pre><code>class(meta)\n</code></pre> <pre><code>[1] data.frame\n</code></pre> <p>To get a display of the data's contents:</p> <pre><code>str(meta)\n</code></pre> <pre><code>'data.frame':   9 obs. of  6 variables:\n $ SampleID               : chr  \"WT.unt.1\" \"WT.unt.2\" \"WT.unt.3\" \"WT.unt.7\" ...\n $ AntibioticUsage        : chr  \"None\" \"None\" \"None\" \"None\" ...\n $ DaySinceExperimentStart: chr  \"DAY0\" \"DAY0\" \"DAY0\" \"DAY0\" ...\n $ Genotype               : chr  \"WT\" \"WT\" \"WT\" \"WT\" ...\n $ Description            : chr  \"16S_WT_unt_1_SRR2627457_1\" \"16S_WT_unt_2_SRR2627461_1\" \"16S_WT_unt_3_SRR2627463_1\" \"16S_WT_unt_7_SRR2627465_1\" ...\n $ OtuCount               : int  1174 1474 1492 1451 314 189 279 175 452\n</code></pre> <p>To get the first 6 rows:</p> <pre><code>head(meta)\n</code></pre> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n</code></pre> <p>To get the last 6 rows:</p> <pre><code>tail(meta)\n</code></pre> <pre><code>SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452\n</code></pre> <p>To get the length of a vector:</p> <pre><code>length(meta$Genotype)\n</code></pre> <pre><code>[1] 9\n</code></pre> <p>To get the dimensions of a matrix/data frame:</p> <pre><code>dim(meta) # answer is given in number of rows, then number of columns\n</code></pre> <pre><code>[1] 9 6\n</code></pre> <p>To get the number of columns/rows:</p> <pre><code>ncol(meta)\n</code></pre> <pre><code>[1] 6\n</code></pre> <pre><code>nrow(meta)\n</code></pre> <pre><code>[1] 9\n</code></pre> <p>To get your column names:</p> <pre><code>colnames(meta)\n</code></pre> <pre><code>[1] \"SampleID\"                \"AntibioticUsage\"        \n[3] \"DaySinceExperimentStart\" \"Genotype\"               \n[5] \"Description\"             \"OtuCount\" \n</code></pre> <p>To get your row names:</p> <pre><code>rownames(meta)\n</code></pre> <pre><code>[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\"\n</code></pre> <p>Now that we know how to import our data and inspect it, we can go ahead and manipulate it!</p>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#manipulating-data","title":"Manipulating Data","text":"<p>So now that we have downloaded and inspected our data we can get to manipulating it! So to start, let's talk about accessing parts of your data. To grab the first column in a data frame/matrix you can do so like:</p> <pre><code>meta[,1]\n</code></pre> <pre><code>[1] \"WT.unt.1\"   \"WT.unt.2\"   \"WT.unt.3\"   \"WT.unt.7\"   \"WT.day3.11\"\n[6] \"WT.day3.13\" \"WT.day3.15\" \"WT.day3.14\" \"WT.day3.9\" \n</code></pre> <p>To grab the first row:</p> <pre><code>meta[1,]\n</code></pre> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype\n1 WT.unt.1            None                    DAY0       WT\n                Description OtuCount\n1 16S_WT_unt_1_SRR2627457_1     1174\n</code></pre> <p>Now if your data is a data frame you have a special way of accessing coluns with the <code>$</code> operator:</p> <pre><code>meta$AntibioticUsage\n</code></pre> <pre><code>[1] \"None\"         \"None\"         \"None\"         \"None\"         \"Streptomycin\"\n[6] \"Streptomycin\" \"Streptomycin\" \"Streptomycin\" \"Streptomycin\"\n</code></pre> <p>This comes in handy for readability. While you can grab your data by column number, it is much easier to read that you are grabbing Sepal Length. To grab mulitple columns/rows, you can do the following for both data frames and matrices:</p> <pre><code>meta[,c(2,4,6)] # grabbing the 2nd, 4th, and 6th columns\n</code></pre> <pre><code>  AntibioticUsage Genotype OtuCount\n1            None       WT     1174\n2            None       WT     1474\n3            None       WT     1492\n4            None       WT     1451\n5    Streptomycin       WT      314\n6    Streptomycin       WT      189\n7    Streptomycin       WT      279\n8    Streptomycin       WT      175\n9    Streptomycin       WT      452\n</code></pre> <p>In a data frame, to access columns you can be more specific and specify by column name:</p> <pre><code>meta[,c(\"SampleID\",\"Genotype\",\"OtuCount\")]\n</code></pre> <pre><code>    SampleID Genotype OtuCount\n1   WT.unt.1       WT     1174\n2   WT.unt.2       WT     1474\n3   WT.unt.3       WT     1492\n4   WT.unt.7       WT     1451\n5 WT.day3.11       WT      314\n6 WT.day3.13       WT      189\n7 WT.day3.15       WT      279\n8 WT.day3.14       WT      175\n9  WT.day3.9       WT      452\n</code></pre> <p>Now if we wanted to add a new column we could add one like so:</p> <pre><code>meta$Day &lt;- c(0,0,0,0,3,3,3,3,3) # name of new column comes after the $ sign\nmeta\n</code></pre> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174   0\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474   0\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492   0\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451   0\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452   3\n</code></pre>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#subsetting-data","title":"Subsetting Data","text":"<p>To subset our data we need to know a little bit about the different logical operators:</p> Operator Description &gt; greater than &gt;= greater than or equal &lt; less than &lt;= less than or equal == equals != not equal &amp; and | or <p>Let's go through a few of these!</p> <p>Subsetting so that we only have rows where the <code>OtuCount</code> is greater than 1000:</p> <pre><code>meta[meta$OtuCount &gt; 1000,]\n</code></pre> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype               Description OtuCount Day\n1 WT.unt.1            None                    DAY0       WT 16S_WT_unt_1_SRR2627457_1     1174   0\n2 WT.unt.2            None                    DAY0       WT 16S_WT_unt_2_SRR2627461_1     1474   0\n3 WT.unt.3            None                    DAY0       WT 16S_WT_unt_3_SRR2627463_1     1492   0\n4 WT.unt.7            None                    DAY0       WT 16S_WT_unt_7_SRR2627465_1     1451   0\n</code></pre> <p>Subsetting so that we only have rows where <code>OtuCount</code> is less than 400:</p> <pre><code>meta[meta$OtuCount &lt; 400,]\n</code></pre> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n</code></pre> <p>Subsetting so that we only have rows where the <code>AntibioticUsage</code> is equal to <code>Stretomycin</code>:</p> <pre><code> meta[meta$AntibioticUsage == \"Streptomycin\",]\n</code></pre> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452   3\n</code></pre> <p>Subsetting so that we only have rows where the <code>AntibioticUsage</code> is not equal to <code>Stretomycin</code>:</p> <pre><code>meta[meta$AntibioticUsage != \"Streptomycin\",]\n</code></pre> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype               Description OtuCount Day\n1 WT.unt.1            None                    DAY0       WT 16S_WT_unt_1_SRR2627457_1     1174   0\n2 WT.unt.2            None                    DAY0       WT 16S_WT_unt_2_SRR2627461_1     1474   0\n3 WT.unt.3            None                    DAY0       WT 16S_WT_unt_3_SRR2627463_1     1492   0\n4 WT.unt.7            None                    DAY0       WT 16S_WT_unt_7_SRR2627465_1     1451   0\n</code></pre> <p>Subsetting so that we only have rows where the <code>AntibioticUsage</code> equals <code>Steptomycin</code> or the <code>OtuCount</code> is less than <code>300</code>:</p> <pre><code>meta[meta$AntibioticUsage == \"Streptomycin\" | meta$OtuCount &lt; 300,]\n</code></pre> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452   3\n</code></pre>"},{"location":"basic/intro-to-r/inspecting-manipulating-data/#using-dplyr","title":"Using Dplyr","text":"<p>When subsetting data we should also mention the R package <code>dplyr</code>. This package has functionality to neatly modify data frames using the <code>%&gt;%</code> operator to separate your subsetting operations. Let's go through a quick example:</p> <pre><code>library(dplyr)\n\nmeta %&gt;%\nfilter(OtuCount &lt; 1400) %&gt;%    # filter rows with OtuCount less than 1400\nselect(c(SampleID,AntibioticUsage,Genotype,OtuCount)) %&gt;%   # Select certain rows\ngroup_by(AntibioticUsage) %&gt;%   # group data by some column\nmutate(HighOtuCount = OtuCount &gt; 1000)   # add a new column \n</code></pre> <pre><code># A tibble: 6 \u00d7 5\n# Groups:   AntibioticUsage [2]\n  SampleID   AntibioticUsage Genotype OtuCount HighOtuCount\n  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;       &lt;int&gt; &lt;lgl&gt;       \n1 WT.unt.1   None            WT           1174 TRUE        \n2 WT.day3.11 Streptomycin    WT            314 FALSE       \n3 WT.day3.13 Streptomycin    WT            189 FALSE       \n4 WT.day3.15 Streptomycin    WT            279 FALSE       \n5 WT.day3.14 Streptomycin    WT            175 FALSE       \n6 WT.day3.9  Streptomycin    WT            452 FALSE  \n</code></pre> <p>Tip</p> <p>For more dplyr data wrangling tips check out the Data Wrangling with dplyr and tidyr Cheat Sheet</p>"},{"location":"basic/intro-to-r/r-ondemand/","title":"Introduction To RStudio For Life Sciences","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN or the Tufts Secure Network</li> </ul>"},{"location":"basic/intro-to-r/r-ondemand/#learning-objectives","title":"Learning objectives","text":"<p>Today we are going to learn about</p> <ul> <li>project organization</li> <li>R packages and how to access them on the tufts HPC</li> <li>working with variables and data frames</li> <li>visualizing data</li> <li>and finally writing a markdown report of our findings</li> </ul>"},{"location":"basic/intro-to-r/r-ondemand/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>4GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshops</code>---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Launch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p> Are you connected to RStudio? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"basic/intro-to-r/r-ondemand/#introduction-to-rstudio","title":"Introduction To RStudio","text":"<p>RStudio is what is known as an Integrated Development Environment or IDE. Here you can write scripts, run R code, use R packages, view plots, and manage projects. This pane is broken up into three panels:</p> <ul> <li>The Interactive R console/Terminal (left)</li> <li>Environment/History/Connections (upper right)</li> <li>Files/Plots/Packages/Help/Viewer (lower right)</li> </ul> <p></p>"},{"location":"basic/intro-to-r/r-ondemand/#project-management","title":"Project Management","text":"<p>Before we dive into R it is worth taking a moment to talk about project management. Often times data analysis is incremental and files build up over time resulting in messy directories:</p> <p></p> <p>Sifting through a non-organized file system can make it difficult to find files, share data/scripts, and identify different versions of scripts. To remedy this, It is reccomended to work within an R Project. Before we make this project, we should make sure you are in your home directory. To do this click on the three dots in the files tab:</p> <p></p> <p>Then enter in a ~ symbol to go home!</p> <p></p>"},{"location":"basic/intro-to-r/r-ondemand/#r-project","title":"R Project","text":"<p>To Create a new R project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>R-Practice</code>)</li> <li><code>Create Project</code></li> </ol> <p>You will notice that your RStudio console switches to this project directory. When you log out of RStudio you can open this project again by clicking the <code>.Rproj</code> file in the project directory. </p> <p>Note</p> <p>The paths will be relative to this project directory as a safe guard against referencing data from outside sources. </p> Have you created the project? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"basic/intro-to-r/r-ondemand/#file-organization","title":"File Organization","text":"<ul> <li>You noticed now that you are inside your project folder</li> <li>Let's start by creating some folders to you organize our files</li> <li>In the files window click new folder and enter scripts</li> <li>Let's do this again to create a data folder and a results folder</li> </ul>"},{"location":"basic/intro-to-r/r-ondemand/#data-principles","title":"Data Principles","text":"<ul> <li>Treat data as read-only</li> <li>Store raw data separately from cleaned data if you do need to manipulate it</li> <li>Ensure scripts to clean data are kept in a separate <code>scripts</code> folder</li> <li>Treat reproducible results as disposable</li> </ul> <p>Tip</p> <p>Result files are good candidate files to cut if you are getting low on storage.</p>"},{"location":"basic/intro-to-r/r-ondemand/#getting-data","title":"Getting Data","text":"<ul> <li>Today we will be using a fake dataset assessing the taxa count on the mouse microbiome before and after antibiotic usage.</li> <li>To copy over this data we will use an R function called file.copy. </li> <li>A function takes some input and delivers an output. </li> <li>In this case we specify two inputs the location of our file and where we want to copy it to. </li> <li>The function's output is copying over this file. So let's try it copy over using the following commands:</li> </ul> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/data/meta.tsv\", to=\"./data/\")\nfile.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/data/meta2.tsv\", to=\"./data/\")\n</code></pre> <p>So here you'll note we copied over the file metadata.tsv to the data folder. Let's copy over our script:</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/scripts/intro-to-r.Rmd\", to=\"./scripts\")\n</code></pre> <p>Here we copy over our script intro-to-r to the scripts folder.</p>"},{"location":"basic/intro-to-r/r-ondemand/#opening-the-script","title":"Opening the Script","text":"<p>Now let's start by opening our script. Go to scripts and then double click on intro-to-r.Rmd!</p>"},{"location":"basic/intro-to-r/visualization/","title":"Visualization","text":""},{"location":"basic/intro-to-r/visualization/#plotting-with-ggplot2","title":"Plotting With ggplot2","text":"<p>While it is possible to use the base plotting system in R, we are going to focus on using the <code>ggplot2</code> library to create plots due to it's widespread use in scientific figure generation and the versitility of the package. The basic formula for creating a plot is as such:</p> <pre><code>library(ggplot2)\n\nggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +    # specify what data you are using and what your x and y columns are\ngeom_point()   # what type of plot do you want to make? here we make a scatterplot\n</code></pre> <p></p> <p>While we will go through a few plot types in this topic note, we reccomend you check out the R Graph Gallery for a complete list of possible plots and how to make them using the <code>ggplot2</code> library.</p>"},{"location":"basic/intro-to-r/visualization/#themes","title":"Themes","text":"<p>You are not just limited to a grey background theme when plotting with <code>ggplot2</code>. A poplular theme used in scientific figures is the dark-on-light theme:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +\ngeom_point()+\ntheme_bw()\n</code></pre> <p></p> <p>Tip</p> <p>For a complete list of themes, visit the Complete ggplot2 Themes page</p>"},{"location":"basic/intro-to-r/visualization/#scaling","title":"Scaling","text":"<p>Oftentimes your data will span mulitple magnitudes and this can result in an awkward distribution of data. We can scale either your x or y axes using a log scale to remedy this:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +\ngeom_point()+\ntheme_bw()+\nscale_y_log10()\n</code></pre> <p></p>"},{"location":"basic/intro-to-r/visualization/#relationships","title":"Relationships","text":"<p>When plotting two numeric data columns against one another, it might be useful to have a representation of their relationship. Here we show how to add a best fit line:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +\ngeom_point()+\ntheme_bw() +\nscale_y_log10() +\ngeom_smooth(method=\"lm\")\n</code></pre> <p></p>"},{"location":"basic/intro-to-r/visualization/#panels-and-colors","title":"Panels and Colors","text":"<p>Panels and colors are an important cue to highlight differences in your data:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount,color = AntibioticUsage)) +    # color by antibiotic usage\ngeom_point()+\nfacet_wrap(~AntibioticUsage)+    # create different panels for different types of antibiotic usage\ntheme_bw()                   </code></pre> <p></p>"},{"location":"basic/intro-to-r/visualization/#modifying-text","title":"Modifying Text","text":"<p>To modify your text style you can leverage the <code>theme()</code> function:</p> <pre><code>ggplot(data = meta, mapping = aes(x = AntibioticUsage,fill = AntibioticUsage)) +\ngeom_bar()+\ntheme_bw() +\ntheme(axis.text.x = element_text(angle = 45,hjust = 1)) # angle the text by 45 degrees and move the text down by 1 point\n</code></pre> <p></p> <p>You can also modify the x label, y label, title, and title of the legend:</p> <pre><code>ggplot(data = meta, mapping = aes(x = AntibioticUsage, y = OtuCount,fill= AntibioticUsage)) +\ngeom_boxplot()+\ntheme_bw() +\ntheme(axis.text.x = element_text(angle = 45,hjust = 1)) +\nlabs(\nx = \"Antibiotic Usage\",      # x axis title\ny = \"OTU Count\",             # y axis title\ntitle = \"Figure 1\",          # main title of figure\ncolor = \"Antibiotic Usage\"   # title of legend\n)\n</code></pre> <p></p>"},{"location":"basic/intro-to-unix/bash-parameters/","title":"Bash Parameters","text":""},{"location":"basic/intro-to-unix/bash-parameters/#making-files-and-directories","title":"Making files and directories","text":"<p>Many bash commands have special parameters, sometimes referred to as flags that open up a lot more possibilities.</p> <p>Let's start by going to your home directory (you choose the command)</p> <p>For new users, this may not return any content (besides <code>privatemodules</code> if you loaded <code>tmux</code> at the beginning.)</p> <p>Let's make a workshop directory and put a file into it.</p> <p>1.) Go to your home directory </p> <pre><code>cd\n</code></pre> <p><code>cd</code> not only changes directory, it allows you to go home by typing the command all by itself without a directory name.</p> <p>2.) Create a new directory for the workshop</p> <pre><code>mkdir Oct22Workshop\n</code></pre> <p>Note</p> <p><code>mkdir</code> is a specific command that allows you to make a directory. <code>rmdir</code> is a command that allows you to remove a directory (but only if it is empty)  When nameing files and directories, avoid spaces and special characters except underscores (\"_\") and hyphens (\"-\").</p> <p>Important</p> <p>Spelling and Capitalization are literal in unix. Be careful when making and using files to be consistent in your process.  This will make it easier to find files later.</p> <p>3.) Let's go into the directory using a very common command <code>cd</code> --&gt; <code>change directory</code></p> <pre><code>cd Oct22Workshop\n</code></pre> <p>4.) Make a new file that is empty</p> <pre><code>touch emptyfile.txt\n</code></pre> <p><code>touch</code> is a bash command that creates an empty file.</p> Why would you want an empty file? <p>Some programs require some pre-existing file names to be created.</p> <p>5.) Make a new file that contains \"Hello World\"</p> <pre><code>echo \"Hello World\" &gt; helloworld.txt\n</code></pre> <p><code>echo</code> is a command that prints the content to the terminal window (sometimes refered to as <code>print to screen</code></p> <p>The <code>&gt;</code> in this command tells the command to place the output into the place it is pointing. </p> <p>In this case, it creates the file <code>helloworld.txt</code> and puts the phrase <code>Hello World</code> into the file. </p> <p>6.) Print out the contents of the file to the terminal</p> <pre><code>cat helloworld.txt\n</code></pre> <p>You should see the output</p> <pre><code>Hello World\n</code></pre> <p>7.) Return to your home directory and run <code>ls</code></p> <pre><code>cd\n</code></pre> <pre><code>ls\n</code></pre> <p>Tip</p> <p>If you want to speed up the execution of commands, you can copy and paste multiple commands at the same time.</p> <pre><code>cd\nls\n</code></pre> <p>Question</p> <p>Please put a green checkmark in your box if you see the new directory when you type <code>ls</code> from your home directory).</p> <p>Be Careful with Redirect</p> <p>Be careful with redirect.</p> <p>When using <code>&gt;</code> to redirect content into a file, if the filename already exists, it will overwrite the file. This means that the original file is gone, and there is no undo in shell.</p> <p>If you want to add to a file (for example if you are running the same command on several files and extracting a piece of information that you want to put together at the end) you can use another form of redirect <code>&gt;&gt;</code>. Using the double redirect will add to the file instead of overwriting it.</p> <p>Which one is used depends on your process. If you are only running a command once, or have an intermediate file in a process that does not need to be retained at the end, then <code>&gt;</code> is okay to use.</p>"},{"location":"basic/intro-to-unix/bash-parameters/#setting-parameters-for-bash-commands","title":"Setting Parameters for Bash Commands","text":"<p>As you start using bash more and more, you will find a mix of files and directories/folders.  If we want to know which is which, we can add a <code>parameter</code> (sometimes referred to as a <code>flag</code>)</p> <p>This is an example of adding a <code>parameter</code> without an <code>argument</code>.</p> <pre><code>ls -F\n</code></pre>"},{"location":"basic/intro-to-unix/bash-parameters/#adding-arguments-to-bash-commands","title":"Adding Arguments to Bash Commands","text":"<p>An <code>argument</code> is a file name or other data that is provided to a command.</p> <pre><code>ls -F Oct22Workshop\n</code></pre> <p>It is possible to list the files and see their types inside a specific directory by adding the <code>argument</code> of the directory name to the <code>ls</code> command.</p> What do you see when you run the two commands above? <p>Anything with a \"/\" after it is a directory. Anything with a <code>*</code> after it are programs. (we will make a program later) If there's nothing there it's an otherwise unremarkable file (e.g. a data file or picture).</p> <p>Depending on which terminal you are using, some of the file types may have different colors. </p> <p>In our ondemand shell:</p> <p>Files are white Directories are blue Programs (also called <code>executables</code>) are green Compressed files are red (e.g. files that end in .zip or .gzip or .tar)</p>"},{"location":"basic/intro-to-unix/bash-parameters/#other-useful-parameters-for-ls","title":"Other Useful Parameters for <code>ls</code>","text":"<p>Show hidden files</p> <pre><code>ls -a\n</code></pre> <p>You should see a file called <code>.bashrc</code> here. This may be a file we need for troubleshooting your work or where you can make shortcuts or add paths to your login.</p> <p>Show the <code>long form</code> of the list command</p> <pre><code>ls -l\n</code></pre> <p>To see whether items in a directory are files or directories. <code>ls -l</code> gives a lot more information too, such as the size of the file.</p> <p>It also shows the permissions of who can read, write or execute a file.</p> <pre><code>drwxrwx--- 2 username05 username05     4096 Jul 18 09:57 JulyWorkshop\n</code></pre> <p>The first 10 letters in this line indicates the permission settings.</p> <p></p>"},{"location":"basic/intro-to-unix/bash-parameters/#getting-help-on-the-command-line","title":"Getting Help on the Command Line","text":"<p>There are an overwhelming number of possibilities with some of these shell commands, so knowing how to find help on demand is important.</p> <p>For example, <code>ls</code> has a lot of flags that can be used.</p> <pre><code>ls --help\n</code></pre> <p>This outputs a list of all the ways that <code>ls</code> can be altered to find information about your files.</p> <p>Parameters can be added together in some cases.</p> <pre><code>ls -ltr\n</code></pre> <p>This can replace <code>ls -l -t -r</code> <code>l</code> is for long form of the list (outputs the permission settings -- something we need to troubleshoot occasionally) <code>t</code> is to order the files chronologically <code>r</code> means to reverse the order of the files to put the newest file at the bottom</p> <p>This command strings together three flags.</p> <p><code>ls -l</code> is list with details <code>ls -t</code> is sort the list by creation time <code>ls -r</code> is sort the list in reverse</p> <p>For very full directories, this is helpful because it outputs the most recent set of files as the last in the list.</p> <p>Another way to get help is to use the <code>man</code> command. Not every unix installation has this installed, but the Tufts cluster does.</p> <p><code>man</code> is short for \"manual\"</p> <p>Navigating a <code>man</code> page</p> <p>Use the <code>spacebar</code> to scroll through the document. Use <code>q</code> to leave the manual and go back to the command line prompt.</p> <pre><code>man ls\n</code></pre> <p>This opens up the manual on the <code>ls</code> command. It spells out the meaning of all the parameters in detail.</p> <p>Most common bash commands have a <code>man</code> page that explains it (I wish they had this for emojis....).</p> <p>Many programs have a help function built in, try adding <code>--help</code> or <code>-h</code> to see if some helpful information pops up. Sometimes just running the command without any arguments or parameters leads to some usage information or describes the correct command to get help.</p> <p>For example, if I want to understand the command <code>tr</code> - which is used to change a word or character to a new value.</p> <p>Most programs recognize when you ask for an incorrect parameter, and will tell you how to get more information, as in this example.  To get help, type the command with the correct parameter.</p> <p>Tip</p> <p>For some programs, the <code>help</code> function may be <code>-h</code>, <code>--help</code> </p> <pre><code>tr -h\n</code></pre> <p>The shell outputs:</p> <pre><code>tr: invalid option -- 'h'\nTry 'tr --help' for more information\n</code></pre> <pre><code>tr --help\n</code></pre> <p>In this case, a <code>man</code> page does exist, so you can get even more direction by typing:</p> <pre><code>man tr\n</code></pre>"},{"location":"basic/intro-to-unix/blast-batch/","title":"BLAST Batch Script","text":""},{"location":"basic/intro-to-unix/blast-batch/#writing-a-bash-script-and-running-it-as-batch","title":"Writing a BASH Script and Running it as \"Batch\"","text":"<p>In this example, we'll repeat the blast command above but refine it by outputting a table which summarizes each blast hit on one line. </p>"},{"location":"basic/intro-to-unix/blast-batch/#return-to-the-workshop-directory","title":"Return to the Workshop Directory","text":"<p>cd ~/Oct22Workshop</p> <p>First, let's add more sequences to our query file. This will extract the first 186 sequences.  </p> <pre><code>head -n 999 mouse.1.protein.faa &gt; mm-second.faa\n</code></pre> <p>See this link for a description of the possible BLAST output formats.</p> <p>In order to do this, we need to open a text editor.</p>"},{"location":"basic/intro-to-unix/blast-batch/#opening-a-text-editor","title":"Opening a Text Editor","text":"<p>The easiest text editor to use on command line for beginners is <code>nano</code>, but there are many other types of command line text editors (<code>vi</code>,<code>emacs</code>,<code>vim</code>, etc.)</p> <p>Nano is nice because it puts the instructions at the bottom of the editor in case you forget.</p> <p>Open nano</p> <pre><code>nano\n</code></pre> <p>Control-X to exit, say no and no. Nothing is saved, because we did not type into the file.</p> <p>Let's reopen and copy and paste our script into the file.</p> <p>Sometimes it is good to give a file name, so let's nano with a filename for our script.</p> <pre><code>nano blast_sbatch.sh\n</code></pre> <p>Before closing, let's put some text into the file. </p> <p>Make sure to change the email address to your own email.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=blast\n#SBATCH --nodes=1\n#SBATCH -n 2\n#SBATCH --partition=batch\n#SBATCH --reservation=bioworkshop\n#SBATCH --mem=8Gb\n#SBATCH --time=0-24:00:00\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-user=youremail@tufts.edu\n\nmodule load blast-plus/2.11.0\nblastp -query mm-second.faa -db zebrafish.1.protein.faa -out mm-second.x.zebrafish.tsv -outfmt 6\n</code></pre> <p>Control -X to close and save and use the same file name (blast_sbatch.sh)</p> <p>Because it is going to one or several virtual locations in the cluster, we need to reload the module as part of the script before running the script. This will make the command recognizable to the machine where the job is running.</p> <pre><code>cat blast_sbatch.sh\n</code></pre> <p>Does it have all the elements?</p> <p>If it does, a simple way to run it is by telling shell that it is a program to run on SLURM.</p> <pre><code>sbatch blast_sbatch.sh\n</code></pre> <p>Check that the job is running</p> <pre><code>squeue -u $USER\n</code></pre> <p>Let's go ahead and run it from the workshop directory where you copied your data to.</p> <p>Because we did not add any ABSOLUTE paths, then the sbatch command will look for the files where the program is running.</p> <p>The results will also show up in that directory.</p> <p>You can look at the output file with <code>less -S</code>, the flag allows scrolling from left to right instead of wrapping text or cutting it off:</p> <pre><code>less -S mm-second.x.zebrafish.tsv\n</code></pre> <p>(and again, type 'q' to get out of paging mode)</p> <p>The command line may move stuff around slightly, but it is a tab delimited file that can be downloaded to your computer and loaded into your spreadsheet program of choice.</p> <p><code>blastp</code> is a versatile tool for finding similar sequences, to see all the options, type <code>blastp -help</code></p> <p>Tip</p> <p>If writing the script on your laptop before copying and pasting, make sure to use a compatible text editor.</p> <p>Even though you can't see it, popular word processors will add hidden symbols and change punctuation to your code.</p> <p>There are several free tools available to avoid these errors.</p> <ul> <li>Notepad+ is free to download and use.</li> <li>BBEdit has a free version.</li> </ul> <p>Other options are Sublime and PyCharm, which have some features to help edit files.</p>"},{"location":"basic/intro-to-unix/blast-batch/#resources-for-further-training-in-command-line","title":"Resources for Further Training in Command Line","text":"<ul> <li>Udemy (free to the Tufts community)</li> <li>Coursera</li> <li>LinkedIn Learning</li> </ul> <p>What are your favorites?</p>"},{"location":"basic/intro-to-unix/blast-example/","title":"Example with BLAST","text":""},{"location":"basic/intro-to-unix/blast-example/#return-to-the-workshop-directory","title":"Return to the workshop directory","text":"<pre><code>cd ~/Oct22Workshop\n</code></pre>"},{"location":"basic/intro-to-unix/blast-example/#loading-modules","title":"Loading Modules","text":"<p>Many common programs are pre-loaded into the Tufts HPC using a system called \"modules\".</p> <p>To see what versions of blast are available as a module, try running this command. </p> <p>Tip</p> <p>You can use the first part of the program name to check if there is a module.</p> <pre><code>module av blast\n</code></pre> <p>As of October 2022, these are the modules you might see displayed.</p> <p></p> <p>Choose the latest blast-plus version of the module and load it. </p> <pre><code>module load blast-plus/2.11.0\n</code></pre> <p>When there is only one version of a module, the full version does not need to be provided, but it is always best to inclue the version as we are loading and updating versions of programs all of the time.</p> <p>Confirm that the module is loaded.</p> <pre><code>module list\n</code></pre> <p>tmux and blast should be listed.</p> <p>If other programs are loaded with the module, they may also show up with this command.</p>"},{"location":"basic/intro-to-unix/blast-example/#bringing-in-files-from-the-internet","title":"Bringing in Files from the Internet","text":"<p>We need some data!  Let's grab the mouse and zebrafish RefSeq protein data sets from NCBI, and put them in our home directory. (this example is adapted from a lesson from Titus Brown's summer institute. These lessons contain a lot of command line examples.</p> <p>Note</p> <p><code>curl</code> and <code>wget</code> are the two most common tools used to bring in files that are available from a url. We are going to use <code>curl</code> because that command works well for files coming from an <code>ftp://</code> url.</p> Copying files over from NCBI <p>For genomics projects, the files are often stored in pubic repositories and we must go and get those files before proceeding. These files originally came from the   NCBI FTP site, a copy has been placed in our github directory for future reference.</p> <p>Now, we'll use <code>curl</code> to download the files from a Web site onto our computer. You will need to be connected to the internet for these commands to work.</p> <ul> <li><code>-o</code> indicates this is the name we are assigning to our files in our own directory</li> <li><code>-L</code> provides the full path for the download</li> </ul> <p>It is possible to copy and paste both conmands to your terminal, they will run in sequence if there is not an error.</p> <pre><code>curl -o mouse.1.protein.faa.gz -L https://tuftsdatalab.github.io/Research_Technology_Bioinformatics/workshops/hpcForLifeSciences_July2022/IntroToLinux/mouse.1.protein.faa.gz\n\ncurl -o zebrafish.1.protein.faa.gz -L https://tuftsdatalab.github.io/Research_Technology_Bioinformatics/workshops/hpcForLifeSciences_July2022/IntroToLinux/zebrafish.1.protein.faa.gz\n</code></pre> <p>Another method for pulling files from the internet is <code>wget</code>, which will be demoed tomorrow. <code>curl</code> can pull more file types than <code>wget</code>, but in this simple case, either can be used.</p> <p>If you look at the files in the current directory:</p> <pre><code>ls -l\n</code></pre> <p>You should now see these 3 files with details on who has permissions and when the files were created (notice that the dates are not today).</p> <pre><code>total 29908\n-rw-rw-r-- 1 username01 username01 12553742 Jun 29 08:41 mouse.1.protein.faa.gz\n-rw-rw-r-- 1 username01 username01 13963093 Jun 29 08:42 zebrafish.1.protein.faa.gz\n</code></pre> <p>The three files you just downloaded are the last three on the list - the <code>.faa.gz</code> files.</p> <p>All three of the files are FASTA protein files (that's what the .faa suggests) that are compressed with <code>gzip</code> (that's what the .gz means). Compressed files may have a different color when you use the <code>ls</code> command.</p> <p>Uncompress the files.</p> <pre><code>gunzip *.faa.gz\n</code></pre> <p>Because both files follow a very similar pattern, and we want to decompress all our .gz files, we can use the <code>*</code> wildcard (filenames that have a pattern that matches and number of missing letters before the part of the file name that is the same</p> <p>Regular Expressions</p> <p><code>*</code> and other wildcards are useful to save on typing scripts, because many actions can be combined in one request.</p> <p>Regular Expressions are a set of special characters combined with unix commands.</p> <p>Here is a link that explains the basic syntax){:target=\"_blank\" rel=\"noopener\"}. </p>"},{"location":"basic/intro-to-unix/blast-example/#checking-the-contents-of-a-file","title":"Checking the contents of a File","text":"<p>We've already used <code>cat</code> and <code>less</code> to look at the content of our helloworld.txt files. Some files are very large and we may only want to check the first few lines to reassure ourselves that the download worked correctly.</p> <p>Let's look at the first few sequences in the file:</p> <pre><code>head mouse.1.protein.faa \n</code></pre> <p>!!! note \"FASTA format</p> <pre><code>These are protein sequences in FASTA format.  FASTA format is something many of you have probably seen in one form or another -- it's pretty ubiquitous.  It's a text file, containing records; each record starts with a line beginning with a '&gt;', and then contains one or more lines of sequence text.\n</code></pre> <p>Let's take those first two sequences and save them to a file.  We'll do this using output redirection with '&gt;', which says \"take all the output and put it into this file here.\"</p> <pre><code>head -n 11 mouse.1.protein.faa &gt; mm-first.faa\n</code></pre> <p><code>-n</code> flag for <code>head</code> specifies a number of lines to pull.</p> <p>The first 11 lines contain two protein sequences. Let's extract those for blasting to test that our process is working.</p> <pre><code>cat mm-first.faa\n</code></pre> <p>Should produce:</p> <pre><code>&gt;YP_220550.1 NADH dehydrogenase subunit 1 (mitochondrion) [Mus musculus domesticus]\nMFFINILTLLVPILIAMAFLTLVERKILGYMQLRKGPNIVGPYGILQPFADAMKLFMKEPMRPLTTSMSLFIIAPTLSLT\nLALSLWVPLPMPHPLINLNLGILFILATSSLSVYSILWSGWASNSKYSLFGALRAVAQTISYEVTMAIILLSVLLMNGSY\nSLQTLITTQEHMWLLLPAWPMAMMWFISTLAETNRAPFDLTEGESELVSGFNVEYAAGPFALFFMAEYTNIILMNALTTI\nIFLGPLYYINLPELYSTNFMMEALLLSSTFLWIRASYPRFRYDQLMHLLWKNFLPLTLALCMWHISLPIFTAGVPPYM\n&gt;YP_220551.1 NADH dehydrogenase subunit 2 (mitochondrion) [Mus musculus domesticus]\nMNPITLAIIYFTIFLGPVITMSSTNLMLMWVGLEFSLLAIIPMLINKKNPRSTEAATKYFVTQATASMIILLAIVLNYKQ\nLGTWMFQQQTNGLILNMTLMALSMKLGLAPFHFWLPEVTQGIPLHMGLILLTWQKIAPLSILIQIYPLLNSTIILMLAIT\nSIFMGAWGGLNQTQMRKIMAYSSIAHMGWMLAILPYNPSLTLLNLMIYIILTAPMFMALMLNNSMTINSISLLWNKTPAM\nLTMISLMLLSLGGLPPLTGFLPKWIIITELMKNNCLIMATLMAMMALLNLFFYTRLIYSTSLTMFPTNNNSKMMTHQTKT\nKPNLMFSTLAIMSTMTLPLAPQLIT\n</code></pre> <p>Now let's BLAST these two sequences against the entire zebrafish protein data set. First, we need to tell BLAST that the zebrafish sequences are (a) a database, and (b) a protein database.  That's done by calling <code>makeblastdb</code></p> <pre><code>makeblastdb -in zebrafish.1.protein.faa -dbtype prot\n</code></pre> <p><code>makeblastdb</code> is a program that was loaded using the <code>module</code> command. If you unload the module, this command may not work.</p> <p>Next, we call BLAST to do the search:</p> <pre><code>blastp -query mm-first.faa -db zebrafish.1.protein.faa\n</code></pre> <p>This should run pretty quickly, but you're going to get a lot of output!! To save it to a file instead of watching it go past on the screen, ask BLAST to save the output to a file that we'll name <code>mm-first.x.zebrafish.txt</code>:</p> <pre><code>blastp -query mm-first.faa -db zebrafish.1.protein.faa -out mm-first.x.zebrafish.txt\n</code></pre> <p>and then you can 'page' through this file at your leisure by typing:</p> <pre><code>less mm-first.x.zebrafish.txt\n</code></pre> <p>(Type spacebar to move down, and 'q' to get out of paging mode.)</p> <p>What are your questions?</p> <p>Note</p> <p>This command was an example of <code>interactive</code> shell scripting because we are typing in the commands manually and waiting for the results. If we walk away from our machine and the session times out, then the program may be interrupted. <code>tmux</code> allows us to keep running the program even if we take a break.</p> <p>The next session demonstrates how to combine all of these commands into a script that runs on SLURM. </p> <p>SLURM differs from <code>interactive</code> computing because you activate the script instead of manually writing the commands one at a time.</p>"},{"location":"basic/intro-to-unix/create-manipulate-files/","title":"Creating & Manipulating Files","text":""},{"location":"basic/intro-to-unix/create-manipulate-files/#reading-file-contents","title":"Reading File Contents","text":"<p>There are a few different ways to see the contents of a file.</p> <p>We already used this first example.</p> <pre><code>cd ~/Oct22Workshops\n</code></pre> <p>Let's look inside the file. We have several methods of viewing the content of files that we have created.</p> <p>A helpful command is <code>cat</code>.</p> <pre><code>cat helloworld.txt\n</code></pre> <p>\"cat\" will open the entire file, so this is not the best command for long files.</p> <p>In that case \"head\" is a good option. Head pulls the top ten lines of the file and prints them to the screen.</p> <pre><code>head helloworld.txt\n</code></pre> <p>It does not look any different from cat in this case because there is only one line in the file.</p> <p>A third way to check file contents is by using a program called \"less\" (or \"more\").</p> <p>\"less\" will open the file interactively, then you can scroll through it and when you are done, push \"q\" on your keyboard to close the file.</p> <pre><code>less helloworld.txt\n</code></pre> <p>Press q to close the file opened by <code>less</code></p> <p>There are many versions of these tools on command line, but \"cat\", \"head\" and \"less\" are very common.</p>"},{"location":"basic/intro-to-unix/create-manipulate-files/#copying-files","title":"Copying Files","text":"<p>Sometimes we have a file that we want to reuse.</p> <p>When copying within the same directory, make sure to change the name of the file, or the original will be overwritten.</p> <p>When copying to a new directory, the name can stay the same.</p> <p>This command copies the file within the same directory with a new name. Both files are kept.</p> <p><pre><code>cp helloworld.txt helloworld1.txt\n</code></pre> Check this with <code>ls</code></p> <p>These commands make a new directory, and then copies the file into the new directory with the same name.</p> <pre><code>mkdir helloworld\ncp helloworld.txt helloworld\n</code></pre> <p>Check this with <code>ls helloworld</code> (lists the contents of the directory).</p>"},{"location":"basic/intro-to-unix/create-manipulate-files/#moving-files","title":"Moving Files","text":"<p><code>mv</code> is an option for renaming files, but also has the potential to overwrite existing files.</p> <p>For example, this command changes the name of the file and removes the original file. If <code>helloworld2.txt</code> already existed, it would be replaced.</p> <pre><code>mv helloworld1.txt helloworld2.txt\n</code></pre> <p>Check this with <code>ls</code></p>"},{"location":"basic/intro-to-unix/create-manipulate-files/#removing-files","title":"Removing Files","text":"<p><code>rm</code> and <code>rmdir</code> are permanent in shell, so make sure you are ready to delete files.</p> <pre><code>rm helloworld/helloworld.txt\n</code></pre> <p>Once the directory is empty, we can remove the directory.</p> <pre><code>rmdir helloworld\n</code></pre> <p>It will throw an error if the directory is not empty.</p> <p>If you are positive that you want to remove a directory and all the files within it, then add two flags, <code>-r</code> for recursive and <code>-f</code> for force.</p> <p>Both commands above could have been replaced with one remove command: <code>rm -rf helloworld</code></p> <p>Tip</p> <p>Until you are confident with file structure and bash commands, it is a good idea to copy instead of move and to    * <code>cp -u</code> will copy files only if they do not already exist.   * <code>cp -r</code> is a good command for copying directories, it means <code>copy recursively</code> which will copy the entire directory.   * <code>cp -rf</code> BE CAREFUL with this, it copies the entire directory AND forces the overwrite of any files that already exist.   * Adding the interactive flag <code>-i</code> on the commands <code>rm</code> and <code>mv</code> to set up a question that you answer <code>y</code> or <code>n</code> to before removing.</p> <pre><code>rm -i helloworld/helloworld.txt\n</code></pre> <p>Generates this question <pre><code>rm: remove regular file \u2018helloworld/helloworld.txt\u2019?\n</code></pre></p> <p><code>mv -i</code> only generates a question if you are in danger of overwriting an existing file.</p> <p>For example:</p> <p>1.) Make a new file from the original file we created</p> <p><pre><code>cp -u helloworld.txt helloworld1.txt\n</code></pre> <code>-u</code> for the copy command will not copy the file if it already exists.</p> <p>2.) Try to rename the file with <code>mv</code>, with the <code>i</code> parameter set to prevent overwriting an existing file.</p> <p><pre><code>mv -i helloworld.txt helloworld1.txt\n</code></pre> Generates the question:</p> <pre><code>mv: overwrite \u2018helloworld1.txt\u2019?\n</code></pre> <p>A great website to look at to understand the nuances of shell commands is:</p> <p>ComputerHope</p>"},{"location":"basic/intro-to-unix/going-home/","title":"Going Home","text":""},{"location":"basic/intro-to-unix/going-home/#going-home","title":"Going Home","text":"<p>Sometimes we get lost, so it is useful to know a few ways to get back to where you started.</p> <pre><code>cd\n</code></pre> <p>This command returns you to your home directory. Check by typing this command.</p> <pre><code>pwd\n</code></pre> <p>Other options for going back to your home directory:</p> <pre><code>cd ~\n</code></pre> <pre><code>cd $HOME\n</code></pre> <p>When lost in the file structure, going home is a good place to start.</p>"},{"location":"basic/intro-to-unix/intro-to-unix/","title":"Intro To Linux","text":""},{"location":"basic/intro-to-unix/intro-to-unix/#intro-to-command-line","title":"Intro to Command Line","text":"<p>Content Developed By Adelaide Rhodes, PhD</p> <p>This short workshop provides some basic training on bash and shell scripting on the command line on the Linux-based Tufts HPC cluster.</p> <p>This course is not meant to be comprehensive, but provides some insights into how the command line works as well as some strategic resources for studying and understanding command line on the HPC cluster.</p> <p>Helpful Vocabulary</p> <ul> <li>Command line is a more general term to indicate that you are using text commands on a terminal (linux bash shell or similar). Command line differs from \"Graphical User Interface (GUI)\" because all commands are texts instead of drag-and-drop or interactive formats such as the Windows or Mac Operating Sytems provide.</li> <li>HPC stands for High Performance Computing, \"cluster\" refers to a shared computer resource to enable more powerful computation than regularly available on an individual machine.</li> <li>Linux can refer to any of the free open source version of \"Unix\" from AT&amp;T Bell labs who pioneered the language in 1965. There are a number of Linux operating systems installed on HPC clusters (Ubuntu, Debian, RedHat Enterprise License (RHEL), CentOs, Fedora, etc.) Each of these systems have slight differences that may impact the commands demoed here. Tufts University Research Cluster is currently using RHEL7.</li> <li>Bash is one type of languages used in a \"shell\", the text interface on the Linux system. This lesson introduces a few objectives to help users understand how to use bash commands on the Linux RHEL shell of our HPC. Other shell languages have slight differences that affect how commands are run (e.g. new MacOSX ship with \"zsh\" as the default shell language on their installed terminal programs).</li> </ul>"},{"location":"basic/intro-to-unix/intro-to-unix/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>What is the shell?</li> <li>How do you access it?</li> <li> <p>How do you use it and what is it good for?</p> </li> <li> <p>Running commands</p> </li> <li>File Directory Structure</li> <li>Manipulating files</li> <li>Simple Bash Scripts</li> </ul>"},{"location":"basic/intro-to-unix/intro-to-unix/#what-is-the-shell","title":"What is the shell?","text":"<p>The shell is a program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination.</p> <p>There are many reasons to learn about the shell.  A few specific ones:</p> <ul> <li> <p>For most bioinformatics tools, you have to use the shell. There is no   graphical interface. If you want to work in metagenomics or genomics you're   going to need to use the shell.</p> </li> <li> <p>The shell gives you power. The command line gives you the power to   do your work more efficiently and more quickly. Shell allows users to automate repetitive tasks.</p> </li> <li> <p>To use remote computers or cloud computing, you need to use the shell.</p> </li> </ul>"},{"location":"basic/intro-to-unix/intro-to-unix/#knowing-shell-increases-speed-and-efficiency-through-automation","title":"Knowing Shell Increases Speed and Efficiency Through Automation","text":"<p>The most important reason to learn the shell is to learn about automation.  Any time you find yourself doing roughly the same computational task more than few times, it may be worth automating it; the shell is often the best way to automate anything to do with files.</p> <p>In this lesson, we're going to go through how to access Unix/Linux and some of the basic shell commands. We will finish with a demonstration of how to run programs interactively as well by submitting a job to SLURM. Slurm is a scalable cluster management and job scheduling system for Linux clusters. Other job scheduling systems you may be familiar with from other universities are \"PBS\" and \"SGE_Batch\".</p>"},{"location":"basic/intro-to-unix/intro-to-unix/#where-to-learn-shell-commands","title":"Where to learn shell commands","text":"<p>The challenge with bash for the command line is that it's not particularly simple - it's a power tool, with its own deep internal logic with lots of details.</p> <p>Practice is the best way to learn, but here are some helpful shell command resources:</p> <ul> <li>Fun With Unix Cheat Sheet</li> <li>Shell Cheatsheet - Software Carpentry</li> <li>Explain shell - a web site where you can see what the different components of a shell command are doing.</li> </ul>"},{"location":"basic/intro-to-unix/intro-to-unix/#how-to-access-the-shell-at-tufts","title":"How to Access the Shell at Tufts","text":"<p>Log In through OnDemand</p> <p></p>"},{"location":"basic/intro-to-unix/intro-to-unix/#mac","title":"Mac","text":"<p>On Mac the shell is available through Terminal Applications -&gt; Utilities -&gt; Terminal Go ahead and drag the Terminal application to your Dock for easy access. Note: newer versions of MacOSX ship with \"zsh\" as the default shell language in their terminal. It is possible to change the preference to \"bash\". However, if you are only using the terminal to log into the Tufts cluster, you don't necessarily need to do this, because you will be using \"bash\" once you are on the cluster. \"zsh\" would only impact scripts and commands run locally on your own machine.</p>"},{"location":"basic/intro-to-unix/intro-to-unix/#windows","title":"Windows","text":"<p>For Windows, an easy one to install and use right away is  gitbash. Download and install gitbash Open up the program.</p>"},{"location":"basic/intro-to-unix/intro-to-unix/#other-options","title":"Other options:","text":"<ul> <li>Microsoft Window Terminal</li> <li>Conemu</li> </ul>"},{"location":"basic/intro-to-unix/intro-to-unix/#linux","title":"Linux","text":"<p>You probably already know how to find the shell prompt.</p>"},{"location":"basic/intro-to-unix/running-interactive/","title":"Running an Interactive Session","text":""},{"location":"basic/intro-to-unix/running-interactive/#running-programs-interactively","title":"Running Programs Interactively","text":""},{"location":"basic/intro-to-unix/running-interactive/#hpc-etiquette","title":"HPC Etiquette","text":"<p>Try not to use the login computers for programs or large file management jobs. Looking things up and small commands such as <code>cat</code> or <code>head</code> are fine, but running programs may block others from logging in to the cluster.</p>"},{"location":"basic/intro-to-unix/running-interactive/#switch-to-an-interactive-session","title":"Switch to an Interactive Session","text":"<p>Do this first before running programs or testing your code.</p> <pre><code>srun -p batch --reservation=bioworkshop -n 2 --mem=8g -t 1-0 --pty bash\n</code></pre> <p>This command only works during the October-November 2022 workshops. To use this command after the workshop is over or if you are working on your own, just remove the reservation flag.</p> <ul> <li> <p><code>-p</code> which partition to use, outside of class it is okay to use <code>interactive</code> or <code>batch</code>, your group may have it's own partition. You can read more about what is available by going to the OnDemand dropdown menu for \"Misc\" and look at \"Scheduler Info\" to find all the partition names.</p> </li> <li> <p><code>--reservation</code> only applies during a specific class or workshop</p> </li> <li><code>-n</code> is the number of cpus to request, 2 or 4 is sufficient for most tests.</li> <li><code>--mem</code> specifies the memory requested, 8g is usually sufficient for small jobs, consult the documentation for a program to find out if a minimum memory requirement is needed.</li> <li><code>-t</code> indicates the time <code>1-0</code> means one day, so for tomorrow's session you will need to rerun this command.</li> <li><code>--pty bash</code> just indicates that the shell opens in <code>bash</code>, meaning that all the commands that we learned today will work.</li> </ul> <p>Question</p> <p>What compute node are you on? Type it into the chat box.</p> <p></p>"},{"location":"basic/intro-to-unix/running-interactive/#finding-your-files-interactively","title":"Finding your files interactively","text":"<p>When you request a computer using an <code>srun</code> command, the beginning of your command line should change to indicate that you are no longer on a <code>login</code> node and instead are on a <code>compute</code> node. It will tell you which node you are on.</p> <p>Your files will <code>mount</code> to the new node. This means that you can be on any computer in the Tufts HPC and it will recognize your home directory structure.</p> <p>Let's go back into the Oct22Workshop directory, but this time use your ABSOLUTE path by changing <code>username01</code> to your username. If you forget your username, try <code>whoami</code>.</p> <pre><code>cd /cluster/home/username01/Oct22Workshop\n</code></pre>"},{"location":"basic/intro-to-unix/running-interactive/#find-and-tree","title":"Find and Tree","text":"<p>File structures can get complicated quickly.</p> <p>Two tools to understand where your files are that can help are <code>find</code> and <code>tree</code>.</p> <p>From your home directory, you can find your file named <code>helloworld.txt</code> by typing the following:</p> <pre><code>cd\nfind . -name helloworld.txt\n</code></pre> <p><code>find</code> is a bash command <code>.</code> means look from this location and into any subdirectories to this location <code>name</code> is the file that you are looking for</p> <p>From the home directory, the answer is given using the RELATIVE path:</p> <pre><code>./Oct22Workshop/helloworld.txt\n</code></pre> <p><code>.</code> in this case is another RELATIVE path direction that indicates \"from this directoy that I am in currently\". Note that the answer is given in the RELATIVE path format, starting with <code>.</code> = here.</p> <p>It is also possible to provide an ABSOLUTE path to this command.</p> <p><pre><code>find /cluster/home/username01 -name helloworld.txt\n</code></pre> This command will work from anywhere in the cluster. Note that the answer is given in the ABSOLUTE path format.</p> <pre><code>/cluster/home/username01/Oct22Workshop/helloworld.txt\n</code></pre> <p>Tip</p> <ul> <li><code>find</code> using the parameter <code>iname</code> allows the search to be insensitive to case <code>find . -iname \"helloworld.txt\"</code> finds <code>Helloworld.txt</code> AND <code>helloworld.txt</code></li> <li>wildcards allow for partial searches of many filenames that may match.</li> <li>The easiest wildcard is <code>*</code> which means any number of characters can match, such as <code>find . -iname \"hello*.txt\"</code> finds any file that begins with <code>hello</code> and has any number of characters before <code>.txt</code> finds <code>helloworld1.txt</code> AND <code>helloworld2.txt</code>. </li> <li><code>*</code> can be used anywhere in the pattern: <code>hello*.txt</code>, <code>hello*.txt</code>, <code>*.txt</code></li> </ul> <p>Another helpful bash command for finding files is <code>tree</code>.</p> <pre><code>tree\n</code></pre> <p>This outputs your directory structure with lines that indicate the tree-like branches of your file structure.</p> <p>This could be very messy if you already have a lot of files in a directory, so limit the level by adding a flag.</p> <pre><code>tree -L 2\n</code></pre> <p>This just shows the top two levels of the file structure.</p> <p>Tip</p> <p>There are some keyboard shortcuts that can help when writing complex commands and running programs interactively.</p> <ul> <li>Control-C will terminate a running process</li> <li>Control-A will put your cursor at the beginning of the line</li> <li>Control-E will put your cursor at the end of the line</li> <li>Up and down arrows will scroll through recent commands - If you make a mistake, just hit up to reveal the command and work on the part that was a mistake instead of retyping the whole thing.</li> </ul> <p>Note</p> <p>When trouble shooting a command using tickets, screen shots of error messages are a good option. (On Macs, Command-Shift-4)</p>"},{"location":"basic/intro-to-unix/running-interactive/#what-is-blast","title":"What is BLAST?","text":"<p>BLAST is the Basic Local Alignment Search Tool. It uses an index to rapdily search large sequence databases; it starts by finding small matches between the two sequences and extending those matches.</p> <p></p> <p>For more information on how BLAST works and the different BLAST functionality, check out the summary on Wikipedia or the NCBI's list of BLAST resources.</p> <p>BLAST can be helpful for identifying the source of a sequence, or finding a similar sequence in another organism. In this lesson, we will use BLAST to find zebrafish proteins that are similar to a small set of mouse proteins.</p>"},{"location":"basic/intro-to-unix/running-interactive/#why-use-the-command-line","title":"Why use the command line?","text":"<p>BLAST has a very nice graphical interface for searching sequences in NCBI's database. However, running BLAST through the commmand line has many benefits:   * It's much easier to run many BLAST queries using the command line than the GUI   * Running BLAST with the command line is reproducible and can be documented in a script   * The results can be saved in a machine-readable format that can be analyzed later on   * You can create your own databases to search rather than using NCBI's pre-built databases   * It allows the queries to be automated   * It allows you to use a remote computer to run the BLAST queries</p> <p>We are next going to write a script that we will send to SLURM which will demonstrate these advantages.</p>"},{"location":"basic/intro-to-unix/shell-navigation/","title":"Shell Navigation","text":""},{"location":"basic/intro-to-unix/shell-navigation/#navigating-in-the-shell","title":"Navigating in the Shell","text":"<p>Best Practices for Naming Files and Directories</p> <p>A directory is like a desk drawer. We create them to store files that relate to each other mostly.</p> <p>When creating directories and filenames it is helpful to put some information about the project and the date of activity.</p> <p></p>"},{"location":"basic/intro-to-unix/shell-navigation/#absolute-and-relative-paths","title":"Absolute and Relative Paths","text":"<p>Let's go into our directory and look around using relative and absolute paths.</p> <p>Go home</p> <p><pre><code>cd\n</code></pre> Go into our workshop directory</p> <pre><code>cd Oct22Workshop\n</code></pre> <p>and then</p> <pre><code>pwd\n</code></pre> <p>You should now see something like this:</p> <pre><code>/cluster/home/username01/Oct22Workshop\n</code></pre> <p>This is an example of an Absolute Path.</p> <p>It gives an address for where you are located on the cluster, much like a postal address that defines where you are in several layers (e.g. /country/state/city/street/specific_house.</p> <p></p> <p>You can have many files and folders that share the same name in your directories (e.g. scripts, data). An absolute path ensures that you go to the correct file, as it will be unique.</p> <p>If you want to go back to the directory that is in the level above our current file (in this case \"home\"), another common shortcut used in bash is <code>..</code></p> <pre><code>cd ..\n</code></pre> <p><code>..</code> is a reference to a RELATIVE PATH</p> <pre><code>pwd\n</code></pre> <p>You should be back in your home directory.</p> <pre><code>/cluster/home/username01/\n</code></pre> <p>If you want to go back to the directory that you just left, type this command.</p> <p><pre><code>cd -\n</code></pre> Then find your location.</p> <pre><code>pwd\n</code></pre> <p>You should be back in the directory you came from.</p> <pre><code>/cluster/home/username01/Oct22Workshop\n</code></pre> <p>A *RELATIVE PATH means that the command only works from the relative location that you are in.</p> <p><code>cd ..</code> and <code>cd -</code> are examples of relative path commands.</p> <p>Note</p> <p>Your home directory is not all the way back at the root ('/'), it is set within the cluster as <code>/cluster/home/username01/</code>.</p> <p>You can make sure that you are in the right directory by using the command <code>cd</code> with the absolute path.</p> <pre><code>cd /cluster/home/username01/Oct22Workshop\n</code></pre> <p>This command will make sense inside a script, because the exact path is specified.</p>"},{"location":"basic/intro-to-unix/shell-navigation/#using-bash-commands-with-absolute-paths","title":"Using Bash Commands with Absolute Paths","text":"<p>Many commands in bash can be used with the ABSOLUTE PATH.</p> <pre><code>ls /cluster/home/username01/Oct22Workshop\n</code></pre> <pre><code>helloworld.txt\nemptyfile.txt\n</code></pre> <p>Absolute Paths are better for SLURM</p> <p>This can get confusing if you are moving around a lot in your directories or sending commands to SLURM, so the alternative method to navigating around the cluster is using an ABSOLUTE PATH.</p>"},{"location":"basic/intro-to-unix/start-w-shell/","title":"Starting with the Shell","text":"<p>We will spend most of our time learning about the basics of the shell by manipulating some experimental data that we download from the internet.</p> For Attendees Using Terminal Programs to Access the Cluster (instead of the Web Browser \"OnDemand <p>If you are using a terminal on your home machine to connect to the tufts cluster, you will first need to log in by sending a simple command. Ignore this if you are using the web browser login tool.</p> <p>Replace \"username01\" with your tufts username.</p> <pre><code>ssh username01@login.pax.tufts.edu\n</code></pre> <p>Your username will have been created when your account was set up. If you do not have a cluster account, you can still follow this tutorial from your laptop or personal computer, except that the file structure will be different from what is described.</p> <p>The login will ask you for your Tufts password.</p> Connection Issues? <p>If you are not on the Tufts network, you will need to set up the Tufts VPN (Virtual Private Network) before logging in:</p> <p>VPN Instructions</p> <p>Best Practices for Logging In</p> <p>If you are logged in to OnDemand, and on a machine called \"login\". If you are not on the login machine, type <code>exit</code> to get there.</p> <p>First, run the <code>tmux</code> commands (remember which login machine you are on, <code>login-prod-#</code></p> <p><pre><code>module load tmux\ntmux new -s OctoberWorkshop\n</code></pre> You will be able to recover this session if you are on the same login node and run <code>tmux a -t OctoberWorkshop</code></p> <p>Second, run the <code>srun</code> command to go to a working machine, remember we have a reservation for this workshop on October 19 so if you are reading this at a different time, just drop the <code>--reservation=bioworkshop</code> parameter from the command.</p> <pre><code>srun -p batch --time=1-2:10:00 -n 2 --mem=4g --reservation=bioworkshop --pty bash\n</code></pre> <p>The beginning of the line is called the 'command line prompt.'</p> <p><pre><code>[username01@login-prod-02 ~]$\n</code></pre> It tells you who you are and what machine you have been assigned.</p> <p>Once you run the <code>srun</code> command, the machine name should change</p> <pre><code>[username01@i2cmp003 ~]$\n</code></pre> <p>Question</p> <p>What machine are you on? Type your answer into the chat box.</p> <p>Tip</p> <p>The <code>$</code> at the end of the line is where you start typing your commands. The <code>$</code> (on some Mac terminals it is a <code>%</code>) is not part of the command. The outputs from commands will not have that piece of information or <code>$</code> at the beginning of the line.</p> <p>Tip</p> <p>The name of the computer you are on is important informatiom when troubleshooting the cluster.  <code>login</code> machines will reject large commands and output an error.  Make sure to switch machines with <code>srun</code> before running programs. </p>"},{"location":"basic/intro-to-unix/start-w-shell/#using-basic-commands","title":"Using Basic Commands","text":"<p>Open up the shell through a terminal (OnDemand or on your laptop) and type the command::</p> <pre><code>whoami\n</code></pre> <p>and then hit ENTER </p> <p>(This is a good question for Mondays ....)</p> <p>When you are on the Tufts cluster, this will return your username according to the cluster. This username is attached to you wherever you are in the cluster and creates a home where your files can be kept, regardless of which machine you are on in the cluster. [If you are on your laptop or personal computer, the answer to this may be different before you log in.]</p>"},{"location":"basic/intro-to-unix/start-w-shell/#running-commands","title":"Running Commands","text":"<p>Let's try some simple commands.</p> <p>Much like text shortcuts, shell commands often use abbreviations to get their point across.</p> <p>For example, the command pwd is short for \"print working directory.\" The word \"print\" here means it will output it into the visible screen.</p> <p>Now type the command</p> <pre><code>pwd\n</code></pre> <p>You should see something similar to this:</p> <pre><code>/cluster/home/username01/\n</code></pre> <p>Try this command</p> <pre><code>ls\n</code></pre> <p>It may be empty for the moment, or it may not if this is not your first time using the shell. We will be creating content in the next part of this lesson.</p> <p>Takeaways</p> <p><code>pwd</code> and <code>ls</code> are examples of commands - programs you run at the shell prompt that do stuff. </p> <ul> <li><code>pwd</code> stands for 'print working directory', while</li> <li><code>ls</code> stands for 'list files'. </li> </ul> <p>It is similar to the abbreviations used in texting, it takes less time to get the point across (lol, tbh, imho, afaik, ftw -- you're saying them outloud in your head, right now, correct?)</p>"},{"location":"basic/intro-to-unix/unix/","title":"Introduction To Unix","text":""},{"location":"hpc-user-guide/batch-job/","title":"Submitting A Batch Job","text":""},{"location":"hpc-user-guide/batch-job/#batch-scripts","title":"Batch Scripts","text":"<ul> <li>When we want to run a script on the Tufts HPC cluster we need to submit it as a batch script.</li> <li>Here is an example of a batch script called sbatch.sh:</li> </ul> <p>sbatch.sh</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=job            # job name is \"job\"\n#SBATCH --nodes=1                 # 1 nodes #for many shared-memory programs,please leave -N as 1.\n#SBATCH -n 2                      # 2 tasks total and 1 cpu per task, that gives you 2 cpu cores for this job\n#SBATCH --partition=batch         # running on \"batch\" partition/queue\n#SBATCH --reservation=bioworkshop # running on a reservation, named \"bioworkshop\", if no access to reservation, omit this line\n#SBATCH --mem=8Gb                 # requesting 8GB of RAM total for the number of cpus you requested\n#SBATCH --time=0-24:00:00         # requested time (DD-HH:MM:SS) 24 hours\n#SBATCH --output=%j.out           # saving standard output to file, %j=JOBID\n#SBATCH --error=%j.err            # saving standard error to file, %j=JOBID\n#SBATCH --mail-type=ALL           # email optitions\n#SBATCH --mail-user=Your_Tufts_Email@tufts.edu  # use your own Tufts email address\n\n# [this is a comment]\n# The order of the \"#SBATCH\" options doesn't matter\n\n#[commands_you_would_like_to_exe_on_the_compute_nodes]\n# for example, running blast \n# load the module so the correct version of blast is available to you\n\nmodule load blast-plus/2.11.0\n\n# running blast\nblastp -query mm-second.faa -db zebrafish.1.protein.faa -out mm-second.x.zebrafish.tsv -outfmt 6\n</code></pre>"},{"location":"hpc-user-guide/batch-job/#submitting-a-batch-job","title":"Submitting a Batch Job","text":"<ul> <li>Submit the job using the following command from command line interface:</li> </ul> <pre><code>sbatch sbatch.sh\n</code></pre> <p>Looking for a sample batch script?</p> <p>Sample Scripts including R, conda, matlab, gaussian, etc. can be found here:</p> <pre><code>/cluster/tufts/hpc/tools/slurm_scripts\n</code></pre>"},{"location":"hpc-user-guide/cluster_storage_increase/","title":"Cluster Storage Increase Request","text":""},{"location":"hpc-user-guide/cluster_storage_increase/#cluster-storage-increase","title":"Cluster Storage Increase","text":"<p>If you need more storage, we are happy to give an increase. But please be mindful and clean up any data that is not needed once you are done with data analysis. To submit an official request for Cluster Storage Increase, please do the following and consider how much data you will generate or work with in the next 12 months.</p> <ol> <li>Use the following link to request an increase in cluster storage:</li> </ol> <p>Cluster Storage Increase Request</p> <ol> <li>Login with your Tufts credentials</li> <li>Fill out blank fields</li> <li>Select \"Research Storage Request\"</li> <li>Select \"Cluster (HPC)\"</li> <li>Select \"Increase\"</li> <li>For FULL PATH of STORAGE input: GroupName | /cluster/tufts/GroupName/</li> <li>Click next</li> <li>Fill out remaining information</li> <li>Click on Submit </li> </ol> <p>Once the request is submitted and the storage owner approves the request, we will apply the increase.</p>"},{"location":"hpc-user-guide/cluster_storage_increase/#questions","title":"Questions","text":"<p>If you have any questions about requesting a cluster storage account, please reach out to tts-research@tufts.edu</p>"},{"location":"hpc-user-guide/compute-resources/","title":"Compute Resources","text":""},{"location":"hpc-user-guide/compute-resources/#compute-resources","title":"Compute Resources","text":""},{"location":"hpc-user-guide/compute-resources/#cpus","title":"CPUs","text":"<ul> <li>Resources are orgnized into partitions on the cluster based on functionality and priority.</li> <li>After logging in on the HPC cluster, you can use command <code>sinfo</code> to check the <code>partition</code> you have access to (all partitions listed in the <code>sinfo</code> output).</li> </ul> <pre><code>sinfo\n</code></pre> <p>output</p> <pre><code>PARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST \ninteractive     up    4:00:00      1    mix c1cmp064 \ninteractive     up    4:00:00      1   idle c1cmp063 \nbatch*          up 7-00:00:00      1  down* p1cmp005 \nbatch*          up 7-00:00:00      1  drain p1cmp056 \nbatch*          up 7-00:00:00     16   resv c1cmp[009,033,035-039,044-049],p1cmp[004,009,054] \nbatch*          up 7-00:00:00     34    mix c1cmp[003-008,010-020,023-024,034,040-043,051-052,054],i2cmp001,p1cmp[003,012,015,018,020-021] \nbatch*          up 7-00:00:00     17  alloc c1cmp[021-022,053],i2cmp003,p1cmp[001,006-008,010-011,013-014,019,022-024,055] \nbatch*          up 7-00:00:00      2   idle p1cmp[016-017] \nmpi             up 7-00:00:00      1  down* p1cmp005 \nmpi             up 7-00:00:00      1  drain p1cmp056 \nmpi             up 7-00:00:00     16   resv c1cmp[009,033,035-039,044-049],p1cmp[004,009,054] \nmpi             up 7-00:00:00     34    mix c1cmp[003-008,010-020,023-024,034,040-043,051-052,054],i2cmp001,p1cmp[003,012,015,018,020-021] \nmpi             up 7-00:00:00     16  alloc c1cmp[021-022,053],p1cmp[001,006-008,010-011,013-014,019,022-024,055] \nmpi             up 7-00:00:00      2   idle p1cmp[016-017] \ngpu             up 7-00:00:00      1    mix p1cmp073 \ngpu             up 7-00:00:00      2  alloc c1cmp[025-026] \nlargemem        up 7-00:00:00      7    mix c1cmp[027-028,030,057,061-062],i2cmp055 \nlargemem        up 7-00:00:00      2  alloc p1cmp[049-050] \nlargemem        up 7-00:00:00      3   idle c1cmp[032,058-059] \npreempt         up 7-00:00:00      2   mix$ p1cmp[094-095] \npreempt         up 7-00:00:00      4  maint p1cmp[090,092,103,109] \npreempt         up 7-00:00:00      1  down* p1cmp005 \npreempt         up 7-00:00:00      2  drain p1cmp[038,056] \npreempt         up 7-00:00:00      3   resv p1cmp[004,009,054] \npreempt         up 7-00:00:00     71    mix cc1gpu[001-005],i2cmp[010-032,038-043,045-051],p1cmp[003,012,015,018,020-021,070-077,079-080,091,093,096,098-102,104-108,110] \npreempt         up 7-00:00:00     25  alloc c1cmp[025-026],i2cmp[004-006,008-009,033-035,037,052-053],p1cmp[006-008,010-011,013-014,019,022-024,055] \npreempt         up 7-00:00:00     20   idle p1cmp[016-017,031-037,039-042,081-086,097] \n</code></pre> <ul> <li> <p>If you are curious about nodes and the details of those nodes you can go to:</p> <ul> <li>OnDemand <code>Misc</code> &gt; <code>Inventory</code> to dipslay more node details (core count &amp; memory)</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"hpc-user-guide/compute-resources/#gpus","title":"GPUs","text":"<ul> <li> <p>NVIDIA GPUs are available in <code>gpu</code> and <code>preempt</code> partitions</p> </li> <li> <p>Request GPU resources with <code>--gres</code>. See details below.</p> </li> <li>Please DO NOT manually set <code>CUDA_VISIBLE_DEVICES</code>. </li> <li>Users can ONLY see GPU devices that are assigned to them with <code>$ nvidia-smi</code>.</li> <li><code>gpu</code> partition<code>-p gpu</code>:</li> <li>NVIDIA P100s<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 6 on one node)</li> </ul> </li> <li>NVIDIA Tesla K20xm<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:k20xm:1</code>(one Tesla K20xm GPU, can request up to 1 on one node)</li> </ul> </li> <li><code>preempt</code> partition <code>-p preempt</code>:</li> <li><code>a100</code>, <code>v100</code>, <code>p100</code>, <code>rtx_6000</code>, <code>t4</code></li> <li>NVIDIA T4<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:t4:1</code>(one T4 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA P100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA rtx_6000<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000:1</code>(one RTX_6000 GPU, can request up to 8 on one node)</li> </ul> </li> <li>NVIDIA V100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:v100:1</code>(one V100 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA A100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> </ul> </li> </ul>"},{"location":"hpc-user-guide/file-transfer/","title":"File Transfer","text":"<p>You can transfer files to and from the cluster using:</p> <ul> <li>OnDemand</li> <li>A File Transfer Client</li> <li>Command Line</li> </ul>"},{"location":"hpc-user-guide/file-transfer/#ondemand","title":"OnDemand","text":"<p>Note</p> <p>Only for transfering files size up to 976MB per file.</p> <ul> <li>Go to:</li> </ul> <p>OnDemand</p> <ul> <li>Under <code>Files</code></li> </ul> <p></p> <ul> <li>Using the <code>Upload</code> or <code>Download</code> buttons to transfer. </li> </ul> <p></p>"},{"location":"hpc-user-guide/file-transfer/#file-transfer-client","title":"File Transfer Client","text":"<ul> <li> <p>Download one of these free file transfer programs:</p> <ul> <li> <p>WinSCP </p> </li> <li> <p>FileZilla </p> </li> <li> <p>Cyberduck </p> </li> </ul> </li> <li> <p>Then use the following information to connect to the cluster:</p> <ul> <li>Hostname: xfer.cluster.tufts.edu</li> <li>Protocol: SCP or SFTP</li> <li>Use port 22 for SFTP</li> </ul> </li> </ul>"},{"location":"hpc-user-guide/file-transfer/#command-line","title":"Command Line","text":"<p>Terminology</p> <ul> <li>Local_Path: is the path to your files or directory on your local computer</li> <li>Cluster_Path: is the path to your files or directory on the cluster</li> <li>Cluster Home Directory: /cluster/home/your_utln/your_folder</li> <li>Cluster Home Directory: /cluster/home/your_utln/your_folder</li> <li>Cluster Research Project Storage Space Directory: /cluster/tufts/yourlabname/your_utln/your_folder</li> </ul> <ul> <li>Execute these commands from your local machine terminal using this general format to transfer files:</li> </ul> <pre><code>scp From_Path To_Path\n</code></pre> <pre><code>rsync From_Path To_Path\n</code></pre> <p>Note</p> <p>If you are transfering very large files that could take hours to finish, we would suggest using <code>rsync</code> as it has ability to restart from where it left if interrupted.</p>"},{"location":"hpc-user-guide/file-transfer/#download-from-cluster","title":"Download from cluster","text":"<pre><code>scp your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre> <pre><code>rsync your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre>"},{"location":"hpc-user-guide/file-transfer/#upload-to-cluster","title":"Upload to cluster","text":"<pre><code>scp Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path </code></pre> <pre><code>rsync Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre> <p>Tip</p> <p>If you are transfering a directory use <code>scp -r</code> or <code>rsync -azP</code></p>"},{"location":"hpc-user-guide/file-transfer/#download-from-cluster_1","title":"Download from cluster","text":"<pre><code>scp -r your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path  </code></pre> <pre><code>rsync -azP your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre>"},{"location":"hpc-user-guide/file-transfer/#upload-to-cluster_1","title":"Upload to cluster","text":"<pre><code>scp -r Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre> <pre><code>rsync -azP Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre>"},{"location":"hpc-user-guide/hpc-storage/","title":"Hpc storage","text":""},{"location":"hpc-user-guide/hpc-storage/#hpc-storage","title":"HPC Storage","text":""},{"location":"hpc-user-guide/hpc-storage/#home-directory","title":"Home Directory","text":"<ul> <li>Your Home Directory (10GB, fixed) should be <code>/cluster/home/your_utln</code></li> </ul>"},{"location":"hpc-user-guide/hpc-storage/#reserach-project-storage","title":"Reserach Project Storage","text":"<ul> <li>To Get Storage for a research project visit:</li> </ul> <p> Tufts Research Technology - High Performance Computing</p> <ul> <li>Where you will see the following options:</li> </ul> <p></p> <ul> <li>Your research projet storage (from 50GB and up) path should be <code>/cluster/tufts/yourlabname/</code></li> <li>Each member of the lab group has a dedicated directory:</li> </ul> <p><code>/cluster/tufts/yourlabname/your_utln</code></p> <ul> <li>To see your research project storage quota run the following command from any node on the new cluster Pax:</li> </ul> <p><code>df -h /cluster/tufts/yourlabname</code> </p> <p>Note</p> <p>Accessing your research project storage space for the first time, please make sure you type out the FULL PATH to the directory.</p> <ul> <li>If your group has existing HPC research project storage space set up, please use the same link to request access. </li> </ul>"},{"location":"hpc-user-guide/hpc_services/","title":"HPC Services","text":""},{"location":"hpc-user-guide/hpc_services/#hpc-for-classes","title":"HPC for Classes","text":"<ul> <li>Please use the following link to request resources for your class:</li> </ul> <p>Class Setup Request</p>"},{"location":"hpc-user-guide/hpc_services/#understanding-slurm","title":"Understanding SLURM","text":"<ul> <li>For more information on SLURM commands visit:</li> </ul> <p>SLURM Commands</p>"},{"location":"hpc-user-guide/hpc_services/#getting-help","title":"Getting Help","text":"<ul> <li>If you need any assistance with the Tufts HPC cluster reach out to tts-research@tufts.edu </li> </ul>"},{"location":"hpc-user-guide/interactive-session/","title":"Interactive Session","text":""},{"location":"hpc-user-guide/interactive-session/#interactive-session","title":"Interactive Session","text":"<ul> <li> <p>An interactive session is a way to temporarily grab resources on the Tufts HPC.</p> <ul> <li>Particularly good for debugging and working with software GUI. </li> </ul> </li> <li> <p>The following is the basic layout of the command to get an interactive session:</p> </li> </ul> <pre><code>srun [options] --pty [command]\n</code></pre> <p>What does this mean?</p> <ul> <li> <p>Command </p> <ul> <li>command to run an application, given the module is already loaded.</li> <li><code>bash</code> for a bash shell</li> </ul> </li> <li> <p>Options</p> <ul> <li>Pseudo terminal <code>--pty</code></li> <li>Partition <code>-p</code> <ul> <li>Default batch if not specified</li> </ul> </li> <li>You can start interactive sessions on any partition you have access to</li> <li>Time <code>-t</code> or <code>--time=</code><ul> <li>Default 15 minutes if not specified on non-\"interactive\" partition</li> </ul> </li> <li>Number of CPU cores <code>-n</code> <ul> <li>Default 1 if not specified</li> </ul> </li> <li>Memory <code>--mem=</code><ul> <li>Default 2GB if not specified</li> </ul> </li> <li>GPU <code>--gres=</code><ul> <li>Default none</li> </ul> </li> <li>X Window <code>--x11=first</code><ul> <li>Default none  </li> </ul> </li> </ul> </li> </ul>"},{"location":"hpc-user-guide/interactive-session/#example-of-getting-an-interactive-session","title":"Example of Getting an Interactive Session","text":"<pre><code>srun -p batch --time=1-2:10:00 -n 2 --mem=8g --pty bash\n</code></pre> <p>What does this mean?</p> <p>Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 2GB of RAM, with X11 forwarding for 1 day, 2 hours, and 30 minutes (use <code>exit</code> to end session and release resources).</p> <p>You will have notice that your prompt changed from:</p> <pre><code>[your_utln@login-prod-01 ~]$\n</code></pre> <p>To:</p> <pre><code>[your_utln@c1cmp044 ~]$\n</code></pre> <ul> <li>This means you have been placed on a compute node!</li> </ul>"},{"location":"hpc-user-guide/job-status/","title":"Checking Job Status","text":""},{"location":"hpc-user-guide/job-status/#checking-job-status","title":"Checking Job Status","text":"<ul> <li>Checking your active jobs:</li> </ul> <pre><code>squeue -u $USER\n</code></pre> <p>output</p> <pre><code>   JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON) \n24063163     batch      job your_utln  R       0:17      1 c1cmp044 \n</code></pre> <pre><code>squeue -u your_utln\n</code></pre> <p>output</p> <pre><code>   JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON) \n24063163     batch      job your_utln  R       0:17      1 c1cmp044 \n</code></pre>"},{"location":"hpc-user-guide/job-status/#to-cancel-a-job","title":"To Cancel A Job","text":"<ul> <li>To cancel a specific job:</li> </ul> <pre><code>scancel JOBID\n</code></pre> <ul> <li>To cancel all of your jobs:</li> </ul> <pre><code>scancel -u $USER\n</code></pre> <pre><code>scancel -u your_utln\n</code></pre>"},{"location":"hpc-user-guide/job-status/#to-check-the-details-of-your-active-jobs","title":"To Check The Details Of Your Active Jobs","text":"<ul> <li>To check details of your active jobs (running \"R\" or pending \"PD\"):</li> </ul> <pre><code>scontrol show jobid -dd JOBID\n</code></pre> <pre><code>scontrol show jobid -dd 24063163\n</code></pre> <p>output</p> <pre><code>JobId=24063163 JobName=job\n   UserId=your_utln(31003) GroupId=your_utln(5343) MCS_label=N/A\n   Priority=12833 Nice=0 Account=normal QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:01:31 TimeLimit=1-00:00:00 TimeMin=N/A\n   SubmitTime=2022-07-20T12:33:14 EligibleTime=2022-07-20T12:33:14\n   AccrueTime=2022-07-20T12:33:14\n   StartTime=2022-07-20T12:33:15 EndTime=2022-07-21T12:33:15 Deadline=N/A\n   PreemptEligibleTime=2022-07-20T12:33:15 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-07-20T12:33:15\n   Partition=batch AllocNode:Sid=c1cmp044:27677\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=c1cmp044\n   BatchHost=c1cmp044\n   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=8G,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   JOB_GRES=(null)\n     Nodes=c1cmp044 CPU_IDs=2-3 Mem=8192 GRES=\n   MinCPUsNode=1 MinMemoryNode=8G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   Reservation=bioworkshop\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/cluster/home/ymalon01/workshop/LS/sbatch.sh\n   WorkDir=/cluster/home/ymalon01/workshop/LS\n   StdErr=/cluster/home/ymalon01/workshop/LS/24063163.err\n   StdIn=/dev/null\n   StdOut=/cluster/home/ymalon01/workshop/LS/24063163.out\n   Power=\n   MailUser=ymalon01 MailType=NONE\n</code></pre>"},{"location":"hpc-user-guide/job-status/#to-check-the-details-of-your-finished-jobs","title":"To Check The Details Of Your Finished Jobs","text":"<ul> <li>Checking your finished jobs:</li> </ul> <p>You can no longer see these jobs in <code>squeue</code> command output.</p> <p>Tip</p> <p>Querying finished jobs helps users make better decisions on requesting resources for future jobs.</p> <p>Display job CPU and memory usage:</p> <pre><code>seff JOBID\n</code></pre> <p>output</p> <pre><code>seff JOBID\nJob ID: JOBID\nCluster: pax\nUser/Group: your_utln/your_utln\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:03:00\nCPU Efficiency: 50.00% of 00:06:00 core-walltime\nJob Wall-clock time: 00:03:00\nMemory Utilized: 54.16 MB\nMemory Efficiency: 0.66% of 8.00 GB\n</code></pre>"},{"location":"hpc-user-guide/job-status/#check-accounting-data-for-a-job","title":"Check Accounting Data for a Job","text":"<ul> <li>Display detailed job accounting data:</li> </ul> <pre><code>sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j JOBID\n</code></pre> <p>output</p> <pre><code> Partition      State  Timelimit               Start                 End    Elapsed     MaxRSS     ReqMem  MaxVMSize   NNodes      NCPUS        NodeList \n---------- ---------- ---------- ------------------- ------------------- ---------- ---------- ---------- ---------- -------- ---------- --------------- batch  COMPLETED 1-00:00:00 2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00                   8Gn                   1          2        c1cmp044 COMPLETED            2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00     55464K        8Gn    198364K        1          2        c1cmp044 OUT_OF_ME+            2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00          0        8Gn    108052K        1          2        c1cmp044 </code></pre>"},{"location":"hpc-user-guide/job-status/#going-further","title":"Going Further","text":"<ul> <li>For more SLURM options check out:</li> </ul> <p>SLURM Workload Manager</p>"},{"location":"hpc-user-guide/navigate-to-cluster/","title":"Navigate To The Cluster","text":""},{"location":"hpc-user-guide/navigate-to-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Request an Research Computing Cluster account</li> <li>Connect to the VPN if off campus</li> </ul>"},{"location":"hpc-user-guide/navigate-to-cluster/#navigate-to-the-cluster","title":"Navigate to the Cluster","text":"<ul> <li>You can access the Tufts HPC in two ways, either the OnDemand website, or command line.</li> </ul>"},{"location":"hpc-user-guide/navigate-to-cluster/#the-ondemand-website","title":"The OnDemand Website:","text":"<p>OnDemand Website</p> <ul> <li>Log in with your tufts credentials</li> <li>Once you are logged in you'll notice a few navigation options:</li> </ul> <p></p> <p>To Access A Shell Terminal</p> <ul> <li>Click on Clusters &gt; Tufts HPC Shell Access</li> </ul> <p>To Access An Interactive App</p> <ul> <li>Click on Interactive Apps</li> <li>Select the App you are interested in using</li> <li>For more information on how to access an interactive app take a look at our tutorials on RStudio and JupyterLab:</li> </ul> <p>RStudio Interactive Session</p> <p>JupyterLab Interactive Session</p>"},{"location":"hpc-user-guide/navigate-to-cluster/#command-line","title":"Command Line:","text":"<ul> <li> <p>You can access the Tufts HPC Cluster via command line with:</p> <ul> <li>The Terminal app on a Mac or Linux machine</li> <li>PuTTy or Cygwin SSH or SecureCRT or other SSH clients on a Windows machine</li> </ul> </li> <li> <p>To Log in open one of the aformentioned apps and enter:</p> </li> </ul> <pre><code>ssh your_utln@login.pax.tufts.edu\n</code></pre> <ul> <li>Next log in with your Tufts Credentials</li> <li>At this point you are on a login node and should see something like the following:</li> </ul> <p>output</p> <pre><code>[your_utln@login-prod-02 ~]\n</code></pre> <ul> <li>While on a login node please do not run any programs. For this you will need to get compute resources. See the following to get compute resources: </li> </ul> <p>Compute Resources</p>"},{"location":"hpc-user-guide/new_cluster_storage/","title":"Cluster Storage Request","text":""},{"location":"hpc-user-guide/new_cluster_storage/#new-cluster-storage-request","title":"New Cluster Storage Request","text":"<p>Please do the following to submit an official request for new Cluster Storage for your research group.</p> <ol> <li>Use the following the link to request a cluster storage account:</li> </ol> <p>Cluster Storage Account Request</p> <ol> <li>Login with your Tufts credentials</li> <li>Fill out blank fields</li> <li>Select \"Research Storage Request\"</li> <li>Select \"Cluster (HPC)\"</li> <li>Select \"New\"</li> <li>Click next</li> <li>Fill out remaining information</li> <li>Click on Submit</li> </ol>"},{"location":"hpc-user-guide/new_cluster_storage/#questions","title":"Questions","text":"<p>If you have any questions about requesting a cluster storage account, please reach out to tts-research@tufts.edu</p>"},{"location":"hpc-user-guide/request-account/","title":"Request an Account","text":"<p>Requirements</p> <ul> <li>You must complete an online account request form and be approved to use the Tufts Research Cluster.</li> <li>Account requests require a valid Tufts Username and Tufts Password</li> <li>Guest and student accounts require faculty or researcher sponsorship</li> </ul>"},{"location":"hpc-user-guide/request-account/#get-started","title":"Get Started","text":"<ul> <li>To request a Research Computing Cluster account use the following link:</li> </ul> <p>Cluster Account Request</p> <ul> <li>Log in with your Tufts Username and Tufts Password.</li> <li>The form will auto-fill as much of your user information as possible. Double-check to make sure it\u2019s correct, selecting the down arrow to adjust the affiliation information if it\u2019s incorrect.</li> <li>Remove the example text in the Usage Information box and briefly describe your planned use of the Cluster.</li> <li>In the Type of Account field, select the Cluster. </li> <li>When finished, click, Submit Request.</li> </ul>"},{"location":"hpc-user-guide/request-account/#help-and-use-cases","title":"Help and Use Cases","text":"<ul> <li>Research Cluster FAQ</li> <li>See how some of your peers are making use of the Research Cluster and its resources.</li> </ul>"},{"location":"hpc-user-guide/software-cluster/","title":"Software On The Cluster","text":""},{"location":"hpc-user-guide/software-cluster/#modules","title":"Modules","text":"<ul> <li>A tool that simplifies shell initialization and lets users easily modify their environment during the session with modulefiles</li> <li>Each modulefile contains the information needed to configure the shell for an application. (PATH, LD_LIBRARY_PATH, CPATH, etc.)</li> <li>Modules are useful in managing different versions of applications. </li> <li>Modules can also be bundled into metamodules that will load an entire set of different applications (dependencies). </li> </ul> <p>Module  Commands</p> <ul> <li><code>module av</code> : to check all available modules</li> <li><code>module load</code> : to load a particular module</li> <li><code>module list</code> : to list modules that are loaded</li> <li><code>module purge</code> : purge any loaded modules</li> </ul>"},{"location":"hpc-user-guide/software-cluster/#check-all-available-modules","title":"Check All Available Modules","text":"<ul> <li>To check ALL available modules installed on the cluster:</li> </ul> <pre><code>module av\n</code></pre> <ul> <li>Upon login, environment <code>PATH</code> is set for the system to search executables:</li> </ul> <pre><code>echo $PATH\n</code></pre> <p>output</p> <pre><code>/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <ul> <li>For example, I would like to use <code>blast</code>, to check what versions of blast are available:</li> </ul> <pre><code>module av blast\n</code></pre> <p>output</p> <pre><code>---------------------- /opt/shared/Modules/modulefiles-rhel6 ----------------------\nblast/2.2.24 blast/2.2.31 blast/2.3.0  blast/2.8.1\n\n---------------------- /cluster/tufts/hpc/tools/module ----------------------------\nblast-plus/2.11.0\n</code></pre>"},{"location":"hpc-user-guide/software-cluster/#load-desired-module","title":"Load Desired Module","text":"<ul> <li>To load the version I would like to use, and use it:</li> </ul> <pre><code>module load blast-plus/2.11.0\n</code></pre>"},{"location":"hpc-user-guide/software-cluster/#check-which-modules-are-loaded","title":"Check Which Modules Are Loaded","text":"<ul> <li>To check which modules are loaded:</li> </ul> <pre><code>module list\n</code></pre> <p>output</p> <pre><code>Currently Loaded Modulefiles:\n    1) use.own     2) blast-plus/2.11.0\n</code></pre>"},{"location":"hpc-user-guide/software-cluster/#check-the-tool-version","title":"Check The Tool Version","text":"<ul> <li>To determine the tool version:</li> </ul> <pre><code>blastp -version\n</code></pre> <p>output</p> <pre><code>blastp: 2.11.0+\n Package: blast 2.11.0, build Aug 17 2021 06:29:22 \n</code></pre>"},{"location":"hpc-user-guide/software-cluster/#check-module-paths","title":"Check Module Paths","text":"<ul> <li>To determine the module paths:</li> </ul> <pre><code>which blastp\n</code></pre> <p>output</p> <pre><code>/cluster/tufts/hpc/tools/spack/linux-rhel7-ivybridge/gcc-9.3.0/blast-plus-2.11.0-ip4jcqabi3a2jscgusnkipvib6goy5mv/bin/blastp\n</code></pre> <pre><code>echo $PATH\n</code></pre> <p>output</p> <pre><code>/cluster/tufts/bio/tools/edirect:/cluster/tufts/hpc/tools/spack/linux-rhel7-ivybridge/gcc-9.3.0/blast-plus-2.11.0-ip4jcqabi3a2jscgusnkipvib6goy5mv/bin:/cluster/home/your_utln/.iraf/bin:/cluster/home/your_utln/.iraf/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin \n</code></pre>"},{"location":"hpc-user-guide/software-cluster/#to-unload-modules","title":"To Unload Modules","text":"<ul> <li>To unload a loaded module:</li> </ul> <pre><code>module unload blast-plus/2.11.0\n</code></pre> <pre><code>echo $PATH\n</code></pre> <p>output</p> <pre><code>/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <ul> <li>To unload ALL of the loaded modules:</li> </ul> <pre><code>module purge\n</code></pre> <pre><code>module list\n</code></pre> <p>output</p> <pre><code>No Modulefiles Currently Loaded.\n</code></pre>"},{"location":"hpc-user-guide/what-is-the-cluster/","title":"What is the Cluster?","text":"<p>Before getting to the cluster it is worth discussing what a cluster is and some of the terminology. First, let's discuss the difference between a CPU and a GPU.</p>"},{"location":"hpc-user-guide/what-is-the-cluster/#cpu-central-processing-unit","title":"CPU -- Central Processing Unit","text":"<ul> <li>A CPU can never be fully replaced by a GPU</li> <li>Can be thought of as the taskmaster of the entire system, coordinating a wide range of general-purpose computing tasks</li> </ul>"},{"location":"hpc-user-guide/what-is-the-cluster/#gpu-graphics-processing-unit","title":"GPU -- Graphics Processing Unit","text":"<ul> <li>GPUs were originally designed to create images for computer graphics and video game consoles</li> <li>Performing a narrower range of more specialized tasks</li> </ul> <p>You'll notice that in the picture above the CPU is composed of a smaller unit, a core. A core is the computing unit in a CPU. You'll also note that the whole system (including CPUs, GPUs and Storage) is a single computer in the system called a node.</p> <p></p> <p>When a CPU performs some computation they use a storage hierarchy. This hierarchy places small/fast storage options close to the CPU and slower/larger options away from the CPU. These small/fast options are called memory/RAM while the slower/larger options are simply called storage.</p> <p></p> <p>Now that we now the components we can put together an image of what a computer cluster is. A computer cluster is a group of loosely or tightly connected computers that work together as a single system. A HPC (High Performance Compute) cluster is a computer cluster capable of performing computations at high speeds.</p> <p></p>"},{"location":"machine-learning/intro-machine/","title":"Intro machine","text":""},{"location":"machine-learning/intro-machine/#introduction-to-machine-learning","title":"Introduction To Machine Learning","text":"<p>Machine learning, broadly speaking are algorithms and statistical models to analyze and draw inferences from patterns in data.  Here we find that we can use machine learning for a few broad tasks:</p> <ul> <li>Dimension Reduction to reduce the number of variables we need to consider</li> <li>Unsupervised Learning identify patterns in data that are unlabelled</li> <li>Classification predict which class label an observation might have</li> <li>Regression assess the relationship between variables </li> </ul> <p>The following cheatsheet is also useful to understand when to use what method:</p> <p></p>"},{"location":"machine-learning/intro-machine/#references","title":"References","text":"<ul> <li> <p>Oxford Languages</p> </li> <li> <p>SAS Blogs</p> </li> </ul>"},{"location":"machine-learning/introduction/","title":"Introduction To Machine Learning","text":""},{"location":"machine-learning/introduction/#setup-for-machine-learning-tutorials","title":"Setup For Machine Learning Tutorials","text":"<ul> <li>Setup</li> </ul>"},{"location":"machine-learning/introduction/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Dimension Reduction</li> <li>Clustering</li> <li>K-means Clustering</li> <li>Hierarchical Clustering</li> </ul>"},{"location":"machine-learning/introduction/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>Linear Models</li> <li>Logistic Regression</li> <li>Survival Analysis Part 1</li> <li>Survival Analysis Part 2</li> <li>Multivariate Regression</li> <li>Model Performance</li> </ul>"},{"location":"machine-learning/setup/","title":"Tutorial Setup","text":""},{"location":"machine-learning/setup/#setup","title":"Setup","text":"<p>For the following machine learning tutorials we will be using glioblastoma data from cBioPortal. Before getting started you will need:</p> <ul> <li>Account on Tufts HPC</li> <li>VPN if accessing the HPC from off campus</li> </ul>"},{"location":"machine-learning/setup/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p>"},{"location":"machine-learning/setup/#setting-up-a-project-space","title":"Setting Up A Project Space","text":"<p>We are going to open an interactive app:</p> RPython <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>32GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Default</code></li> <li><code>Load Supporting Modules</code>: <code>curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6</code></li> </ul> <p>Click on <code>Interactive Apps &gt; JupyterLab</code> and you will see a form to fill out to request compute resources to use JupyterLab on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>32GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Default</code></li> <li><code>Load Supporting Modules</code>: <code>curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6</code></li> </ul> <p>We will now need to create our project that we will work out of:</p> RPython <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. Now Create a new project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>machine-learning</code>)</li> <li><code>Create Project</code></li> </ol> <p>In terminal, start setting up your directories:</p> <pre><code>mkdir data\nmkdir scripts\nmkdir results\n</code></pre> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>cd data\nwget https://cbioportal-datahub.s3.amazonaws.com/gbm_cptac_2021.tar.gz\ntar -xvf gbm_cptac_2021.tar.gz cd ..\n</code></pre> <p>Click <code>Launch</code> and wait until your session is ready. Click <code>Connect to JupyterLab</code>, and you will notice a new window will pop up with JupyterLab. Now Create a new project:</p> <p>Open <code>Terminal</code> in the launcher window and start setting up your directories:</p> <pre><code>mkdir data\nmkdir scripts\nmkdir results\n</code></pre> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>cd data\nwget https://cbioportal-datahub.s3.amazonaws.com/gbm_cptac_2021.tar.gz\ntar -xvf gbm_cptac_2021.tar.gz cd ..\n</code></pre>"},{"location":"machine-learning/supervised/linear-model/","title":"Introduction To Linear Regression","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine-learning/supervised/linear-model/#linear-regression","title":"Linear Regression","text":"<p>In a regression model assesses the relationship between two quantitative variables by fitting a line to the data.  Using our glioblastoma data, we will assess the relationship between IDH1 gene expression (a gene commonly mutated in this type of cancer)  and TMB score (a measure of mutational burden). You can use a regression model to determine:</p> <ul> <li>how strong the relationship between these two variables is</li> <li>the value of the dependent variable given the independent variable</li> </ul> <p>A linear model follows the following formula:</p> \\[  y = \\beta_0 + \\beta_1 X + \\epsilon \\] <p>What do these terms mean?</p> <ul> <li>\\(y\\): dependent variable</li> <li>\\(\\beta_0\\): intercept (where \\(y\\) = 0)</li> <li>\\(\\beta_1\\): regression coefficient or slope</li> <li>\\(X\\): independent variable</li> <li>\\(\\epsilon\\): error or our estimate (what is the variation in our regression coefficient)</li> </ul> <p>This formula describes the best fit line for our data that tries to minimizes our error \\(\\epsilon\\):</p> <p></p>"},{"location":"machine-learning/supervised/linear-model/#pre-processing","title":"Pre-Processing","text":"RPython <p>Open an R script by going to <code>File</code> &gt; <code>New File</code> &gt; <code>R Script</code>. Save this script using a descriptive name, like <code>linear-model.R</code>. In the script use the following code:</p> <pre><code>## Loading our packages\n## Load the counts data\n## Load the meta data\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(caret)\n\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\") meta &lt;- read.csv(\nfile = \"data/gbm_cptac_2021/data_clinical_sample.txt\",\nskip=4,\nheader = T,\nsep = \"\\t\"\n)\n</code></pre> <p>Open a new notebook by going to <code>File</code> &gt; <code>New</code> &gt; <code>Notebook</code>. Save this script using a descriptive name, like <code>linear-model.ipynb</code>. In a code block enter:</p> <pre><code>## Import our libraries\n## Import our data set\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\ncounts = pd.read_csv(\n    'data/gbm_cptac_2021/data_mrna_seq_fpkm.txt' ,\n    sep = '\\t')\nmeta = pd.read_csv(\n    'data/gbm_cptac_2021/data_clinical_sample.txt' , \n    sep = '\\t',\n    skiprows=4)\n</code></pre> <p>Now we will need to do some data cleaning before we plug this into our model:</p> RPython <pre><code>## Change the patient id column to match \n## column names in the counts df\nmeta$PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)\n\n## grab IDH1 gene expression\nidh1 = counts %&gt;%\nfilter(Hugo_Symbol == \"IDH1\") %&gt;%\nselect(-Hugo_Symbol) %&gt;%\nt() %&gt;%\nas.data.frame() %&gt;%\nmutate(PATIENT_ID = rownames(.))\n\ncolnames(idh1) &lt;- c(\"IDH1\",\"PATIENT_ID\")\n\n## merge meta data and IDH1 \n## gene expression\nmerged &lt;- merge(\nmeta,\nidh1,\nby=\"PATIENT_ID\")\n</code></pre> <pre><code>## Grab IDH1 row and transpose \n## ensure that patient id is a column\n## and that IDH1 is the column name\nIDH1= counts.loc[counts[\"Hugo_Symbol\"] == \"IDH1\",]\nIDH1 = IDH1.T\nIDH1[\"PATIENT_ID\"] = IDH1.index\nIDH1.columns = [\"IDH1\",\"PATIENT_ID\"]\n\n## Grab TMB score\n## merge IDH1 gene expression and TMB score\n## merging this way ensures data are organized\n## by patient \nTMB= meta[['PATIENT_ID','TMB_NONSYNONYMOUS']]\nmerged=pd.merge(IDH1,TMB,on=\"PATIENT_ID\")\nmerged = merged.set_index('PATIENT_ID')\nmerged.head()\n</code></pre>"},{"location":"machine-learning/supervised/linear-model/#normalizecreate-the-model","title":"Normalize/Create the Model","text":"<p>These data, IDH1 gene expression and TMB score are on two different scales. To ensure a fair comparison of these variables we will normalize (or bring our data to a common scale) our data:</p> RPython <pre><code>## create a normalization function\n## apply this function to our data \n## to get data on a similar scale\nNormalizeData &lt;- function(data){\nnormalized = (data - min(data)) / (max(data) - min(data))\nreturn(normalized)\n}\n\nnorm = as.data.frame(\napply(merged %&gt;% select(IDH1,TMB_NONSYNONYMOUS), 2,NormalizeData)\n)\n</code></pre> <pre><code>## You might notice our data is on two \n## drastically different scales\n## We will normalize our data\n\ndef NormalizeData(data):\n    normalized = (data - np.min(data)) / (np.max(data) - np.min(data))\n    df = pd.DataFrame(normalized)\n    return normalized\n\nnorm = NormalizeData(merged)\n</code></pre> <p>Now we can fit our regression model!</p> RPython <pre><code>## fit our linear regression model\nmodel &lt;- lm(TMB_NONSYNONYMOUS ~ IDH1, data = norm)\n\n## let's plot our data\n## with the predicted values\nggplot(norm, aes(x=IDH1, y=TMB_NONSYNONYMOUS)) + geom_point() +\ntheme_bw() +\nylab(\"TMB\") +\nxlab(\"IDH1\") +\ngeom_smooth(method=lm)\n</code></pre> <p></p> <pre><code>## fit our linear regression model\ntmb = norm['TMB_NONSYNONYMOUS']\nidh1 = norm['IDH1'].astype(float)\nmodel = sm.OLS(tmb,idh1).fit()\n\n## Let's plot our data\n## along with the predictions from \n## our model\nplt.figure(figsize=(12, 6))\nplt.plot(norm['IDH1'], norm['TMB_NONSYNONYMOUS'], 'o') \nplt.plot(norm['IDH1'], model.predict(idh1), 'r', linewidth=2)\nplt.xlabel('IDH1')\nplt.ylabel('TMB NONSYNONYMOUS')\n\nplt.show()\n</code></pre> <p></p>"},{"location":"machine-learning/supervised/linear-model/#model-results","title":"Model Results","text":"<p>To assess our model we will generate a summary of some important metrics:</p> RPython <pre><code>summary(model)\n</code></pre> <pre><code>Call:\nlm(formula = TMB_NONSYNONYMOUS ~ IDH1, data = norm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.06145 -0.02361 -0.01212  0.00096  0.94101 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  0.06631    0.02216   2.992  0.00351 **\nIDH1        -0.03437    0.05362  -0.641  0.52305   \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nResidual standard error: 0.09938 on 97 degrees of freedom\nMultiple R-squared:  0.004218,    Adjusted R-squared:  -0.006048 \nF-statistic: 0.4109 on 1 and 97 DF,  p-value: 0.523\n</code></pre> <pre><code>model.summary()\n</code></pre> <pre><code>OLS Regression Results\nDep. Variable:  TMB_NONSYNONYMOUS   R-squared:   0.004\nModel:  OLS Adj. R-squared: -0.006\nMethod: Least Squares   F-statistic:    0.4109\nDate:   Fri, 02 Sep 2022    Prob (F-statistic): 0.523\nTime:   09:38:56    Log-Likelihood: 89.110\nNo. Observations:   99  AIC:    -174.2\nDf Residuals:   97  BIC:    -169.0\nDf Model:   1       \nCovariance Type:    nonrobust       \n            coef    std err t       P&gt;|t|   [0.025  0.975]\nIntercept   0.0663  0.022   2.992   0.004   0.022   0.110\nIDH1       -0.0344  0.054   -0.641  0.523   -0.141  0.072\nOmnibus:    202.245 Durbin-Watson:  2.072\nProb(Omnibus):  0.000   Jarque-Bera (JB):   28769.651\nSkew:   8.846   Prob(JB):   0.00\nKurtosis:   84.618  Cond. No.   6.12\n</code></pre> <p>Let's cover what a few of these mean:</p> RPython <ul> <li><code>Estimate</code> : the model's effect, so here we see that a one unit increase in IDH1 results in a 0.0344 decrease in </li> <li><code>Std. Error</code> : standard error of our estimate</li> <li><code>t value</code> : test stastic - the larger the statistic, the less likely this effect occured by chance</li> <li><code>Pr(&gt;|t|)</code> : pvalue that assesses the effect of IDH1 on TMB score if the null hypothesis of no effect were correct</li> <li><code>Multiple R-squared:</code> : \\(r^2\\) value of model - how much of the TMB variability is explained by IDH1 - here it is 0.4%</li> </ul> <ul> <li><code>coef</code> : the model's effect, so here a </li> <li><code>std</code> : standard error of our estimate</li> <li><code>t</code> : test stastic - the larger the statistic, the less likely this effect occured by chance</li> <li><code>P&gt;|t|</code> : pvalue that assesses the effect of IDH1 on TMB score if the null hypothesis of no effect were correct</li> <li><code>R-squared</code> : \\(r^2\\) value of model - how much of the TMB variability is explained by IDH1 - here it is 0.4%</li> </ul>"},{"location":"machine-learning/supervised/linear-model/#recap","title":"Recap","text":"<p>So what have we found? Well the model only explains 0.4% of our outcome variation and increased IDH1 expression seems to be associated with a minimal decrease in TMB. Additionally, our high p-value of <code>0.523</code> indicates that we should not reject the possibility that this effect was due to chance. </p>"},{"location":"machine-learning/supervised/linear-model/#assumptions","title":"Assumptions","text":"<p>So we found that IDH1 gene expression is not a great feature to model an outcome of TMB score. However, what if you do get a low p-value, a good \\(r^2\\) value, and a sizeable effect - are you in the clear? Not so fast, a linear model is limited by assumptions in our data:</p> <ul> <li>The data is normally distributed</li> <li>Homogeneity in the variances (Homoscedacity)</li> <li>Independence of your observations</li> </ul>"},{"location":"machine-learning/supervised/linear-model/#testing-assumptions","title":"Testing Assumptions","text":"<p>Mathematically, we can assess Normality and Homoscedacity with the shapiro test and Breusch-Pagan test, respectively:</p> RPython <pre><code>shapiro.test(norm$IDH1)\n</code></pre> <pre><code>Shapiro-Wilk normality test\n\ndata:  norm$IDH1\nW = 0.95137, p-value = 0.001093\n</code></pre> <pre><code>stats.shapiro(norm['IDH1'])\n</code></pre> <pre><code>(0.9513656497001648, 0.0010935224127024412)\n</code></pre> <p>We note here a pvalue (highlighted) of less than <code>0.05</code> which indicates that our data deviates from a normal distribution. Now how about our homoscedacity?</p> RPython <pre><code>lmtest::bptest(model)\n</code></pre> <pre><code>studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.72081, df = 1, p-value = 0.3959\n</code></pre> <pre><code>names = ['Lagrange multiplier statistic', 'p-value',\n    'f-value', 'f p-value']\ntest = sms.het_breuschpagan(model.resid, model.model.exog)\nlzip(names,test)\n</code></pre> <pre><code>[('Lagrange multiplier statistic', 0.7208106123366453),\n('p-value', 0.39587814229097884),\n('f-value', 0.7114286333891047),\n('f p-value', 0.4010451501525186)]\n</code></pre> <p>Here we see that our pvalue is well above <code>0.05</code> indicating there is not enough evidence to reject the null hypothesis that the variance of the residuals are constant - i.e. we do have Homogeneity in our variances. </p>"},{"location":"machine-learning/supervised/linear-model/#references","title":"References","text":"<ul> <li>scribbr</li> <li>STHDA</li> <li>Medium</li> </ul>"},{"location":"machine-learning/supervised/logistic-regression/","title":"Introduction To Logistic Regression","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine-learning/supervised/logistic-regression/#logistic-regression","title":"Logistic Regression","text":"<p>A logistic regression model attempts to classify, so for example can ALDH3A1 gene expression to predict smoking status? So how do we do this? We could fit a linear model to our data but we would end up with something like this:</p> <p></p> <p>Here we see that if we used a linear model, we'd end up predicting values that are not our two catetories - smoker or non-smoker (and on the graph 0 or 1). So how do we fit our data when it is binary? We use a sigmoid function:</p> \\[ p(X) = \\frac{ e^{\\beta_{0} + \\beta_{1}X} }{1 + e^{\\beta_{0} + \\beta_{1}X} } \\] <p>What do these terms mean?</p> <ul> <li>\\(p(X)\\) : probability of smoking status given ALDH3A1 gene expression</li> <li>\\(X\\) : ALDH3A1 gene expression</li> <li>\\(\\beta_{0}\\) : y intercept</li> <li>\\(\\beta_{1}\\) : slope of our line</li> </ul> <p>This sigmoid function creates our S-shaped curve! However we'd like our probability to be linear relationship with X. So we can manipulate this equation to get:</p> \\[ \\frac{p(X)}{1 - p(X)} = e^{\\beta_{0} + \\beta_{1}X}\\] <p>Where \\(\\frac{p(X)}{1 - p(X)}\\) is known as the odds ratio and this can range from \\(0\\) to \\(\\infty\\). However, again this doesn't match our 0 to 1 scale, so we take the log of both sides to get:</p> \\[ log(\\frac{p(X)}{1 - p(X)}) = \\beta_{0} + \\beta_{1}X \\] <p>Here we get \\(log(\\frac{p(X)}{1 - p(X)})\\) or the logit function - where a one unity increase in \\(X\\) increases \\(p(X)\\) by \\(\\beta_{0}\\). </p>"},{"location":"machine-learning/supervised/logistic-regression/#pre-processing","title":"Pre-processing","text":"<p>First let's do some preprocessing to get our combine ALDH3A1 gene expression with smoking status:</p> RPython <pre><code>## load our libraries via our library path\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggplot2)\n\n## load our counts and meta data\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\") meta &lt;- read.csv(\nfile = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\nskip=4,\nheader = T,\nsep = \"\\t\"\n)\n\n## ensure our patient ID's match between \n## the counts and meta data\nmeta$PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)\n\n## grab IDH1 gene expression and \n## patient ID \naldh3a1 = counts %&gt;%\nfilter(Hugo_Symbol == \"ALDH3A1\") %&gt;%\nselect(-Hugo_Symbol) %&gt;%\nt() %&gt;%\nas.data.frame() %&gt;%\nmutate(PATIENT_ID = rownames(.))\n\ncolnames(aldh3a1) &lt;- c(\"aldh3a1\",\"PATIENT_ID\")\n\n## merge counts and meta data\nmerged &lt;- merge(\nmeta,\naldh3a1,\nby=\"PATIENT_ID\")\n\n## create smoking status variable\n## and normalize ALDH3A1 expression\nmerged &lt;- merged %&gt;%\nmutate(smoking = ifelse(grepl(\"non-smoker\",SMOKING_HISTORY),0,1)) %&gt;%\nmutate(aldh3a1 = log2(aldh3a1+1))\n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine-learning/supervised/logistic-regression/#visualizing-data","title":"Visualizing Data","text":"<p>Now let's take a look at our data:</p> RPython <pre><code>## let's plot our data\nggplot(merged, aes(x=aldh3a1, y=smoking)) + geom_point() +\ntheme_bw() +\nylab(\"Smoking Status\") +\nxlab(\"ALDH3A1 Gene Expression\") </code></pre> <p></p> <pre><code>\n</code></pre> What problem do you see with creating a logistic regression model with this data? <p>There is no clear separation of ALDH3A1 gene expression between smokers and non-smokers.</p>"},{"location":"machine-learning/supervised/logistic-regression/#create-the-model","title":"Create the Model","text":"<p>We see that there isn't a great delineation between the two conditions - smoking and non-smoking. But let's creat our logistic regression model to confirm:</p> RPython <pre><code>## let's make our model\nmodel &lt;- glm( smoking ~ aldh3a1, data = merged, family = binomial)\nsummary(model) </code></pre> <pre><code>Call:\nglm(formula = smoking ~ aldh3a1, family = binomial, data = merged)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0299  -0.9776  -0.9364   1.3867   1.4800  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.15978    1.61680   0.099    0.921\naldh3a1     -0.05947    0.13897  -0.428    0.669\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 127.95  on 96  degrees of freedom\nResidual deviance: 127.77  on 95  degrees of freedom\n  (2 observations deleted due to missingness)\nAIC: 131.77\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <p>Here we note:</p> <ul> <li><code>Estimate</code> : the model's effect, so here we see that a one unit increase in ALDH3A1 results in a 0.05947 decrease in the probability of being a smoker</li> <li><code>Std. Error</code> : standard error of our estimate</li> <li><code>z value</code> : test stastic - the larger the statistic, the less likely this effect occured by chance</li> <li><code>Pr(&gt;|z|)</code> : pvalue that assesses the effect of ALDH3A1 on Smoking status if the null hypothesis of no effect were correct</li> </ul> <p>You will note that there is not an \\(r^2\\) value for this model like we saw in the linear regression tutorial. We will need a special metric known as McFadden's \\(r^2\\):</p> <pre><code>pscl::pR2(model)[\"McFadden\"]\n</code></pre> <pre><code>fitting null model for pseudo-r2\n   McFadden \n0.001440711 \n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine-learning/supervised/logistic-regression/#visualizing-predictions","title":"Visualizing Predictions","text":"<p>We will now plot our logistic regression curve:</p> RPython <pre><code>## let's visualize our predictions\nggplot(merged, aes(x=aldh3a1, y=smoking)) + geom_point() +\ntheme_bw() +\nylab(\"Smoking Status\") +\nxlab(\"ALDH3A1 Gene Expression\") +\ngeom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) </code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"machine-learning/supervised/logistic-regression/#recap","title":"Recap","text":"<p>So what have we found? Well it seems that ALDH3A1 expression has a minimal effect on the probability of being a smoker. Additionally, our high p-value of 0.669 indicates that we should not reject the possibility that this effect was due to chance. This is also confirmed in our visual of the logistic regression curve - where it is not S shaped. This is due to the absense of some boundry, where some value of ALDH3A1 expression separates smokers and non smokers. We can also see this fit isn't great in McFadden's \\(r^2\\) value of <code>0.001440711</code>. McFadden's \\(r^2\\) values close to 0 are indicative of a poor fit while values over 0.4 are indicative of a good fit. </p>"},{"location":"machine-learning/supervised/logistic-regression/#assumptions","title":"Assumptions","text":"<p>When we create a logistic regression model we need to be aware of the limitations:</p> <ul> <li>the response variable must be binary </li> <li>the observations are independent (i.e. observations can't influence the response of other observations)</li> <li>no multicollinearity (will be covered the multivariate regression tutorial) - if you have multiple dependent variables they cannot be correlated</li> <li>no outliers</li> <li>there must be a linear relationship between the logit function and the dependent variable </li> </ul> <p>While we will cover multicollinearity in the multivariate regression tutorial, we will assess here whether or not there is a linear relationship between the logit function and the dependent variable using the Box-Tidwell test:</p> RPython <pre><code>## run the box tidwell test on\n## our model\ncar::boxTidwell(model$linear.predictors ~ merged$aldh3a1[!is.na(merged$aldh3a1)])\n</code></pre> <pre><code> MLE of lambda Score Statistic (z) Pr(&gt;|z|)\n         1              0.7139     0.4753\n\niterations =  0 \n</code></pre> <pre><code>\n</code></pre> <p>Here we note that the probability is above 0.05, implying that ALDH3A1 expression and the logit function are linearly related. </p>"},{"location":"machine-learning/supervised/logistic-regression/#references","title":"References","text":"<ul> <li>datacamp</li> <li>STHDA</li> <li>Statology</li> </ul>"},{"location":"machine-learning/supervised/model-performance/","title":"Introduction To Model Performance","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine-learning/supervised/model-performance/#training-and-testing-data","title":"Training and Testing Data","text":""},{"location":"machine-learning/supervised/model-performance/#model-error","title":"Model Error","text":""},{"location":"machine-learning/supervised/model-performance/#sensitivity-v-specificity","title":"Sensitivity v. Specificity","text":""},{"location":"machine-learning/supervised/model-performance/#area-under-the-curve","title":"Area Under The Curve","text":""},{"location":"machine-learning/supervised/model-performance/#references","title":"References","text":""},{"location":"machine-learning/supervised/multivariate-regression/","title":"Introduction To Multivariate Regression","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine-learning/supervised/multivariate-regression/#multivariate-regression","title":"Multivariate Regression","text":"<p>In the linear regression and logistic regression tutorials we cover univariate modelling -  or modelling with one variable. Here we discuss how to create multivariate models, or models with multiple independent variables. </p>"},{"location":"machine-learning/supervised/multivariate-regression/#pre-processing","title":"Pre-Processing","text":"RPython <pre><code>## load our libraries via our library path\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggplot2)\n\n## load our counts and meta data\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\") meta &lt;- read.csv(\nfile = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\nskip=4,\nheader = T,\nsep = \"\\t\"\n)\n\n## ensure our patient ID's match between \n## the counts and meta data\nmeta$PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)\n\n## grab IDH1 gene expression and \n## patient ID \naldh3a1 = counts %&gt;%\nfilter(Hugo_Symbol == \"ALDH3A1\") %&gt;%\nselect(-Hugo_Symbol) %&gt;%\nt() %&gt;%\nas.data.frame() %&gt;%\nmutate(PATIENT_ID = rownames(.))\n\ncolnames(aldh3a1) &lt;- c(\"aldh3a1\",\"PATIENT_ID\")\n\n## merge counts and meta data\nmerged &lt;- merge(\nmeta,\naldh3a1,\nby=\"PATIENT_ID\")\n\n## create smoking status variable\n## and normalize ALDH3A1 expression\nmerged &lt;- merged %&gt;%\nmutate(smoking = ifelse(grepl(\"non-smoker\",SMOKING_HISTORY),0,1)) %&gt;%\nmutate(aldh3a1 = log2(aldh3a1+1))\n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine-learning/supervised/multivariate-regression/#build-a-multivariate-model","title":"Build a Multivariate Model","text":"RPython <pre><code>## build several multivariate models\nmodel1 &lt;- glm( smoking ~ AGE, data = merged, family = binomial)\nmodel2 &lt;- glm( smoking ~  AGE + WEIGHT , data = merged, family = binomial)\nmodel3 &lt;- glm( smoking ~  AGE + WEIGHT + BMI , data = merged, family = binomial)\nAIC(model1,\nmodel2,\nmodel3)\nBIC(model1,\nmodel2,\nmodel3)\n</code></pre> <pre><code>       df      AIC\nmodel1  2 130.1894\nmodel2  3 121.4100\nmodel3  4 123.2545\n\n       df      BIC\nmodel1  2 135.3797\nmodel2  3 129.1953\nmodel3  4 133.6350\n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine-learning/supervised/multivariate-regression/#aic-bic","title":"AIC &amp; BIC","text":"<p>Above, we created several logistic regression models with different numbers of variables (separated by a <code>+</code> sign). We also display the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) values for the different models. So what are these? These are values are a way of summarizing our models and seeing at what point adding too many variables interferes rather that helps. The lower the AIC/BIC, the better the model. Above we note that model 2 has the lowest AIC/BIC and when we add the third term, <code>BMI</code>, we raise our criterion values. </p>"},{"location":"machine-learning/supervised/multivariate-regression/#overfitting","title":"Overfitting","text":"<p>So why might adding more variables hurt our model if it can help improve our model metrics? Well, while adding more variables can help improve our model's metrics, it doesn't necessarily mean it can help improve our model's ability to predict. Here is a visual:</p> <p></p> <p>Here we see that when a model is under fit, it doesn't necessary follow the direction of the data. On the other hand, if we are too good at predicting our data set, we trade off with the ability to predict new data. </p>"},{"location":"machine-learning/supervised/multivariate-regression/#multicollinearity","title":"Multicollinearity","text":"<p>Additionally, we need to think carefully about the variables we are plugging into our model. Each variable should add new information and should not correlate with one another. Let's try to determine this visually:</p> RPython <pre><code>## grab our correlations\n## plot these correlations\ncors &lt;- cor(merged %&gt;% select(AGE,WEIGHT,BMI))\ncorrplot::corrplot(cors)\n</code></pre> <p></p> <pre><code>\n</code></pre> <p>We can also use the variance inflation factor (VIF) to assess multicollinearity - with values between 5 and 10 indicating multicollinearity:</p> RPython <pre><code>## use the vif function to\n## assess multicollinearity\ncar::vif(model2)\ncar::vif(model3)\n</code></pre> <pre><code>     AGE   WEIGHT \n1.106764 1.106764 \n     AGE   WEIGHT      BMI \n1.108918 3.265643 3.123681 \n</code></pre> <pre><code>\n</code></pre> <p>Here we see that while the VIF values of <code>WEIGHT</code> and <code>BMI</code> do not go over 5, we see in the correlation plot that these variables are indeed highly correlated. Also, when <code>BMI</code> is added to <code>model3</code> we see that the VIF values for <code>WEIGHT</code> and <code>BMI</code> are bumped up closer to 5. </p> Why do you think that BMI has a lower VIF than WEIGHT? <p>Weight is used to calculate BMI. However, BMI also captures information about height which isn't captured by our WEIGHT variable.</p>"},{"location":"machine-learning/supervised/multivariate-regression/#references","title":"References","text":"<ul> <li>STHDA</li> <li>Towards Data Science - overfitting</li> <li>Towards Data Science - logistic</li> </ul>"},{"location":"machine-learning/supervised/surv-part1/","title":"Survival Analysis Part 1","text":""},{"location":"machine-learning/supervised/surv-part1/#survival-analysis-part-1","title":"Survival Analysis Part 1","text":"<p>Survival Analysis refers to a statistical framework to assess the time it takes for some event of interest to occur. In medical research this framework is often used to ask questions about:</p> <ul> <li>a patient's probability of survival given some time period</li> <li>the characteristics that might impact survival</li> <li>the differences in survival between groups</li> </ul>"},{"location":"machine-learning/supervised/surv-part1/#events-and-censoring","title":"Events and Censoring","text":"<p>The type of event can include:</p> <ul> <li>Relapse</li> <li>Death </li> <li>Progression</li> </ul> <p>Now not all patients in a study will experience the same type of event described above and as such these observations need to be censored as to not  influence our results. Censoring can be caused by:</p> <ul> <li>a patient not yet experiencing an event of interest (Relapse, Death, Progression, etc.)</li> <li>a patient being lost to follow up during the study (meaning they drop out for one reason or another)</li> <li>a patient expreiences an alternative event to the event of interest</li> </ul> <p>Given that these are assessed at the end of the study, this is referred to as right censoring.</p>"},{"location":"machine-learning/supervised/surv-part1/#kaplan-meier-survival-probability","title":"Kaplan-Meier Survival Probability","text":"<p>The probability that the event of interest will not happen in a given time period is referred to as the survival probability. We can calculate this using the non-parametric Kaplan-Meier method:</p> \\[S(t_i) = S({t_i} - 1) (1 - \\frac{d_i}{n_i})\\] \\[S(0) = 1\\] \\[t_0 = 0\\] <p>Explanation of Terms</p> <ul> <li>\\(S(t_i)\\) Survival probability at time \\(t_i\\)</li> <li>\\(S({t_i} - 1)\\) Survival probability at time \\({t_i} - 1\\)</li> <li>\\(n_i\\) number of patients that have not experienced event of interest right before time \\(t_i\\)</li> <li>\\(d_i\\) number of events of interest at \\(t_i\\)</li> <li>\\(t_0 = 0\\) your starting time must be 0</li> <li>\\(S(0) = 1\\) your probability of survival at time 0 is 1 </li> </ul>"},{"location":"machine-learning/supervised/surv-part1/#pre-processing","title":"Pre-Processing","text":"<p>Let's start by loading our meta data and ensuring that we have a censored column:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(\"survival\")\nlibrary(\"survminer\")\n\n# load in patient meta data\nmeta &lt;- read.csv(\nfile = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\nskip=4,\nheader = T,\nsep = \"\\t\"\n)\n\n# create a censored status column\n# 1 being they are censored\n# 2 being they are not censored\nmeta$status &lt;- ifelse(\n(meta$LOST_TO_FOLLOW_UP == \"Yes\" &amp; meta$VITAL_STATUS == \"Living\"),\n1,\n2\n)\n</code></pre>"},{"location":"machine-learning/supervised/surv-part1/#fitting-a-survival-curve","title":"Fitting a Survival Curve","text":"<p>To create our survival curve we will fit a model to determine: What is the survival probability of male and female patients and how do these curves compare?</p> <pre><code># fit our survival curve to our data\n# time is days\n# status is our censor status\n# and SEX is our variable of interest\nfit &lt;- survfit(\nSurv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX,\ndata = meta)\n\n# examine our fit\nsummary(fit)$table\n</code></pre> <pre><code>           records n.max n.start events   *rmean *se(rmean) median 0.95LCL 0.95UCL\nSEX=Female      27    27      27     27 308.0741   47.10662    274     147     401\nSEX=Male        35    35      35     35 399.2286   39.34208    381     294     511\n</code></pre> <p>What does this table tell us?</p> <ul> <li>First we start off with 27 females and 35 males</li> <li>We then see that we observe the event (death) 27 times in females and 35 times in males</li> <li>the mean survival time in days and it's standard error (<code>*rmean</code> and <code>*se(rmean)</code>)</li> <li>the median survival time in days (<code>median</code>)</li> <li>the confidence interval around our parameter</li> </ul>"},{"location":"machine-learning/supervised/surv-part1/#visualizing-survival-curves","title":"Visualizing Survival Curves","text":"<p>Now if we want to visualize these survival curves we can use the following code:</p> <pre><code># plot the survival curves\nggsurvplot(fit,\npval = TRUE, # add in a p-value\nconf.int = TRUE, # add in a confidence interval\nrisk.table = TRUE, # add in a risk table to our plot\nrisk.table.col = \"strata\", # color the risk table by group\nlinetype = \"strata\", # ensure lines are made per group\nsurv.median.line = \"hv\", # add in median survival time line\nggtheme = theme_bw(), # set theme to black and white\npalette = c(\"#C06C84\",\"#355C7D\")) # change colors of lines\n</code></pre> <p></p> <p>What does this graph tell us?</p> <ul> <li>Here we see time against survival probability</li> <li>We also note a risk table down below that lists the number of individuals at risk for the event</li> <li>Given a p-value above 0.05 we do not have enough evidence to rule out the possibility that any difference between the male and female survival curve is due to chance. </li> </ul>"},{"location":"machine-learning/supervised/surv-part1/#log-rank-test","title":"Log-Rank Test","text":"<p>We can see above that we asked is there any significant difference between these two curves. The test conducted to determine this is called the log rank-test. This test is non-parametric, meaning it does not make any assumptions about the distribution of survival times. If we were solely interested in testing the difference between these two curves we could use the following code:</p> <pre><code>survdiff(Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX, data = meta)\n</code></pre> <pre><code>Call:\nsurvdiff(formula = Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX, \n    data = meta)\n\nn=62, 37 observations deleted due to missingness.\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nSEX=Female 27       27     22.6     0.876      1.41\nSEX=Male   35       35     39.4     0.501      1.41\n\n Chisq= 1.4  on 1 degrees of freedom, p= 0.2 \n</code></pre> <p>What do these results mean?</p> <ul> <li>Here we see that we started with 99 observations but ended up with 37 after removing our censored data points</li> <li>Our chi-square test statistic was 1.4</li> <li>Our p-value is 0.2, again indicating we do not have enough evidence to rule out the possibility that any difference between the male and female survival curve is due to chance. </li> </ul>"},{"location":"machine-learning/supervised/surv-part1/#references","title":"References","text":"<ol> <li>Survival Analysis Basics</li> <li>Survival Analysis Part I: Basic concepts and first analyses</li> <li>Nonparametric Estimation from Incomplete Observations</li> <li>Survival plots of time-to-event outcomes in clinical trials: good practice and pitfalls</li> </ol>"},{"location":"machine-learning/supervised/surv-part2/","title":"Survival Analysis Part 2","text":""},{"location":"machine-learning/supervised/surv-part2/#survival-analysis-part-2","title":"Survival Analysis Part 2","text":"<p>In the first Survival Analysis topic note we assessed survival probability curves and if two different survival curves were significantly different. Instead of survival probability, here we will discuss hazard probability, or the probability that an individual has an event at some time \\(t\\). Survival probability on the other hand, refers to the opposite - that the individual will survive to time \\(t\\). To define the hazard probability we will use the following function:</p> \\[h(t)=h_0(t)\u00d7exp(b_{1}x_{1}+b_{2}x_{2}+...+b_{n}x_{n})\\] <p>Explanation of Terms</p> <ul> <li>\\(t\\) the survival time</li> <li>\\(h(t)\\) the hazard probability</li> <li>\\(n\\) different number of covariates \\(x\\) (so variables like age, sex, etc.)</li> <li>\\(b\\) are the coefficients of the impact of each of these covariates </li> <li>\\(h_0(t)\\) would be the baseline hazard</li> </ul> <p>These \\(b\\) coefficients can also be referred to as hazard ratios. And they can be interpreted like so:</p> <p>Hazard Ratio Interpretation</p> <ul> <li>Hazard Ratio &lt; 1: there is a decrease in the hazard</li> <li>HHazard RatioR &gt; 1: there is a increase in the hazard</li> <li>Hazard Ratio = 1: there is no effect on the hazard</li> </ul>"},{"location":"machine-learning/supervised/surv-part2/#pre-processing","title":"Pre-Processing","text":"<p>We will start as we did in the first Survival Analysis topic note and load our libraries, data, and add a censor status column:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(\"survival\")\nlibrary(\"survminer\")\n\n# load in patient meta data\nmeta &lt;- read.csv(\nfile = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\nskip=4,\nheader = T,\nsep = \"\\t\"\n)\n\n# create a censored status column\n# 1 being they are censored\n# 2 being they are not censored\nmeta$status &lt;- ifelse(\n(meta$LOST_TO_FOLLOW_UP == \"Yes\" &amp; meta$VITAL_STATUS == \"Living\"),\n1,\n2\n)\n</code></pre>"},{"location":"machine-learning/supervised/surv-part2/#cox-proportional-hazards-model","title":"Cox Proportional-Hazards Model","text":"<pre><code># run the Cox Proportional-Hazards Model on our data \n# with a few covariates and get a summary\ncox &lt;- coxph(Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX + AGE + BMI,\ndata = meta)\nsummary(cox)\n</code></pre> <pre><code>Call:\ncoxph(formula = Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX + \n    AGE + BMI, data = meta)\n\n  n= 62, number of events= 62 \n   (37 observations deleted due to missingness)\n\n             coef exp(coef)  se(coef)      z Pr(&gt;|z|)\nSEXMale -0.300913  0.740143  0.266021 -1.131    0.258\nAGE     -0.003842  0.996166  0.011765 -0.327    0.744\nBMI     -0.006180  0.993839  0.025223 -0.245    0.806\n\n        exp(coef) exp(-coef) lower .95 upper .95\nSEXMale    0.7401      1.351    0.4394     1.247\nAGE        0.9962      1.004    0.9735     1.019\nBMI        0.9938      1.006    0.9459     1.044\n\nConcordance= 0.564  (se = 0.041 )\nLikelihood ratio test= 1.52  on 3 df,   p=0.7\nWald test            = 1.55  on 3 df,   p=0.7\nScore (logrank) test = 1.56  on 3 df,   p=0.7\n</code></pre> <p>What Does this mean?</p> <ul> <li>Here we note that all the coefficients for SEX(being male), AGE, and BMI have slight negative coefficients or hazard ratios</li> <li>We also note that no p-value is below 0.05 indicating that any observed effect on our hazard (death) could be due to chance</li> <li>We also note that sex has the most significant p-value and has the largest magnitude of all the coefficients</li> </ul>"},{"location":"machine-learning/supervised/surv-part2/#assumptions","title":"Assumptions","text":"<p>Before accepting the results of this model we should discuss the assumptions of the Cox Proportional-Hazards Model</p> <p>Assumptions of the Cox Proportional-Hazards Model</p> <ul> <li>the hazard curves different groups should be proportional and not cross each other</li> <li>There are no outliers in our data</li> <li>The relationship between the log hazard and the covariates must be linear</li> </ul> <p>Testing Proportionality in Hazards</p> <pre><code># test proportional hazards assumption\ncox_test &lt;- cox.zph(cox)\nggcoxzph(cox_test)\n</code></pre> <p></p> <p>What does this mean?</p> <ul> <li>Here we see that the global test is above 0.05, indicating we have not violated the proportional hazards assumption</li> </ul> <p>Testing for Outliers</p> <pre><code># test the outlier assumption\nggcoxdiagnostics(cox,\ntype = \"dfbeta\",\nlinear.predictions = FALSE, ggtheme = theme_bw())\n</code></pre> <p></p> <p>What does this mean?</p> <ul> <li>Here we see that there are a few outliers as the patterns of the residuals, especially for BMI, are not spread equally around 0. </li> </ul> <p>Testing for Linearity</p> <p>NEEDS MORE EXPLANATION HERE</p> <pre><code># test for non-linearity for AGE\nggcoxfunctional(Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ AGE + log(AGE) + sqrt(AGE),\ndata = meta)\n</code></pre> <p></p>"},{"location":"machine-learning/supervised/surv-part2/#references","title":"References","text":"<ol> <li>Cox Proportional-Hazards Model</li> <li>Regression Models and Life-Tables</li> <li>Survival Analysis Part II: Multivariate data analysis \u2013 an introduction to concepts and methods</li> </ol>"},{"location":"machine-learning/unsupervised/clustering/","title":"Distance Metrics","text":""},{"location":"machine-learning/unsupervised/clustering/#clustering","title":"Clustering","text":"<p>Clustering, like the name implies, is a way to group data points to find patterns. Clustering is type of unsupervised learning - where patterns are discovered without labeled data. </p>"},{"location":"machine-learning/unsupervised/clustering/#distance-metrics","title":"Distance Metrics","text":"<p>Before generating clusters we need to calculate the distance between observations:</p> <p></p> <p>To do so we have a few different options for distance metrics:</p> <p>Euclidean Distance</p> \\[d_{euc}(x,y) = \\sqrt{\\sum_{i=1}^n{(x_i - y_i)^2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Manhattan Distance</p> \\[d_{man}(x,y) = \\sum_{i=1}^n{|(x_i - y_i)|}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Eisen Cosine Correlation Distance</p> \\[d_{eis}(x,y) = 1 - \\frac{|\\sum_{i=1}^n{x_iy_i}|}{\\sqrt{\\sum_{i=1}^n {x_i^2}\\sum_{i=1}^n {y_i^2}}}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Pearson Correlation Distance </p> \\[d_{pearson}(x,y) = 1 - \\frac{\\sum_{i=1}^n{(x - \\mu_x)(y - \\mu_y)}}{\\sqrt{\\sum_{i=1}^n{(x - \\mu_x)^2} \\sum_{i=1}^n{(y - \\mu_y)^2}}} \\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(\\mu_x\\) mean of variable x</li> <li>\\(\\mu_y\\) mean of variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Spearman Correlation Distance </p> \\[d_{spearman}(x,y) = 1 - \\frac{\\sum{(x\\prime - \\mu_{x\\prime} )(y\\prime  - \\mu_{y\\prime} )}}{\\sqrt{\\sum{(x\\prime  - \\mu_{x\\prime} )^2} \\sum{(y\\prime  - \\mu_{y\\prime} )^2}}}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(\\mu_x\\) mean of variable x</li> <li>\\(\\mu_y\\) mean of variable y</li> <li>\\(n\\) number of observations</li> </ul>"},{"location":"machine-learning/unsupervised/clustering/#distance-metrics-in-r","title":"Distance Metrics In R","text":"<p>Let's try creating a distance matrix!</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(factoextra)\n\n# load our counts data\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\")\n\n# make the genes our rownames\nrownames(counts) &lt;- make.names(counts$Hugo_Symbol,unique = TRUE)\n\n# remove the gene symbol column\ncounts &lt;- counts %&gt;%\nselect(-c(Hugo_Symbol)) # log2 transform our data \n# transpose our data so that our patients are rows\ncounts &lt;- t(log2(counts + 1))\n\n# Change NA counts to 0\ncounts[!is.finite(counts)] &lt;- 0\n\n# generate correlation distance matrix\ndist &lt;- get_dist(counts,method = \"pearson\")\n\n# plot correlation distance matrix\nfviz_dist(dist) +\ntheme(axis.text = element_text(size = 3)) +\nlabs(\ntitle = \"Pearson Correlation Distances Between Samples\",\nfill = \"Pearson Correlation\"\n)\n</code></pre> <p></p>"},{"location":"machine-learning/unsupervised/clustering/#references","title":"References","text":"<ol> <li>Clustering Distance Measures</li> <li>K-Means Clustering in R: Algorithm and Practical Examples</li> <li>Agglomerative Hierarchical Clustering</li> <li>Distance Method Formulas</li> </ol>"},{"location":"machine-learning/unsupervised/dimension-reduction/","title":"Introduction To Dimension Reduction","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul> <ul> <li>Process of reducing the number of variables to a set of principal values where variation in your data becomes apparent. Here is an example with three dimensions:</li> </ul> <p> </p> Dimension Reduction Example <ul> <li>Here we see that most of the variation is visible along the x-y axes</li> <li> <p>So what are the advantages:</p> <ul> <li>simplification</li> <li>denoising</li> <li>variable selection</li> <li>visualization</li> </ul> </li> </ul>"},{"location":"machine-learning/unsupervised/dimension-reduction/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<ul> <li>PCA works by summarizing a set of continuous (quantitative) multivariate (multiple variable) data into a set of linearly uncorrelated variables called principal components.</li> </ul>"},{"location":"machine-learning/unsupervised/dimension-reduction/#pros","title":"Pros","text":"<ul> <li>can be used on large data</li> <li>can be used with sparse data</li> <li>preserves the structure (reproducible)</li> </ul>"},{"location":"machine-learning/unsupervised/dimension-reduction/#cons","title":"Cons","text":"<ul> <li>if one variable is on a different scale (like kg instead of g) it can bias the results. So ensure data is on one scale!</li> <li>points can be crowded with large numbers of observations and reveal no pattern</li> <li>susceptible to outliers</li> </ul>"},{"location":"machine-learning/unsupervised/dimension-reduction/#pre-processing","title":"Pre-Processing","text":"<p>Let's try this in code! First we will need to do some preprocessing:</p> RPython <pre><code>## load our libraries via our library path\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(ggplot2)\nlibrary(missMDA)\nlibrary(patchwork)\n\n## load counts/meta data\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\")\n\nmeta &lt;- read.csv(\nfile = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\nskip=4,\nheader = T,\nsep = \"\\t\"\n)\n\n## ensure patient IDs match \n## patient IDs in counts data\nmeta &lt;- meta %&gt;%\nmutate(PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)) %&gt;%\ncolumn_to_rownames(\"PATIENT_ID\")\n</code></pre> <pre><code># still in development - sorry!\n</code></pre>"},{"location":"machine-learning/unsupervised/dimension-reduction/#normalization","title":"Normalization","text":"<p>Now we will ensure our data are on a common scale by log2 transforming it. This will ensure that we don't bias our PCA in the direction of higher magnitude variables. We will also select the top 50 genes with the highest variance as usually high variance genes are more biologically interesting. </p> RPython <pre><code>## log2 normalize our data\nnorm = log2(counts %&gt;% select(-Hugo_Symbol)+1)\n\n## extract variances\nvars = apply(\ncounts %&gt;% select(-Hugo_Symbol),\n1, function(x){return(var(x,na.rm = T))})\n\n## select the genes with the\n## top 50 variances\nselected &lt;- norm %&gt;%\nfilter(rank(-vars)&lt;=50) %&gt;%\nmutate(gene = counts$Hugo_Symbol[rank(-vars)&lt;=50]) %&gt;%\ncolumn_to_rownames(\"gene\") %&gt;%\nt() %&gt;%\nmerge(.,meta,by=\"row.names\",all=TRUE) %&gt;%\ncolumn_to_rownames(\"Row.names\")\n</code></pre> <pre><code># still in development - sorry!\n</code></pre>"},{"location":"machine-learning/unsupervised/dimension-reduction/#pca-plot-interpretation","title":"PCA Plot Interpretation","text":"RPython <pre><code>## run PCA and extract eigenvalues\npca &lt;- PCA(selected[,1:50],graph = FALSE)\nhead(get_eig(pca))\n\n## visualize our eigenvalues/principal components\nfviz_screeplot(pca,addlabels = TRUE)\n</code></pre> <pre><code>      eigenvalue variance.percent cumulative.variance.percent\nDim.1   9.395012        18.790023                    18.79002\nDim.2   6.177962        12.355925                    31.14595\nDim.3   3.924839         7.849677                    38.99563\nDim.4   3.734111         7.468223                    46.46385\nDim.5   2.917162         5.834324                    52.29817\nDim.6   2.352690         4.705380                    57.00355\n</code></pre> <pre><code># still in development - sorry!\n</code></pre> <p>Here we display our our principal components (<code>Dim.1</code>,<code>Dim.2</code>, etc.) and their eigenvalues - or amount of variation that this principal component captures. For example, the first principal component has an eigenvalue of ~ 9.395 and captures about 18.8% of the variance in the data. We can also visualize which variables are manipulating our data the most and our samples themselves:</p> RPython <pre><code>## which variables are contributing\n## to principal components\nfviz_contrib(pca, choice = \"var\", axes = 1, top = 10) |\nfviz_contrib(pca, choice = \"var\", axes = 2, top = 10) ## let's visualize our samples in \n## principal component space\nfviz_pca_ind(pca,\nlabel = \"none\", # hide individual labels\nhabillage = as.factor(selected$SEX), # color by groups\naddEllipses = FALSE # Concentration ellipses\n)\n</code></pre> <p></p> <p></p> <pre><code># still in development - sorry!\n</code></pre> <p>Here we note that the gene TIMP1, contributes the most to variance of principal component 1. Interestingly, this gene has been implicated in immune infiltration in glioblastoma. We also can see in the variance contribution plot for the second principal component that MT-ATP6 contributes the most to the variance of this dimension. In the plot below the variable contribution plot, we visualize our samples along the first two principal components and color by sex. Here we do not see a discernable pattern - but this kind of coloring is useful say if you want to ensure two conditions are distributed the way you'd expect.</p> <p>Example</p> <p>An example of using PCA to see if conditions are distributed as expected could be a case-control study. Do your case patients cluster together and do your control patients cluster together.</p>"},{"location":"machine-learning/unsupervised/dimension-reduction/#references","title":"References","text":"<ul> <li>RPubs</li> <li>STHDA</li> </ul>"},{"location":"machine-learning/unsupervised/hierarchical/","title":"Hierarchical Clustering","text":""},{"location":"machine-learning/unsupervised/hierarchical/#agglomerative-hierarchical-clustering","title":"Agglomerative Hierarchical Clustering","text":"<p>Agglomerative Hierarchical Clustering is a bottom up approach wherein observations are their own cluster and then merged into larger and larger clusters until their is one root cluster:</p> <p></p> <p>This \"hierarchical\" view of these clusters is called a dendrogram. Here we will discuss Ward's Method for merging these clusters as it is one of the most popular:</p> \\[D_{12} = \\frac{||\\overline{x_1} - \\overline{x_2}||^2}{\\frac{1}{N_1}+\\frac{1}{N_2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(D_{12}\\) distance between clusters 1 and 2</li> <li>\\(N_1\\) number of points in cluster 1</li> <li>\\(N_2\\) number of points in cluster 2</li> <li>\\(\\overline{x_1}\\) mean of cluster 1</li> <li>\\(\\overline{x_2}\\) mean of cluster 2</li> </ul>"},{"location":"machine-learning/unsupervised/hierarchical/#pre-processing","title":"Pre-Processing","text":"<p>Before we apply k-means we will need to create our distance matrix:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(factoextra)\n\n# load our counts data\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\")\n\n# make the genes our rownames\nrownames(counts) &lt;- make.names(counts$Hugo_Symbol,unique = TRUE)\n\n# remove the gene symbol column\ncounts &lt;- counts %&gt;%\nselect(-c(Hugo_Symbol)) # log2 transform our data \n# transpose our data so that our patients are rows\ncounts &lt;- t(log2(counts + 1))\n\n# Change NA counts to 0\ncounts[!is.finite(counts)] &lt;- 0\n\n# generate correlation distance matrix\ndist &lt;- get_dist(counts,method = \"pearson\")\n\n# plot correlation distance matrix\nfviz_dist(dist) +\ntheme(axis.text = element_text(size = 3)) +\nlabs(\ntitle = \"Pearson Correlation Distances Between Samples\",\nfill = \"Pearson Correlation\"\n)\n</code></pre> <p></p>"},{"location":"machine-learning/unsupervised/hierarchical/#clustering-with-wards-method","title":"Clustering with Ward's method","text":"<p>Let's apply this in R!</p> <pre><code># apply ward's clustering\nhc &lt;- hclust(d = dist, method = \"ward.D2\")\n\n# visualizing the dendrogram\n# and color by k number of clusters\nfviz_dend(hc,\nk = 4, k_colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\"))\n</code></pre> <p></p> <p>Info</p> <ul> <li>here we see each sample starts as its own cluster and is gradually merged into larger clusters</li> <li>we choose to visualize 4 clusters by this is really up to your discretion</li> </ul>"},{"location":"machine-learning/unsupervised/hierarchical/#hierarchical-clustering-shortcomings","title":"Hierarchical Clustering Shortcomings","text":"<p>Hierarchical clustering does come with a few issues:</p> <p>Hierarchical Clustering Shortcomings</p> <ul> <li>Hierarchical clustering is computationally expensive and is much slower than the k-means algorithm</li> <li>While this method is less sensitive to the shape of the data, given it starts generating clusters from individual data points; The dendrogram can be difficult to interpret and where to draw the line with cluster membership is not necessarily defined.</li> </ul>"},{"location":"machine-learning/unsupervised/hierarchical/#references","title":"References","text":"<ol> <li>Clustering Distance Measures</li> <li>K-Means Clustering in R: Algorithm and Practical Examples</li> <li>Agglomerative Hierarchical Clustering</li> <li>Distance Method Formulas</li> <li>Hierarchical Clustering \u2014 Explained</li> </ol>"},{"location":"machine-learning/unsupervised/k-means/","title":"K-Means Clustering","text":""},{"location":"machine-learning/unsupervised/k-means/#k-means-clustering","title":"K-means Clustering","text":"<p>K-means clustering seeks to derive \\(k\\) number of clusters so that the variation within the cluster is minimized. Additionally, the number of clusters, \\(k\\), is specified by the user. The standard k-means algorithm is the Hartigan-Wong algorithm, which starts by determining the sum of squares for each cluster:</p> \\[W(C_k) = \\sum_{x_i\\in{C_k}}{(x_i - \\mu_k)^2}\\] <p>Then the total within cluster variation is calculated by:</p> \\[total\\ within\\ cluster\\ variation = \\sum_{k=1}^k{\\sum_{x_i\\in{C_k}}{(x_i - \\mu_k)^2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(C_k\\) cluster number \\(k\\)</li> <li>\\(x_i\\) point in cluster \\(C_k\\)</li> <li>\\(\\mu_k\\) mean of points in cluster \\(C_k\\)</li> <li>\\(k\\) number of clusters</li> </ul> <p>Info</p> <p>This total within cluster variation is then minimized to best assign data points to the \\(k\\) number of clusters</p>"},{"location":"machine-learning/unsupervised/k-means/#pre-processing","title":"Pre-Processing","text":"<p>Before we apply k-means we will need to create our distance matrix:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(factoextra)\n\n# load our counts data\ncounts &lt;- read.csv(\nfile=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\nheader = T,\nsep = \"\\t\")\n\n# make the genes our rownames\nrownames(counts) &lt;- make.names(counts$Hugo_Symbol,unique = TRUE)\n\n# remove the gene symbol column\ncounts &lt;- counts %&gt;%\nselect(-c(Hugo_Symbol)) # log2 transform our data \n# transpose our data so that our patients are rows\ncounts &lt;- t(log2(counts + 1))\n\n# Change NA counts to 0\ncounts[!is.finite(counts)] &lt;- 0\n\n# generate correlation distance matrix\ndist &lt;- get_dist(counts,method = \"pearson\")\n\n# plot correlation distance matrix\nfviz_dist(dist) +\ntheme(axis.text = element_text(size = 3)) +\nlabs(\ntitle = \"Pearson Correlation Distances Between Samples\",\nfill = \"Pearson Correlation\"\n)\n</code></pre> <p></p>"},{"location":"machine-learning/unsupervised/k-means/#choosing-k-number-of-clusters","title":"Choosing K Number of Clusters","text":"<p>In R we can use the <code>fviz_nbclust()</code> to determine the optimal number of clusters. This will generate a plot and where the plot dips dramatically is our optimal number of \\(k\\)!</p> <pre><code># k-means clustering\n# choosing k\nfviz_nbclust(counts, kmeans, method = \"wss\") +\ngeom_point(color=\"midnightblue\")+\ngeom_line(color=\"midnightblue\")+\ngeom_vline(xintercept = 3,color=\"firebrick\")\n</code></pre> <p></p> <p>Here we do not see a drastic dip in our plot so we will choose 3 clusters here.</p>"},{"location":"machine-learning/unsupervised/k-means/#applying-k-means","title":"Applying K-means","text":"<p>We will now perform k-means clustering and plot the results!</p> <pre><code># applying the k-means function\nkm &lt;- kmeans(counts, 3, nstart = 25)\nfviz_cluster(km, counts,\ngeom = \"point\",\nellipse.type = \"norm\")+\ntheme_bw()+\nlabs(\ntitle = \"K-means Cluster Plot with 3 Clusters\"\n)\n</code></pre> <p></p> <p>You will note here that the clusters overlap to a great degree and there isn't great separation between them.</p>"},{"location":"machine-learning/unsupervised/k-means/#k-means-shortcomings","title":"K-means Shortcomings","text":"<p>Given the lackluster cluster plot above it is worth discussing the shortcomings of k-means clustering:</p> <p>K-means Shortcomings</p> <ul> <li>you are going to need to determine the optimal number of clusters ahead of time</li> <li>the initial center point is chosen at random! And as such your cluster can change depending on that random center point</li> <li>this approach can rely heavily on the mean of the cluster points and the mean is sensitive to outliers in the data!</li> <li>k-means clustering can be affected by data order as well</li> </ul>"},{"location":"machine-learning/unsupervised/k-means/#references","title":"References","text":"<ol> <li>Clustering Distance Measures</li> <li>K-Means Clustering in R: Algorithm and Practical Examples</li> </ol>"},{"location":"news/2022_news/","title":"2022","text":""},{"location":"news/2022_news/#december-2022","title":"December 2022","text":"<p>Welcome to the beta release of this tutorials page! Content is still under development, but we look forward to flushing this out more in 2023!</p> <p>2023 2022 </p>"},{"location":"news/2023_news/","title":"2023","text":""},{"location":"news/2023_news/#january-2023","title":"January 2023","text":"<p>TBD</p> <p>2023 2022 </p>"},{"location":"omics/omics/","title":"Introduction","text":""},{"location":"omics/omics/#genomics","title":"Genomics","text":"<ul> <li>Intro To NGS</li> <li>Intro To 16S Metabarcoding</li> </ul>"},{"location":"omics/omics/#transcriptomics","title":"Transcriptomics","text":"<ul> <li>Intro To RNA-Seq</li> </ul>"},{"location":"omics/omics/#proteomics","title":"Proteomics","text":"<ul> <li>Intro To Proteomics</li> <li>Intro To AlphaFold2</li> </ul>"},{"location":"omics/intro-to-16S/background/","title":"Introduction to the Microbiome","text":"<ul> <li>The microbiome refers to the collective set of genes belonging to the microbiota in a specimen. The term microbiota represents the community of microbes themselves.</li> <li>Disturbances in the microbiome have been linked to multiple chronic conditions, including obesity, inflammatory bowel disease, alcoholic and nonalcoholic fatty liver disease, and hepatocellular carcinoma.</li> </ul>"},{"location":"omics/intro-to-16S/background/#microbiome-variability","title":"Microbiome Variability","text":"<ul> <li>Assessing a microbiome disturbance is not a trivial task as it is highly variable from person to person.</li> <li>Large sample sizes, hundreds of patients, are needed to overcome interindividual variability.</li> </ul>"},{"location":"omics/intro-to-16S/background/#sample-collection","title":"Sample Collection","text":"<ul> <li>Sample collection is also a difficult challenge and highly dependent on the study question.</li> <li>The microbiome can change in an individual over time, especially in diseases marked by flare ups like IBD.</li> <li>Samples might not be representative of the site in question. For example, a stool sample sits in the rectum \u2013 an environment that is undergoing dehydration and fermentation which might select for different bacteria than in the small intestine.</li> </ul>"},{"location":"omics/intro-to-16S/background/#confounding-factors","title":"Confounding Factors","text":"<ul> <li>When conducting a clinical experiment, it is pertinent to stratify accounting for age, gender, diet, etc.</li> <li>Sampling over time is incredibly valuable as you can better capture intrapatient variability.</li> <li>Additionally, the way the sample is processed can also confound your results</li> </ul>"},{"location":"omics/intro-to-16S/background/#what-is-an-amplicon","title":"What is an Amplicon?","text":"<ul> <li>Microbiome Amplicon sequencing involves sequencing a specific gene from microbial community</li> </ul>"},{"location":"omics/intro-to-16S/background/#why-sequence-one-gene","title":"Why Sequence One Gene?","text":"<ul> <li> <p>Genes can vary per organism and may not be well conserved across species. To assess the microbial community composition, we need to sequence a conserved gene across organisms of interest:</p> <ul> <li>16S ribosome DNA (rDNA) for prokaryotes</li> <li>18S rDNA and internal transcribed spacers (ITS) for eukaryotes </li> </ul> </li> <li> <p>In the selected gene there are different levels of conservation across organisms. To circumvent this parts of the gene with high conservation (like the V4 region of 16S rRNA) are selected for</p> </li> </ul> <p></p>"},{"location":"omics/intro-to-16S/background/#our-data","title":"Our Data","text":"<p>Today we will be analyzing the microbiome of wild type mice and the C57BL/6NTac laboratory mouse strain, from Rosshart et al. (2107), using amplicon data analysis:</p> <p></p>"},{"location":"omics/intro-to-16S/background/#amplicon-data-analysis","title":"Amplicon Data Analysis","text":"<p>The goal of amplicon data analysis is to generate amplicon sequence variant table (also called feature table).  Researchers can use this table to conduct further downstream analysis including:</p> <ul> <li>alpha/beta-diversity</li> <li>taxonomic composition</li> <li>difference comparison </li> <li>correlation anlysis</li> <li>network analysis</li> </ul> <p> </p> Modified from Liu et al. Protein &amp; Cell (2021) <p>Today we will be using the DADA2 method to perform our amplicon data analysis!</p> Other 16S Analysis Methods <ul> <li>USEARCH</li> <li>Mothur</li> <li>QIIME</li> </ul>"},{"location":"omics/intro-to-16S/differential-abundance/","title":"Differential Abundance","text":"<p>When assessing a microbial community, you might be interested to determine which taxa are differentially abundant between conditions. Given that we have a counts matrix we can use DESeq2!</p>"},{"location":"omics/intro-to-16S/differential-abundance/#phylum-present","title":"Phylum Present","text":"<p>Before we assess which phylum are differentially abundant, a bar plot can be a quick first pass at determining this:</p> <pre><code># transform the sample counts to proportions\n# separate out our proportions\n# separate our our tax info\nps.prop &lt;- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))\notu = data.frame(t(data.frame(ps.prop@otu_table)))\ntax = data.frame(ps.prop@tax_table) # merge the otu table and phylum column\n# reshape our data to be accepted by ggplot\n# merge taxa data with sample meta data\nmerged &lt;- merge(otu,\ntax %&gt;% select(Phylum),\nby=\"row.names\") %&gt;%\nselect(-Row.names) %&gt;%\nreshape2::melt() %&gt;%\nmerge(.,\ndata.frame(ps.prop@sam_data) %&gt;%\nselect(Run,Host),\nby.x=\"variable\",\nby.y=\"Run\")\n\n# plot our taxa \ntaxa_plot &lt;- ggplot(merged,aes(x=variable,y=value,fill=Phylum)) +\ngeom_bar(stat='identity') +\ntheme_bw()+\ntheme(axis.text.x = element_text(angle=45,hjust=1))+\nlabs(\nx=\"\",\ny=\"Abundance\",\ntitle = \"Barplot of Phylum Abundance\"\n)+\nfacet_wrap(Host ~ ., scales = \"free_x\")\ntaxa_plot\n</code></pre> <p></p> <p>Here we note that the wild type seem to have an abundance of Campylobacteria and the C57BL/6NTac have an abundance of Bacteriodota. Let's see if our DESeq2 results confirm this.</p>"},{"location":"omics/intro-to-16S/differential-abundance/#differential-abundance","title":"Differential Abundance","text":"<p>Differential Abundance measures which taxa are differentially abundant between conditions. So how does it work:</p>"},{"location":"omics/intro-to-16S/differential-abundance/#deseq2-normalization","title":"DESeq2 Normalization:","text":"<ol> <li>Geometric mean per ASV</li> <li>Divide rows by geometric mean</li> <li>Take the median of each sample</li> <li>Divide all ASV counts by that median</li> </ol>"},{"location":"omics/intro-to-16S/differential-abundance/#deseq2-model","title":"DESeq2 Model","text":"<ol> <li>The normalized abundances of an ASV are plotted against two conditions</li> <li>The regression line that connects these data is used to determine the p-value for differential abundance</li> </ol>"},{"location":"omics/intro-to-16S/differential-abundance/#deseq2-p-value","title":"DESeq2 P-Value","text":"<ol> <li>The Slope or \ud835\udefd1 is used to calculate a Wald Test Statistic \ud835\udc4d</li> <li>This statistic is compared to a normal distribution to determine the probability of getting that statistic </li> </ol> <p>Now how do we do this in R?</p> <pre><code># Differential Abundance\n\n## convert phyloseq object to DESeq object this dataset was downsampled and \n## as such contains zeros for each ASV, we will need to\n## add a pseudocount of 1 to continue and ensure the data are still integers\n## run DESeq2 against Host status, and ensure wild type is control,\n## filter for significant changes and add in phylogenetic info\ndds = phyloseq_to_deseq2(ps, ~ Host)\ndds@assays@data@listData$counts = apply((dds@assays@data@listData$counts +1),2,as.integer)\ndds = DESeq(dds, test=\"Wald\", fitType=\"parametric\")\nres = data.frame(\nresults(dds,\ncooksCutoff = FALSE, contrast = c(\"Host\",\"C57BL/6NTac\",\"Mus musculus domesticus\")))\nsigtab = res %&gt;%\ncbind(tax_table(ps)[rownames(res), ]) %&gt;%\ndplyr::filter(padj &lt; 0.05) ## order sigtab in direction of fold change\nsigtab &lt;- sigtab %&gt;%\nmutate(Phylum = factor(as.character(Phylum), levels=names(sort(tapply(\nsigtab$log2FoldChange, sigtab$Phylum, function(x) max(x)))))\n)\n\n# as a reminder let's plot our abundance data again\ntaxa_plot\n\n## plot differential abundance\nggplot(sigtab , aes(x=Phylum, y=log2FoldChange, color=padj)) + geom_point(size=6) + theme_bw() +\ntheme(axis.text.x = element_text(angle = 60, hjust = 1)) +\nggtitle(\"Mus musculus domesticus v. C57BL/6NTac\")\n</code></pre> <p></p> <p></p> <p>Explanation of Results</p> <ul> <li>Wild type seem to have an abundance of Campylobacteria and the C57BL/6NTac have an abundance of Bacteriodota</li> <li>Proteobacteria are severely downregulated in our C57BL/6NTac mice. However, they only show up in one sample!</li> <li>Be sure that your data are not influenced by outliers!</li> <li>Additionally, we collapsed our ASV's to the Phylum level since all ASV's had an identified phylum</li> </ul> Optional: How do I turn this R markdown into an R script? <ul> <li>run the following code (being sure to change the path to where your script is): </li> <li><code>knitr::purl(\"dada2pipeline.Rmd\")</code></li> <li>You should now find an R script called <code>dada2pipeline.R</code>!</li> </ul> <p>References</p> <ol> <li>Galaxy Project - Metagenomics</li> <li>Microbiome 101</li> <li>Current understanding of the human microbiome</li> <li>Amplicon and metagenomics overview</li> <li>Variable regions of the 16S ribosomal RNA</li> <li>A primer on microbial bioinformatics for nonbioinformaticians</li> <li>usearch</li> <li>Sample Multiplexing Overview</li> <li>DADA2: High resolution sample inference from Illumina amplicon data</li> <li>Chimeric 16S rRNA sequence formation and detection in Sanger and 454-pyrosequenced PCR amplicons</li> <li>DADA2 Pipeline Tutorial (1.16)</li> <li>Statistics How To</li> <li>Hierarchical Clustering in Data Mining</li> <li>Abundance-based dissimilarity metrics</li> <li>Differential expression analysis with DESeq2</li> <li>Introduction to RNA-Seq with Galaxy</li> <li>Evaluation of 16S rRNA Databases for Taxonomic Assignments Using a Mock Community</li> <li>Wild Mouse Gut Microbiota Promotes Host Fitness and Improves Disease Resistance</li> <li>Normalization and microbial differential abundance strategies depend upon data characteristics</li> <li>Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible</li> <li>A Primer on Metagenomics</li> <li>Clustal W and Clustal X version 2.0</li> <li>The neighbor-joining method: a new method for reconstructing phylogenetic trees.</li> <li>Large-scale contamination of microbial isolate genomes by Illumina PhiX control</li> <li>Dadasnake, a Snakemake implementation of DADA2 to process amplicon sequencing data for microbial ecology</li> </ol>"},{"location":"omics/intro-to-16S/diversity-analysis/","title":"Diversity Analysis","text":""},{"location":"omics/intro-to-16S/diversity-analysis/#constructing-the-phylogenetic-tree","title":"Constructing the Phylogenetic Tree","text":"<p>We will now construct a phylogenic tree based on our sequence data. To construct our tree we will be first aligning our ASV's using ClustalW and then constructing a phylogenetic tree via the neighborhood joining method. </p> <p>To learn more about ClustalW and the neighborhood joining method visit:</p> <ul> <li>Clustal W and Clustal X version 2.0</li> <li>The neighbor-joining method: a new method for reconstructing phylogenetic trees.</li> </ul> <p>Let's work this in R!</p> <pre><code># extract sequences\n# name the sequences with their sequence so \n# that the ends of the phylogenetic tree are labeled\n# align these sequences\nseqs &lt;- getSequences(seqtab)\nnames(seqs) &lt;- seqs mult &lt;- msa(seqs, method=\"ClustalW\", type=\"dna\", order=\"input\")\n\n# convert multiple sequence alignment to a phyDat object\n# calculate the nucleotide distances between ASVs\n# use a neighbor joining algorithm to generate the tree\n# finally calculate the likelihood of the tree given the sequence alignment\nphang.align &lt;- as.phyDat(mult, type=\"DNA\", names=getSequence(seqtab))\ndm &lt;- dist.ml(phang.align)\ntreeNJ &lt;- NJ(dm)\nfit = pml(treeNJ, data=phang.align)\n</code></pre>"},{"location":"omics/intro-to-16S/diversity-analysis/#making-a-phyloseq-object","title":"Making a PhyloSeq Object","text":"<p>Once we have quantified our community, we can analyze its composition. Two main methods of doing so are exploring the alpha and beta diversity of the community. First we will need to take our taxonomic data and pass it to the <code>phyloseq</code> package for easier manipulation:</p> <pre><code># Create phyloseq object\n\n# upload meta data for study\n# ensure the rownames of our meta data are our sample name\nmeta &lt;- read.csv(\"../data/metaData.txt\")\nrownames(meta) &lt;- meta$Run\n\n# combine the ASV table, the meta data, and taxonomic data\n# to create the phyloseq object\nps &lt;- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), sample_data(meta), tax_table(taxa),\nphy_tree(fit$tree)\n)\n\n# Update ASV names to be shorter\n\n# The full ASV DNA sequence can be hard to look at\n# for this reason we move the sequence information to \n# the refseq slot of the phyloseq object\ndna &lt;- Biostrings::DNAStringSet(taxa_names(ps))\nnames(dna) &lt;- taxa_names(ps)\nps &lt;- merge_phyloseq(ps, dna)\ntaxa_names(ps) &lt;- paste0(\"ASV\", seq(ntaxa(ps)))\n</code></pre>"},{"location":"omics/intro-to-16S/diversity-analysis/#to-rarefy-or-not-to-rarefy","title":"To Rarefy Or Not To Rarefy?","text":"<ul> <li>Rarefaction curves are used to estimate the fraction of species that have been sequenced and usually result in a plot looking something like the following:</li> </ul> <p>What does this mean?</p> <ul> <li>Green curve: a plateau is present and it appears that most species have been sequenced</li> <li>Blue curve: this appears to be a species rich environment and we have not hit our plateau yet</li> <li>Brown curve: only a small fraction of the species appear to have been sequenced as the curve is rapidly rising</li> </ul> <ul> <li>There has been recent debate about whether or not to rarefy amplicon sequencing data:<ul> <li>Pros: Weiss et al. 2017 have noted that sequencing depth has an effect on ordination space and how species richness is displayed </li> <li>Cons: McMurdie and Holmes 2014 have noted that this depends on the species richness metric. </li> </ul> </li> <li>In this tutorial we won't be applying rarefaction to our data.</li> </ul>"},{"location":"omics/intro-to-16S/diversity-analysis/#alpha-diversity","title":"Alpha Diversity","text":"<ul> <li> <p>Alpha Diversity: diversity of organisms sharing the same community or habitat. Alpha diversity metrics can look at richness, evenness, or both within a sample. </p> <ul> <li>Alpha Diversity Metrics:<ul> <li>Faith\u2019s PD: Phylogenetic diversity</li> <li>Observed OTUs: Richness of community</li> <li>Shannon: Balances richness and evenness</li> <li>Pielou\u2019s Evenness: Evenness of community</li> </ul> </li> </ul> </li> <li> <p>We will use the Shannon or Simpson Diversity indices to measure this complexity per sample.</p> </li> </ul> Optional: How to calculate these diversity metrics <p></p> <ul> <li> <p>Here we note:</p> <ul> <li>Shannon Diversity Index: higher values = higher diversity</li> <li>Simpson Diversity Index: higher values = higher diversity</li> </ul> </li> </ul> <p>In R we can visualize this with:</p> <pre><code># Plotting Alpha Diversity Metrics\nplot_richness(ps, x=\"Host\", measures=c(\"Shannon\", \"Simpson\"), color=\"Host\")+\ntheme_bw()+\ntheme(axis.text.x = element_text(angle=65,hjust=1))\n</code></pre> <p></p> <p>Note</p> <p>When running alpha and beta diversity plots you will notice some errors. This is due to the subsampling we needed to do on this data to ensure multiple users could run this workshop at the same time.</p>"},{"location":"omics/intro-to-16S/diversity-analysis/#beta-diversity","title":"Beta Diversity","text":"<ul> <li> <p>Beta Diversity: diversity between communities. Beta diversity calculates how similar two total ecosystems are.</p> <ul> <li>Beta Diversity<ul> <li>Unweighted Unifrac: Presence / absence phylogenetic distance between samples</li> <li>Weighted Unifrac: Abundance weighted phylogenetic distance between samples</li> <li>Jaccard: Presence / absence distance between samples</li> <li>Bray Curtis: Abundance weighted distance between samples</li> </ul> </li> </ul> </li> <li> <p>Here we will use the weighted UniFrac distance since it aware of phylogenetic distances</p> </li> </ul> Optional: How to calculate UniFrac Distance <p></p> <ul> <li>\\(N\\) is the number of nodes in the tree</li> <li>\\(S\\) is the number of sequences represented by the tree</li> <li>\\(L_i\\) is the branch length between node \\(i\\) and its parent </li> <li>\\(L_j\\) is the total branch length from the root to the tip of the tree for sequence \\(j\\)</li> <li>\\(A_i\\) and \\(B_i\\) are the number of sequences from communities \\(A\\) and \\(B\\) that descend from the node, </li> <li>\\(A_T\\) and \\(B_T\\) are the total number of sequences from communities \\(A\\) and \\(B\\).</li> </ul> <p>Mothur UniFrac Alogrith</p> <p>We can plot this in R code:</p> <pre><code># calculate the unifrac distance between samples \n# plot unifrac distances\nordu = ordinate(ps, \"PCoA\", \"unifrac\", weighted=TRUE)\nplot_ordination(ps, ordu, color=\"Host\")+\ntheme_bw()+\nlabs(title = \"Unifrac Distances\")\n</code></pre> <p></p> <p>Here we note that the wild type and C57BL/6NTac cluster together.</p> Which mouse line do you expect to be more spread on the Bray-Curtis Distance plot? <ul> <li>Laboratory Mouse Line (C57BL/6NTac)</li> <li>Wild Type (Mus musculus domesticus)</li> </ul>"},{"location":"omics/intro-to-16S/error-asv/","title":"Error Model & ASVs","text":""},{"location":"omics/intro-to-16S/error-asv/#dada2-error-model","title":"DADA2 Error Model","text":"<ul> <li>The DADA2 error model attempts to assess whether a sequence is too abundant to be explained by errors in amplicon sequencing. </li> </ul> <p>Here we will leverage this model to learn error rates and then plot them:</p> <pre><code># Learn Error Rates\n\n# dada2 uses a parametric model to learn the error rates\n# for each sequence\nerrForward &lt;- learnErrors(filtForward)\nerrReverse &lt;- learnErrors(filtReverse)\n\n# plot the error rate against theoretical error rates\nplotErrors(errForward,nominalQ=TRUE)\n</code></pre> <p></p> <p>Info</p> <p>So the red line indicates our expected error rate. Essentially, as the quality score gets better so does our error rate.  The black points/line our are actual error rates and we are looking for the trend of the black line to match the trend of the red line.  Here we expect a little deviation since our sample has been subsampled.</p>"},{"location":"omics/intro-to-16S/error-asv/#inferring-sequence-variants","title":"Inferring Sequence Variants","text":"<ul> <li>So far, we have assigned p-values for each sequence in each sample</li> <li>DADA2 then tries to determine which sequences are of biological origin and which aren\u2019t by assessing which sequences are present in other samples</li> <li>If a sequence is present in another sample, it is more likely that it is a real biological sequence</li> </ul> <pre><code># Infer Sequnce Variants\n\n# we will now run the dada2 algorithm \n# this algorithm delivers \"true\" sequence variants\n# with information gathered from the error model \n# generated above\ndadaForward &lt;- dada(filtForward, err=errForward)\ndadaReverse &lt;- dada(filtReverse, err=errReverse)\n\n# let's get a summary of our first sample\ndadaForward[[1]]\n</code></pre> <pre><code>dada-class: object describing DADA2 denoising results\n35 sequence variants were inferred from 430 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n</code></pre> <p>Info</p> <p>Here we note that even though we have 430 unique sequences in our data, only 35 of them have been deemed true sequence variants.</p>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/","title":"Merging, Chimeras & Taxonomy","text":""},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#merging-reads","title":"Merging Reads","text":"<ul> <li>For paired-end data there is a good deal of overlap between the forward and reverse read</li> <li>To resolve this redundancy, these reads are collapsed into contigs</li> </ul> <p>Let's do this with code now!</p> <pre><code># Merge Read Pairs\n\n# so far we have \"denoised\", so to speak, \n# these sequence variants. We now need to merge the\n# forward and reverse strands\nmergers &lt;- mergePairs(\ndadaForward,\nfiltForward,\ndadaReverse, filtReverse, verbose=TRUE)\n</code></pre> <pre><code>619 paired-reads (in 18 unique pairings) successfully merged out of 807 (in 82 pairings) input.\n570 paired-reads (in 29 unique pairings) successfully merged out of 815 (in 136 pairings) input.\n619 paired-reads (in 28 unique pairings) successfully merged out of 868 (in 128 pairings) input.\n713 paired-reads (in 18 unique pairings) successfully merged out of 860 (in 76 pairings) input.\n609 paired-reads (in 29 unique pairings) successfully merged out of 851 (in 133 pairings) input.\n620 paired-reads (in 30 unique pairings) successfully merged out of 810 (in 115 pairings) input.\n679 paired-reads (in 28 unique pairings) successfully merged out of 845 (in 104 pairings) input.\n616 paired-reads (in 28 unique pairings) successfully merged out of 830 (in 106 pairings) input.\n</code></pre> <p>Info</p> <p>Here we see that for each sample we get the number of reads that were able to be successfully merged out of the total number of reads that could be merged.</p>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#asvs-vs-otus","title":"ASVs vs. OTUs","text":"<ul> <li>Now that we have finally merged our sequence variants we are left with an Amplicon Sequence Variant. </li> <li>Traditional 16S metagenomic approaches use OTUs or operational taxonomic units instead of ASVs. </li> <li>Operational taxonomic units (OTUs): clusters of reads that differ by less than a fixed sequence dissimilarity threshold, most commonly 3%  Instead of clustering sequences, methods that resolve amplicon sequence variants (ASVs) distinguish sequence variants differing by as little as one nucleotide. ASVs represent a biological reality and provide nucleotide-level resolution. Callahan, MucMurdie, &amp; Holmes (2017) have demonstrated that \u201cASVs capture all biological variation present in the data, and ASVs inferred from a given data set can be reproduced in future data sets and validly compared between data sets.\u201d</li> </ul>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#asv-table","title":"ASV Table","text":"<p>Now that our sequences are merged we can create an ASV counts table, basically telling us how which samples contain which ASV's:</p> <pre><code># Making a Sequence Table\n\n# now that we have merged sequences we can construct\n# an Amplicon Sequence Variant (ASV) table\nseqtab &lt;- makeSequenceTable(mergers)\n</code></pre>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#chimera-removal","title":"Chimera Removal","text":"<ul> <li>During Sequencing microbial DNA is subjected to PCR to amplify DNA</li> <li>During PCR it is possible for two unrelated templates to form a non-biological hybrid sequence</li> <li>DADA2 finds these chimeras by:<ul> <li>aligning each sequence to more abundant sequences </li> <li>now check low abundant sequences and determine:<ul> <li>can this sequence be created if we mix the left and right sides of the abundant sequences</li> </ul> </li> </ul> </li> </ul> <p>Now in code:</p> <pre><code># Removing Chimeras\n\n# Chimeric sequences occur as errors during PCR \n# when two unrelated templates for a hybrid sequence\n# we will need to remove them before going forward\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab, method=\"consensus\", verbose=TRUE)\n</code></pre> <p>Now let's check if any chimeric sequences are removed:</p> <pre><code>## check to see if the dimensions are different\n## between the chimera filtered and unfiltered\n## ASV tables\n\ndim(seqtab)\ndim(seqtab.nochim)\n</code></pre> <pre><code>[1]   8 119\n[1]   8 117\n</code></pre> <p>Info</p> <p>We can see here that 2 chimeric sequences were removed because our before and after sequence count matrices differ by two columns.</p>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#pipeline-quality-control","title":"Pipeline Quality Control","text":"<p>We will also take a moment to do some final QC:</p> <pre><code># Final QC\n\n## we have performed quite a few steps \n## and it would be nice to get a final qc check \n## before assigning taxonomy\ngetN &lt;- function(x) sum(getUniques(x))\nfinalQC &lt;- cbind(\nout, sapply(dadaForward, getN),\nsapply(dadaReverse, getN),\nsapply(mergers, getN),\nrowSums(seqtab.nochim))\ncolnames(finalQC) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\nrownames(finalQC) &lt;- sampleNames\nfinalQC\n</code></pre> <pre><code>           input filtered denoisedF denoisedR merged nonchim\nSRR5690809  1000      905       840       855    619     611\nSRR5690810  1000      937       853       885    570     549\nSRR5690811  1000      937       880       910    619     594\nSRR5690812  1000      924       886       888    713     700\nSRR5690819  1000      938       872       906    609     609\nSRR5690820  1000      916       870       844    620     620\nSRR5690821  1000      921       883       879    679     679\nSRR5690822  1000      940       865       891    616     616\n</code></pre> <p>Info</p> <p>Here we see that we start with 1000 sequences per sample, end up with around 900 after filtering, around 800 after denoising to  find unique sequences, and around 600-700 sequences after merging sequences and removing chimeric sequences.</p>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#assigning-taxonomy","title":"Assigning Taxonomy","text":"<ul> <li>To determine which taxon each  ASV belongs to DADA2 uses a na\u00efve bayes classifier </li> <li>This classifier uses a set of reference sequences with known taxonomy, here we use the SILVA database, as the training set and and outputs taxonomic assignments with bootstrapped confidence</li> </ul> <pre><code># Assigning Taxonomy\n\n# dada2 uses a naive Bayes classifier when\n# assigning taxonomy. This means we need a training\n# set of sequences with known taxonomy information.\n# here we use the silva database\n\ntaxa &lt;- assignTaxonomy(seqtab.nochim, \"../data/silva_nr99_v138.1_train_set.fa.gz\")\n</code></pre>"},{"location":"omics/intro-to-16S/merging-chimeras-taxonomy/#databases","title":"Databases","text":"<p>While we use the SILVA database here, there are other options databases:</p> <ul> <li>NCBI 16S RefSeq Database</li> <li>Greengenes</li> </ul> <p>Time for a break!</p>"},{"location":"omics/intro-to-16S/metabarcoding-intro/","title":"Metabarcoding intro","text":""},{"location":"omics/intro-to-16S/metabarcoding-intro/#intro-to-16s-metabarcoding","title":"Intro To 16S Metabarcoding","text":"<p>November 2,2022</p>"},{"location":"omics/intro-to-16S/metabarcoding-intro/#tts-research-technology-instructors","title":"TTS Research Technology Instructors","text":"<ul> <li>Jason Laird, M.S., Bioinformatics Scientist</li> <li>Naisi Zhao, M.S., Dr.PH, Research Assistant Professor</li> <li>Adelaide Rhodes, Ph.D,  Senior Bioinformatics Scientist</li> </ul> <p>TTS Help</p> <p>If you'd like to contact Research Technology with questions regarding cluster and storage accounts at Tufts, feel free to reach out to us at</p> <p>tts-research@tufts.edu</p>"},{"location":"omics/intro-to-16S/metabarcoding-intro/#recording","title":"Recording","text":"<p>We will be recording this workshop and distributing among Tufts HPC users as a reference so please contact us if you have any questions about this. </p>"},{"location":"omics/intro-to-16S/metabarcoding-intro/#best-elist","title":"BEST Elist","text":"<p>We are also happy to mention that the Bioinformatics team within TTS Research Technology has an elist, sign up with this link best@elist.tufts.edu to find out about Bioinformatics Education, Software and Tools</p> <p>Find out about other Data Lab and Bioinformatics Workshops being offered this semester from this link.</p> <p>Bioinformatics Workshops</p> <p>If you have a question regarding bioinformatics workshops specifically, please reach out to </p> <p>bioinformatics-workshop-questions@elist.tufts.edu</p> <p>Acknowledgement</p> <p>We would like to thank Delilah Maloney, Kyle Monahan, Christina Divoll, Kayla Sansevere, and Uku Uustalu for their review of this content</p>"},{"location":"omics/intro-to-16S/quality-control/","title":"Quality Control","text":""},{"location":"omics/intro-to-16S/quality-control/#sequencing-overview","title":"Sequencing Overview","text":"<ul> <li>Marker gene (16S, 18S, or ITS) is selected</li> <li>Primers target areas of high conservation in gene </li> <li>DNA is fragmented </li> <li>Adapters are added to help the DNA attach to a flow cell</li> <li>Barcodes may also be added to identify which DNA came from which sample</li> <li>The fragments are sequenced to produce reads</li> <li>Reads can be single-end (one strand sequenced) or paired-end (both strands sequenced) </li> </ul>"},{"location":"omics/intro-to-16S/quality-control/#sequencing-read-data","title":"Sequencing Read Data","text":"<ul> <li>After sequencing we end up with a FASTQ file which contains:<ul> <li>A sequence label</li> <li>The nucleic acid sequence</li> <li>A separator</li> <li>The quality score for each base pair</li> </ul> </li> </ul>"},{"location":"omics/intro-to-16S/quality-control/#demultiplexing","title":"Demultiplexing","text":"<ul> <li>Sometimes samples are mixed to save on sequencing cost </li> <li>To identify which DNA is from which sample Barcodes are added</li> <li>Before moving forward samples need to separated and those DNA barcodes need to be removed </li> <li>Tools like sabre can demultiplex pooled FASTQ data</li> </ul>"},{"location":"omics/intro-to-16S/quality-control/#quality-scores","title":"Quality Scores","text":"<ul> <li>Quality Scores are the probability that a base was called in error</li> <li>Higher scores indicate that the base is less likely to be incorrect</li> <li>Lower scores indicate that the base is more likely to be incorrect</li> </ul>"},{"location":"omics/intro-to-16S/quality-control/#dada2-quality-control","title":"DADA2 Quality Control","text":"<p>Warning</p> <p>DADA2 assumes that your read data has had any adapters removed and that your data is demultiplexed!  Check out sabre to demultiplex your samples and Cutadapt to remove adapters</p> <p>We begin by specifying the path to our data, sorting by forward and reverse strands, and grabbing our sample names:</p> <pre><code># path to files\npath &lt;- \"../data/raw_fastq\"\n\n# sort our files by forward and reverse strands \n# so that the sample names for each strand matches\n# our data has the pattern \"_pass_1.fastq.gz\" \n# and \"_pass_2.fastq.gz\"\npath2Forward &lt;- sort(\nlist.files(\npath,\npattern=\"_pass_1.fastq.gz\",\nfull.names = TRUE)\n)\npath2Reverse &lt;- sort(\nlist.files(\npath,\npattern=\"_pass_2.fastq.gz\",\nfull.names = TRUE)\n)\n\n# now let's grab our sample names\nsampleNames &lt;- sapply(\nstrsplit(\nbasename(path2Forward), \"_\"), `[`, 1)\n</code></pre> <p>DADA2 has built in plotting features that allow you to inspect your fastq files:</p> <pre><code># plot the forward strand quality plot of our first two sample\ndada2::plotQualityProfile(path2Forward[1:2])+\nguides(scale = \"none\")\n</code></pre> <p></p> <pre><code># plot the reverse strand quality plot of our first two sample\ndada2::plotQualityProfile(path2Reverse[1:2])+\nguides(scale = \"none\")\n</code></pre> <p></p> <p>What does the graph tell us?</p> <ul> <li>Here we see that the quality scores drop off around the 200th base position for the forward reads and the 150th base position for the reverse reads</li> <li>The error rate is considered when determining true biological sequences but is more sensitive to rare biological senquences when reads are trimmed.</li> </ul> <p>Trimming Considerations</p> <ul> <li>The data we are using are 2x250 V4 sequence data. For data that do not overlap as much (i.e. data from the V1-V2 or V3-V4 regions), be wary that this may affect how the reads are merged later on. </li> </ul>"},{"location":"omics/intro-to-16S/quality-control/#trimming","title":"Trimming","text":"<p>Here we notice a dip in quality scores and will trim using the base DADA2 filters:</p> <pre><code># create new file names for filtered forward/reverse fastq files\n# name each file name in the vector with the sample name\n# this way we can compare the forward and reverse files \n# when we filter and trim\nfiltForward &lt;- file.path(path, \"filtered\", paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltReverse &lt;- file.path(path, \"filtered\", paste0(sampleNames, \"_R_filt.fastq.gz\"))\nnames(filtForward) &lt;- sampleNames\nnames(filtReverse) &lt;- sampleNames\n\n# Now we will filter and trim our sequences\nout &lt;- filterAndTrim(\npath2Forward,\nfiltForward,\npath2Reverse, filtReverse,\ntruncLen = c(200,150),\nmaxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,\ncompress=TRUE)\n</code></pre> <p>What do these options mean?</p> <ul> <li><code>truncLen</code>: truncate reads after this base <ul> <li>Here we truncate after base 200 for the forward reads and after basae 150 for the reverse reads</li> </ul> </li> <li><code>maxN</code>: After truncation, sequences with more than maxN Ns will be discarded. Note that dada does not allow Ns.</li> <li><code>maxEE</code>: After truncation, reads with higher than maxEE \"expected errors\" will be discarded.</li> <li><code>truncQ</code>: Truncate reads at the first instance of a quality score less than or equal to <code>truncQ</code></li> <li><code>rm.phix</code>: If TRUE, discard reads that match against the phiX genome<ul> <li>Illumina control libraries derived from a PhiX genome, are used to reduce ambiguity in base calls when highly repetitive sequences are generated. It is pertinent to remove reads mapping to the PhiX genome to ensure you are assessing your microbial community and not the Illumina control run.</li> </ul> </li> <li><code>compress</code>:  If TRUE, the output fastq file(s) are gzipped</li> </ul>"},{"location":"omics/intro-to-16S/setup/","title":"Setup","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN if off campus</li> </ul>"},{"location":"omics/intro-to-16S/setup/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>8GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshop</code>---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p>"},{"location":"omics/intro-to-16S/setup/#project-setup","title":"Project Setup","text":"<p>We are going to create a new project to begin:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>intro-to-16S</code>)</li> <li><code>Create Project</code></li> </ol>"},{"location":"omics/intro-to-16S/setup/#file-organization","title":"File Organization","text":"<p>In our project we will need some folders to contain our scripts, data and results:</p> <ul> <li>Click the New Folder icon</li> <li>Create a folder called data and click ok</li> <li>Following the same process, create a scripts folder and a results folder</li> </ul>"},{"location":"omics/intro-to-16S/setup/#data-scripts","title":"Data &amp; Scripts","text":"<p>Today we will be working with data from Rosshart et al. (2107) where wild-type and laboratory strain mouse microbiomes were assessed. To copy over  this data we will enter the following command into the console:</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/raw_fastq/\",to=\"./data/\", recursive = TRUE)\nfile.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/meta/metaData.txt\",to=\"./data/\", recursive = TRUE)\nfile.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/silva/silva_nr99_v138.1_train_set.fa.gz\",to=\"./data/\")\nfile.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/scripts/dada2pipeline.Rmd\",to=\"./scripts/\")\n</code></pre> <p>Now that we have our data and scripts copied, let's navigate to our scripts folder and open up \"dada2pipeline.Rmd\".</p>"},{"location":"omics/intro-to-16S/setup/#libraries","title":"Libraries","text":"<p>To run a code chunk in this R markdown file, click the play button at the top right hand side of the code chunk. We will practice by running the code chunk that loads the R libraries we will need for this workshop:</p> <pre><code># load our libraries\n.libPaths(c('/cluster/tufts/hpc/tools/R/4.0.0',.libPaths()))\nlibrary(dada2)\nlibrary(phyloseq)\nlibrary(ggplot2)\nlibrary(DESeq2)\nlibrary(tidyverse)\nlibrary(phangorn)\nlibrary(msa)\n</code></pre>"},{"location":"omics/intro-to-alphafold2/01_background/","title":"Background","text":""},{"location":"omics/intro-to-alphafold2/01_background/#protein-organization","title":"Protein Organization","text":"<ul> <li>Primary Structure: amino acid sequence</li> <li>Secondary Structure: amino acid sequences linked by hydrogen bonds</li> <li>Tertiary Structure: organization of secondary structures</li> <li>Quaternary Structure: organization of multiple amino acid chains</li> </ul>"},{"location":"omics/intro-to-alphafold2/01_background/#the-importance-of-protein-structure","title":"The Importance of Protein Structure","text":"<ul> <li>Can help determine what a protein does</li> <li>Often more conserved than the amino acid sequences that form them</li> </ul>"},{"location":"omics/intro-to-alphafold2/01_background/#protein-structure-problem","title":"Protein Structure Problem","text":"<p>As of now there are a few different ways to predict protein structure in the lab:</p> <ul> <li>X-ray Crystallography</li> <li>Nuclear Magnetic Resonance (NMR) Spectroscopy</li> <li>3D Electron Microscopy</li> </ul> <p>However there are 100,000,000 known distinct proteins, each with a unique structure that determines function. Determining protein structure is time consuming and as such only a small fraction of exact 3D structures are known.</p>"},{"location":"omics/intro-to-alphafold2/01_background/#levinthals-paradox","title":"Levinthal's Paradox","text":"<ul> <li>Finding the native folded state of a protein by random searching of all possible configurations would take an enormous amount of time</li> <li>However, proteins can often fold within seconds</li> <li>Meaning some process must be guiding this folding</li> </ul>"},{"location":"omics/intro-to-alphafold2/01_background/#using-sequence-to-predict-structure","title":"Using Sequence To Predict Structure","text":"<ul> <li>Instead of laboratory experimentation, there have been massive efforts to use a protein\u2019s sequence to determine structure</li> <li>In 1994, the Critical Assessment of Structure Protein (CASP) was established as a biennial assessment of methods to predict structure from sequence</li> </ul>"},{"location":"omics/intro-to-alphafold2/01_background/#enter-alphafold2","title":"Enter AlphaFold2","text":"<ul> <li>Google\u2019s DeepMind team Entered AlphaFold 2 in CASP14 </li> <li>Achieved a median Global Distance Test Score of 92.4 <ul> <li>This score essentially says: How close is my predicited structure to the known structure?</li> </ul> </li> <li>AlphaFold 2 works by:<ul> <li>starts with a user's query protein sequence</li> <li>finding similar sequences to that query</li> <li>aligns these sequences to create a mutliple sequence alignment</li> <li>uses available structure data based on query sequence to create initial distances between residues</li> <li>uses a neural network to iteratively update the distances between residues by using information from the sequence alignment</li> <li>passes this to another neural network to determine how these residues are oriented in 3D space</li> </ul> </li> </ul>"},{"location":"omics/intro-to-alphafold2/01_background/#our-data","title":"Our Data","text":"<p>Today we will be comparing AlphaFold2's structure prediction of Proliferating Cell Nuclear Antigen (PCNA) and DNA ligase 1 (LIG1)!</p>"},{"location":"omics/intro-to-alphafold2/02_setup/","title":"Setup","text":""},{"location":"omics/intro-to-alphafold2/02_setup/#log-into-the-hpc-clusters-on-demand-interface","title":"Log into the HPC cluster\u2019s On Demand Interface","text":"<ul> <li>Open a Chrome browser and go to On Demand</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <ul> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <pre><code>[tutln01@login001 ~]$\n</code></pre> <ul> <li>This indicates you are logged in to the login node of the cluster. Please do not run any program from the login node.</li> </ul>"},{"location":"omics/intro-to-alphafold2/02_setup/#starting-an-interactive-session","title":"Starting an Interactive Session","text":"<ul> <li>To run our analyses we will need to move from the login node to a compute node. We can do this by entering:</li> </ul> <pre><code>srun --pty -t 3:00:00 --mem 16G -N 1 --cpus 4 bash\n</code></pre> <p>Where:</p> <p>Explanation of Commands</p> <ul> <li><code>srun</code>: SLURM command to run a parallel job</li> <li><code>--pty</code>: get a pseudo terminal</li> <li><code>-t</code>: time we need here we request 3 hours</li> <li><code>--mem</code>:  memory we need here we request 16 Gigabytes</li> <li><code>-N</code>:  number of nodes needed here we requested 1 node</li> <li><code>--cpus</code>:  number of CPUs needed here we requested 4</li> </ul> <ul> <li>When you get a compute node you'll note that your prompt will no longer say login and instead say the name of the node:</li> </ul> <pre><code>[tutln01@c1cmp048 ~]$\n</code></pre>"},{"location":"omics/intro-to-alphafold2/02_setup/#set-up-for-analysis","title":"Set Up For Analysis","text":"<ul> <li>To get our AlphaFold data we will enter:</li> </ul> <pre><code>cp -r /cluster/tufts/bio/tools/tool_examples/af2Workshop ./\n</code></pre> <ul> <li>You will note that we are copying an existing directory with AlphaFold output rather than generating it. This is because depending on the protein and compute resource availability, running AlphaFold can take a few hours to over a day. At the end of this workshop will be instructions for creating a batch script to run AlphaFold. </li> <li>Today we will examine how well AlphaFold predicted the structures of Proliferating Cell Nuclear Antigen and DNA ligase 1.</li> </ul>"},{"location":"omics/intro-to-alphafold2/02_setup/#proliferating-cell-nuclear-antigen-pcna","title":"Proliferating Cell Nuclear Antigen (PCNA)","text":"<p>PCNA is a very well conserved protein across eukaryotes and even Archea. It acts as a processivity factor of DNA Polymerase delta, necessary for DNA replication:</p> <p></p> <p>Aside from DNA replication PCNA is involved in:</p> <ul> <li> <p>chromatin remodelling </p> </li> <li> <p>DNA repair</p> </li> <li> <p>sister-chromatid cohesion</p> </li> <li> <p>cell cycle control</p> </li> </ul> <p>It should also be noted that PCNA is a multimeric protein consisting of three monomers.</p>"},{"location":"omics/intro-to-alphafold2/02_setup/#dna-ligase-1-lig1","title":"DNA ligase 1 (LIG1)","text":"<p>As evidenced by the picture above, DNA Ligase 1 is also involved in DNA replication but also DNA repair. As a part of the DNA replication machinery, DNA Ligase 1 joins Okazaki fragments during lagging strand DNA sythesis. This ligase also interacts with PCNA:</p> <p></p> <p>Here we note that DNA ligase is a monomer consisting of the following domains:</p> <ul> <li>PCNA interacting motif</li> <li>Oligemer Binding Fold Domain</li> <li>Adenylation Domain</li> <li>DNA Binding Domain</li> </ul> <p>The contact between the PCNA interacting motif and PCNA induce a conformational change to create the DNA ligase catalytic region. </p>"},{"location":"omics/intro-to-alphafold2/02_setup/#fasta-format","title":"FASTA Format","text":"<p>So we have copied over data, what does it look like? We can check it out by running the following command:</p> <pre><code>cat af2Workshop/data/1AXC.fasta </code></pre> <pre><code>&gt;1AXC_1|Chain A|PCNA|Homo sapiens (9606)\nMFEARLVQGSILKKVLEALKDLINEACWDISSSGVNLQSMDSSHVSLVQLTLRSEGFDTYRCDRNLAMGVNLTSMSKILKCAGNEDIITLRAEDNADTLALVFEAPNQEKVSDYEMKLMDLDVEQLGIPEQEYSCVVKMPSGEFARICRDLSHIGDAVVISCAKDGVKFSASGELGNGNIKLSQTSNVDKEEEAVTIEMNEPVQLTFALRYLNFFTKATPLSSTVTLSMSADVPLVVEYKIADMGHLKYYLAPKIEDEEGS\n&gt;1AXC_2|Chain B|P21/WAF1|Homo sapiens (9606)\nGRKRRQTSMTDFYHSKRRLIFS\n&gt;1AXC_1|Chain C|PCNA|Homo sapiens (9606)\nMFEARLVQGSILKKVLEALKDLINEACWDISSSGVNLQSMDSSHVSLVQLTLRSEGFDTYRCDRNLAMGVNLTSMSKILKCAGNEDIITLRAEDNADTLALVFEAPNQEKVSDYEMKLMDLDVEQLGIPEQEYSCVVKMPSGEFARICRDLSHIGDAVVISCAKDGVKFSASGELGNGNIKLSQTSNVDKEEEAVTIEMNEPVQLTFALRYLNFFTKATPLSSTVTLSMSADVPLVVEYKIADMGHLKYYLAPKIEDEEGS\n&gt;1AXC_2|Chain D|P21/WAF1|Homo sapiens (9606)\nGRKRRQTSMTDFYHSKRRLIFS\n&gt;1AXC_1|Chain E|PCNA|Homo sapiens (9606)\nMFEARLVQGSILKKVLEALKDLINEACWDISSSGVNLQSMDSSHVSLVQLTLRSEGFDTYRCDRNLAMGVNLTSMSKILKCAGNEDIITLRAEDNADTLALVFEAPNQEKVSDYEMKLMDLDVEQLGIPEQEYSCVVKMPSGEFARICRDLSHIGDAVVISCAKDGVKFSASGELGNGNIKLSQTSNVDKEEEAVTIEMNEPVQLTFALRYLNFFTKATPLSSTVTLSMSADVPLVVEYKIADMGHLKYYLAPKIEDEEGS\n&gt;1AXC_2|Chain F|P21/WAF1|Homo sapiens (9606)\nGRKRRQTSMTDFYHSKRRLIFS\n</code></pre> <p>What does this mean?</p> <ul> <li>Here we have 6 sequences and each sequence has two lines:<ul> <li>A line starting with <code>&gt;</code> which is the sequence header and contains information about the sequence</li> <li>A second line with the amino acid sequence</li> </ul> </li> </ul>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/","title":"AlphaFold2 Pre-Processing","text":""},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#alphafold2-pre-processing","title":"AlphaFold2 Pre-Processing","text":"<p>Let's talk a bit about how AlphaFold2 go from a protein FASTA file to a full structure prediction.</p>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#searching-for-similar-sequences","title":"Searching for Similar Sequences","text":"<ul> <li>This query sequence is compared to:<ul> <li>UniRef90\u00a0database to find similar sequences </li> <li>PDB70 to find similar structures</li> </ul> </li> <li>Sequences that are too similar to our query are filtered out so that we don\u2019t just build a replicate based on that sequence</li> <li>These sequences are arranged as an MSA</li> </ul>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#multiple-sequence-alignment-msa","title":"Multiple Sequence Alignment (MSA)","text":"<ul> <li>An MSA is an array of sequences</li> <li>These sequences are aligned with one another as to best match similar regions</li> <li>These sequences don\u2019t always line up perfectly and as such we see:<ul> <li>Conserved positions: where the letter does not change</li> <li>Coevolved positions: where the letter will change with another letter</li> <li>Specificity Determining positions: where the letter is consistently different </li> </ul> </li> </ul>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#why-is-an-msa-useful-in-structure-prediction","title":"Why is an MSA Useful In Structure Prediction?","text":"<ul> <li>The theory is that residues that coevolve are generally close to each other in the protein\u2019s folded state</li> <li>So, by assessing what residues change together we get an idea of where they might be spatially!</li> </ul>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#msa-embedding","title":"MSA Embedding","text":"<ul> <li>An MSA is still essentially an array of letters </li> <li>To be more computer friendly these letters are embedded as numbers using their positional information</li> <li>AlphaFold embeds these letter values as numeric ones and terms this the MSA representation</li> </ul>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#embedding-example","title":"Embedding Example","text":"<ul> <li>Take for example the sentence \u201cI ate an apple and played the piano\u201d</li> <li>This string is  embedded by positional information. </li> <li>e.g. ate was the second word so there is a 1 in the second column at row \u201cate\u201d</li> </ul>"},{"location":"omics/intro-to-alphafold2/03_af2_preprocess/#pair-representation","title":"Pair Representation","text":"<ul> <li>Similar Structures were also queried for using our protein sequence.</li> <li>These structure files (A.K.A Crystallographic Information Files (CIF)) contain 3D coordinates for a protein\u2019s atoms in space</li> <li>These coordinates are used to initialize a pairwise distance matrix between residues that AlphaFold calls the pair representation</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/","title":"AlphaFold2 Evoformer/Structure Module","text":""},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#the-evoformer","title":"The Evoformer","text":"<ul> <li>The MSA representation and the pair representation are fed into in special type of neural network that AlphaFold terms the Evoformer</li> <li>The Evoformer is a combination of two special types of neural networks called Transformers</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#what-are-neural-networks","title":"What Are Neural Networks?","text":"<ul> <li>Neural Networks are machine learning algorithms that mimic the way neurons communicate</li> <li>They usually consist an input, hidden and output layer</li> <li>Each node has a threshold and if the output of the node isn\u2019t above that threshold it doesn\u2019t communicate with the next node</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#what-is-in-a-node","title":"What Is In A Node?","text":"<ul> <li>Each node can be thought of as a linear regression model with input data, weights, a bias term and an output</li> <li>The weights are assigned as to weight importance \u2013 the larger the weight the more important the variable</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#to-communicate-or-not-communicate","title":"To Communicate Or Not Communicate?","text":"<ul> <li>Each node will have an output based on this regression function</li> <li>That output is then fed into something called an activation function</li> <li>The output of this activation function is compared to some threshold</li> <li>If the threshold is met it \u201dfires\u201d and communicates with the next layer</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#neural-network-customization","title":"Neural Network Customization","text":"<ul> <li>There are different types of neural networks depending on what functions you use and how you organize node communication</li> <li>AlphaFold uses a Recurrent Neural Network</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#recurrent-neural-network","title":"Recurrent Neural Network","text":"<ul> <li>In a feed forward neural network you have input that is processed through a node and if that node is activated it communicates with the next node</li> <li>In a recurrent neural network, the output of a node can be used to inform and change the output of the node</li> <li>Naturally, this comes at a memory cost when it tries to pull from old connections</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#transformer-and-attention","title":"Transformer And Attention","text":"<ul> <li>To save on computational cost, Recurrent Neural Networks can have their attention limited</li> <li>Basically, values are scaled down to reveal which data points are worth paying attention to</li> <li>This focused recurrent neural network is called a Transformer</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#msa-transformer","title":"MSA Transformer","text":"<ul> <li>The MSA Transformer limits its attention two ways:</li> <li>Row-wise: to determine which residues are most related</li> <li>Column-wise: to determine which sequences are most important</li> <li>The limited MSA along with the Pair Representation are then fed into the first head of the Evoformer</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#evoformer-part-1","title":"Evoformer Part 1","text":"<ul> <li>The first block of the Evoformer works to determine how close residues are </li> <li>start with correlations between two sets of residues, say A and B</li> <li>Highly correlation indicates these residues are close</li> <li>Now process is iterated - residue C is correlated with B</li> <li>So, B and C are close</li> <li>This process is repeated for all residues</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#evoformer-part-2","title":"Evoformer Part 2","text":"<ul> <li>The second block of the Evoformer works through pair wise distances between residues</li> <li>Here 3 residues are compared, and triangle inequality is enforced</li> <li>So, one side of the triangle must be less than or equal to the other two sides</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#structure-module","title":"Structure Module","text":"<ul> <li>The Evoformer outputs distances between residues, but residues are themselves three dimensional objects</li> <li>How are they oriented?</li> <li>Each residue starts as a \u201cresidue gas\u201d or triangle between the Alpha Carbon, R-group Carbon, and the Nitrogen</li> </ul> <ul> <li>All residue gases start at the origin of the coordinate system</li> <li>Each position is defined as an affine matrix, or xyz coordinates for the three points of the triangle, which is multiplied by a displacement vector to \"move\" the residue gas to\u00a0 its final location</li> </ul>"},{"location":"omics/intro-to-alphafold2/04_af2_evoformer_structure/#invariant-point-attention","title":"Invariant Point Attention","text":"<ul> <li>The Structure Module also uses an attention mechanism called Invariant Point Attention</li> <li>This limits the data the model needs because points in 3D space are invariant to translation/rotation </li> <li>Basically, this means that no matter how you rotate/translate the final structure you still produce the same answer</li> </ul>"},{"location":"omics/intro-to-alphafold2/05_af2_output/","title":"AlphaFold2 Output","text":""},{"location":"omics/intro-to-alphafold2/05_af2_output/#alphafold-output","title":"AlphaFold Output","text":"<ul> <li>Here we predicted the structures of PCNA and LIG1. Let's examine the output by navigating to the PCNA output folder:</li> </ul> <p><pre><code>cd af2Workshop/pcna/1AXC/\nls </code></pre>  - You should see output that looks like the following:</p> <pre><code>features.pkl  ranked_1.pdb  ranked_4.pdb                  relaxed_model_2_multimer.pdb  relaxed_model_5_multimer.pdb  result_model_3_multimer.pkl  timings.json                    unrelaxed_model_3_multimer.pdb\nmsas          ranked_2.pdb  ranking_debug.json            relaxed_model_3_multimer.pdb  result_model_1_multimer.pkl   result_model_4_multimer.pkl  unrelaxed_model_1_multimer.pdb  unrelaxed_model_4_multimer.pdb\nranked_0.pdb  ranked_3.pdb  relaxed_model_1_multimer.pdb  relaxed_model_4_multimer.pdb  result_model_2_multimer.pkl   result_model_5_multimer.pkl  unrelaxed_model_2_multimer.pdb  unrelaxed_model_5_multimer.pdb\n</code></pre> <ul> <li>Where:</li> </ul> File/Directory Description features.pkl A pickle file w/ input feature NumPy arrays msas A directory containing the files describing the various genetic tool hits that were used to construct the input MSA. unrelaxed_model_*.pdb A PDB file w/ predicted structure, exactly as outputted by the model relaxed_model_*.pdb A PDB file w/ predicted structure, after performing an Amber relaxation procedure on the unrelaxed structure prediction ranked_*.pdb A PDB file w/ relaxed predicted structures, after reordering by model confidence (using predicted LDDT (pLDDT) scores). ranked_1.pdb = highest confidence ranked_5.pdb = lowest confidence ranking_debug.json A JSON file w/pLDDT values used to perform the model ranking, and a mapping back to the original model names. timings.json A JSON file w/times taken to run each section of the AlphaFold pipeline. result_model_*.pkl A pickle file w/ a nested dictionary of the various NumPy arrays directly produced by the model: StructureModule Output, Distograms, Per-residue pLDDT scores, predicted TMscore, predicted pairwise aligned errors"},{"location":"omics/intro-to-alphafold2/05_af2_output/#assessing-alphafold2-accuracy","title":"Assessing AlphaFold2 Accuracy","text":"<ul> <li>We can assess the accuracy of the AlphaFold prediction using:<ul> <li>Predicted Local Distance Difference Test (pLDDT)</li> <li>Predicted Alignment Errorb</li> </ul> </li> </ul>"},{"location":"omics/intro-to-alphafold2/05_af2_output/#predicted-local-distance-difference-test-plddt","title":"Predicted Local Distance Difference Test (pLDDT)","text":"<ul> <li>per-residue confidence metric\u00a0 ranging from 0-100 (100 being the highest confidence)</li> <li>Regions below 50 could indicate disordered regions</li> </ul>"},{"location":"omics/intro-to-alphafold2/05_af2_output/#predicted-alignment-error-pae","title":"Predicted Alignment Error (PAE)","text":"<ul> <li>The color at (x, y) corresponds to the expected distance error in residue x\u2019s position, when the prediction and true structure are aligned on residue y.</li> <li>So, in the example below:<ul> <li>The darker color indicates a lower error</li> <li>When we are aligning on residue 300, we are more confident in the position of residue 200 and less confident in the position of residue 600</li> </ul> </li> </ul> <ul> <li>The example in the above came from a multimer prediction</li> <li>Here we see that the error is higher when assessing the position between the two chains:</li> </ul>"},{"location":"omics/intro-to-alphafold2/05_af2_output/#plotting-structure-prediction-information","title":"Plotting Structure Prediction Information","text":"<ul> <li>We can leverage the <code>pkl</code> files to gain insight into our structure predictions. To do so we use a python script provided by the VIB Bioinformatics Core which we call <code>vizaf2.py</code>. First we will need to move back up one directory and load the AlphaFold module so that we have the packages needed to run our script.</li> </ul> <pre><code>cd ../../\n</code></pre> <pre><code>ls\n</code></pre> <ul> <li>You should then see the following output:</li> </ul> <pre><code>data  lig1  lig1af2.sh  pcna  pcnaaf2.sh  plotaf2.ipynb  vizaf2.py\n</code></pre> <pre><code>module load alphafold/2.1.1\n</code></pre> <ul> <li>Now we will need to feed our script three arguments:</li> <li><code>--input_dir</code> input directory with model files mentioned above</li> <li><code>--output_dir</code> output directory to put our plots of model information</li> <li><code>--name</code> optional prefix to add to our file names</li> </ul> <pre><code>python vizaf2.py --input_dir pcna/1AXC/ --output_dir pcna/visuals/ --name pcna\n</code></pre> <ul> <li>Running this will generate two images in your output directory:</li> <li><code>pcna_coverage_LDDT.png</code> - plots of your msa coverage and pLDDT scores per residue per model</li> <li><code>pcna_PAE.png</code> - plots of your predicted alignment error for each of your models</li> </ul>"},{"location":"omics/intro-to-alphafold2/05_af2_output/#pcna_coverage_lddtpng","title":"pcna_coverage_LDDT.png","text":""},{"location":"omics/intro-to-alphafold2/05_af2_output/#pcna_paepng","title":"pcna_PAE.png","text":"<ul> <li>Now that we have these plots for the PCNA structure prediction, let's run this on the LIG1 prediction as well!</li> </ul> <pre><code>python vizaf2.py --input_dir lig1/1X9N/ --output_dir lig1/visuals/ --name lig1\n</code></pre>"},{"location":"omics/intro-to-alphafold2/05_af2_output/#lig1_coverage_lddtpng","title":"lig1_coverage_LDDT.png","text":""},{"location":"omics/intro-to-alphafold2/05_af2_output/#lig1_paepng","title":"lig1_PAE.png","text":""},{"location":"omics/intro-to-alphafold2/06_pymol_viz/","title":"PyMOL Visualization","text":""},{"location":"omics/intro-to-alphafold2/06_pymol_viz/#visualizing-with-pymol","title":"Visualizing With Pymol","text":"<ul> <li>In the previous slide we plotted our MSA alignment, the pLDDT scores, and the predicted alignement error. However, it is also useful to visualize the actual predicted protein structure and compare it to the known structure if there is one. Here we use a software called PyMOL to do just that:</li> </ul> <ul> <li>Here we see that PyMOL takes either the PDB ID or a PDB file and creates a vizualization for us to examine. If you have not done so already please download PyMOL and open the app. You should see a window like the follwing:</li> </ul> <ul> <li>Here we have a:</li> <li>History Window with log of previous commands</li> <li>Command Interface to enter PyMOL commands</li> <li>List of Objects Loaded which list of objects/proteins that have been loaded into PyMOL</li> <li> <p>Visualization Window to visualize protiens loaded into PyMOL</p> </li> <li> <p>Let's try on our data!</p> </li> </ul>"},{"location":"omics/intro-to-alphafold2/06_pymol_viz/#download-alphafold-output","title":"Download AlphaFold Output","text":"<ul> <li>First we will need to download our predicted structure pdb files. To this go to Files &gt; Home Directory:</li> </ul> <ul> <li>Then navigate to your AlphaFold Workshop directory, where you will note the two folders we examined earlier that have our AlphaFold Outputs:</li> </ul> <ul> <li>In each folder you will note a <code>visuals</code> folder and an ID number. Navigate to the ID number folder and download the file \"ranked_0.pdb\" - this structure is AlphaFold's best prediction of the protein's structure.</li> </ul> <ul> <li>Make sure to add a prefix to each <code>ranked_0.pdb</code> file as to not confuse the two. You can rename the <code>ranked_0.pdb</code> in the <code>lig1</code> folder to <code>lig1_ranked_0.pdb</code> and the <code>ranked_0.pdb</code> in the <code>pcna</code> folder to <code>pcna_ranked_0.pdb</code>. </li> </ul>"},{"location":"omics/intro-to-alphafold2/06_pymol_viz/#alphafold-output-in-pymol","title":"AlphaFold Output In PyMOL","text":"<ul> <li>To visualize these protein structures in PyMOL go to File &gt; Open - then choose your pdb files. </li> <li>Loading two objects can make it difficult to see examine both individually, so enter the following command to hide lig1:</li> </ul> <p><pre><code>disable lig1_ranked_0\n</code></pre> - Now you should only see <code>pnca_ranked_0</code>. It would be interesting to see how well this prediction lines up with the know structure for PCNA. We can load that structure with the following command</p> <p><pre><code>fetch 1axc\n</code></pre> - To see how well our predicted structure lines up we can compare the two by aligning them:</p> <pre><code>align pcna_ranked_0, 1axc\n</code></pre> <p></p> <ul> <li>In the history window you'll note that when we aligned our structures we were given an RMSD or root mean square deviation value. The smaller this value is, the better our two structures have aligned. </li> <li>Now that we have aligned the predicted PCNA structure, repeat these steps to align LIG1 (the PDB ID for LIG1 id <code>1x9n</code>).</li> </ul>"},{"location":"omics/intro-to-alphafold2/06_pymol_viz/#alphafold2-limitations","title":"AlphaFold2 Limitations","text":"<p>AlphaFold2 attempts to predict protein structures based on available structure data in the PDB. But do these structures in the PDB reflect actual protein structures?</p> <ul> <li>PDB structures are usually created from experiments where the context of that structure is specific to the study question. </li> </ul> <p>For example there are lots of studies examining what a particular protein structure looks like when bound to ions, when it\u2019s chemically modified, or when its in larger complexes</p> <ul> <li>Protein interactions/multimers might not be captured in the PDB database. Given this, AlphaFold2\u2019s multimeric prediction might not be reflective of the true interaction structure.</li> <li>Proteins can also contain disordered regions (i.e. loops), which are difficult to crystallize and as such AlphaFold\u2019s prediction of these disordered regions is bound to be poor.</li> </ul> <p>AlphaFold2 is indeed a powerful tool but just be aware of what it is prediciting and if any of the items mentioned above interfere with the study question you are using AlphaFold2 to answer!</p>"},{"location":"omics/intro-to-alphafold2/06_pymol_viz/#references","title":"References","text":"<p>References</p> <ol> <li>PROTEIN</li> <li>Protein Function</li> <li>Analyzing Protein Structure and Function</li> <li>serial scanning 3D electron microscopy</li> <li>AI revolutions in biology</li> <li>Methods for Determining Atomic Structures</li> <li>X-ray crystallography</li> <li>AlphaFold</li> <li>Levinthal's paradox</li> <li>AlphaFold: a solution to a 50-year-old grand challenge in biology</li> <li>Protein Structure Prediction Center</li> <li>Neural network</li> <li>Understanding LSTMs</li> <li>Attention Is All You Need</li> <li>Transformer Neural Network: Step-By-Step Breakdown of the Beast</li> <li>FASTA Format</li> <li>Multiple Sequence Alignment</li> <li>Origins of coevolution between residues distant in protein 3D structures</li> <li>AlphaFold 2 is here: what\u2019s behind the structure prediction miracle</li> <li>Alphafold Github</li> <li>Q9FX77</li> <li>Limitations of AlphaFold</li> </ol>"},{"location":"omics/intro-to-alphafold2/07_af2_batch/","title":"Optional: AlphaFold2 Batch Script","text":""},{"location":"omics/intro-to-alphafold2/07_af2_batch/#log-into-the-hpc-clusters-on-demand-interface","title":"Log into the HPC cluster\u2019s On Demand Interface","text":"<ul> <li>Open a Chrome browser and go to On Demand</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <ul> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <pre><code>[tutln01@login001 ~]$\n</code></pre> <ul> <li>This indicates you are logged in to the login node of the cluster. DO NOT run anything from this node as it is a shared login node. To complete jobs we either need to start an interactive session to get on to a compute node (more details can be found here about this option) or we can write a batch script to submit our job to a compute node.</li> </ul>"},{"location":"omics/intro-to-alphafold2/07_af2_batch/#alphafold2-batch-script","title":"AlphaFold2 Batch Script","text":"<p>A batch script can be broken into two parts - the header section with information on how to run the job and a command section where we use UNIX commnads to do a job. Here is our script:</p> <pre><code>#!/bin/bash\n#SBATCH -p preempt                          # partition we submit to\n#SBATCH -n 8                                # The number of cpu cores we would like\n#SBATCH --mem=64g                           # The amount of RAM we would like\n#SBATCH --time=2-0:00:00                    # The time we think our job will take       \n#SBATCH -o output.%j                        # The name of the output file\n#SBATCH -e error.%j                         # The name of the error file\n#SBATCH -N 1                                # The number of nodes we would like\n#SBATCH --gres=gpu:1                        # The number of GPUs\n#SBATCH --exclude=c1cmp[025-026]            # The nodes to exclude when using AlphaFold2\n\n# Load the AlphaFold2 and NVIDIA modules\nmodule load alphafold/2.1.1\nnvidia-smi\n\n# Make the results direcory\nmkdir /path/to/your/home/directory/af2\n\n# Specify where your output directory and raw data are\noutputpath=/path/to/your/home/directory/af2\nfastapath=/path/to/your/home/directory/data/1AXC.fasta\n\n# Date to specify if you want to avoid using template\nmaxtemplatedate=2020-06-10\n\nsource activate alphafold2.1.1\n\n# Running alphafold 2.1.1\nrunaf2 -o $outputpath -f $fastapath -t $maxtemplatedate -m multimer \n</code></pre> <p>What do these commands mean?</p> <ul> <li><code>module load alphafold/2.1.1</code>: load the AlphaFold2 module</li> <li><code>nvidia-smi</code>: load the NVIDIA-SMI module</li> <li><code>mkdir /path/to/your/home/directory/af2</code>: make a directory for our outputs</li> <li><code>outputpath=/path/to/your/home/directory/af2</code>: specify where our outputs should go</li> <li><code>fastapath=/path/to/your/home/directory/data/1AXC.fasta</code>: specify our input FASTA file</li> <li><code>maxtemplatedate=2020-06-10</code>: specify our maximum template date</li> <li><code>source activate alphafold2.1.1</code>: activate our AlphaFold2 conda environment</li> <li><code>runaf2 -o $outputpath -f $fastapath -t $maxtemplatedate -m multimer</code>: run our AlphaFold2 program<ul> <li><code>-o</code>: output path</li> <li><code>-f</code>: our input FASTA file path</li> <li><code>-t</code>: our maximum template date</li> <li><code>-m</code>: whether or not this is a multimeric protein</li> </ul> </li> </ul> <p>What's up with the maxtemplatedate?</p> <p>The <code>maxtemplatedate</code> option is a bit more complicated. If we ask AlphaFold to predict the structure of a protein with a structure already in the Protein Data Bank (PDB) - then we have the option of using that structure in the prediction. If we do not want AlphaFold 2 to use this structure in the prediction we need to specify a date before the release date of that structure.</p> What do the SBATCH commands mean? <code>command</code> description <code>#!/bin/bash</code> specify our script is a bash script <code>#SBATCH -p ccgpu</code> The partition we are requesting, and if you don't have ccgpu access, use \"preempt\" <code>#SBATCH -n 8</code> The number of cpu cores we would like - here it is 8 <code>#SBATCH --mem=64g</code> The amount of RAM we would like - here it is 64 Gigabytes <code>#SBATCH --time=2-0:00:00</code> The time we think our job will take - here we say 2 days (days-hours:minutes:seconds) <code>#SBATCH -o output.%j</code> The name of the output file - here it is \"output.jobID\" <code>#SBATCH -N 1</code> The number of nodes we would like - here it is 1 <code>#SBATCH --gres=gpu:1</code> The number of GPUs - here we ask for 1 <code>#SBATCH --exclude=c1cmp[025-026]</code> These are nodes to exclude when using AlphaFold2"},{"location":"omics/intro-to-alphafold2/07_af2_batch/#run-the-alphafold2-batch-script","title":"Run the AlphaFold2 Batch Script","text":"<ul> <li>First save your batch script, here we will save it as <code>runaf2.sh</code>. <ul> <li>The bash script must end in <code>.sh</code></li> </ul> </li> <li>Now submit your script with the command:<ul> <li><code>sbatch runaf2.sh</code></li> </ul> </li> <li>To check on the status of this script use the following command:<ul> <li><code>squeue -u your_utln</code></li> <li>Be sure to swap out <code>your_utln</code> with your Tufts utln!</li> </ul> </li> </ul>"},{"location":"omics/intro-to-ngs/01_Setup/","title":"Setup","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-ngs/01_Setup/#goals","title":"Goals","text":"<ul> <li>Connect to the HPC cluster via On Demand Interface</li> <li>Download data</li> </ul>"},{"location":"omics/intro-to-ngs/01_Setup/#log-into-the-hpc-clusters-on-demand-interface","title":"Log into the HPC cluster's On Demand interface","text":"<ul> <li>Open a Chrome browser and enter the URL https://ondemand.cluster.tufts.edu</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <ul> <li>Type your password at the prompt (the password will be hidden for security purposes): <code>tutln01@login.cluster.tufts.edu's password:</code></li> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <p><code>[tutln01@login001 ~]$</code></p> <p>This indicates you are logged in to the login node of the cluster. - Type <code>clear</code> to clear the screen</p>"},{"location":"omics/intro-to-ngs/01_Setup/#set-up-for-the-analysis","title":"Set up for the analysis","text":""},{"location":"omics/intro-to-ngs/01_Setup/#find-500m-storage-space","title":"Find 500M storage space","text":"<ul> <li>Check how much available storage you have in your home directory by typing <code>showquota</code>.</li> </ul> <p>Result: <pre><code>Home Directory Quota\nDisk quotas for user tutln01 (uid 31394):\n     Filesystem  blocks   quota   limit   grace   files   quota   limit   grace\nhpcstore03:/hpc_home/home\n                  1222M   5120M   5120M            2161   4295m   4295m        \n\n\nListing quotas for all groups you are a member of\nGroup: facstaff Usage: 16819478240KB    Quota: 214748364800KB   Percent Used: 7.00%\n</code></pre></p> <p>Under <code>blocks</code> you will see the amount of storage you are using, and under quota you see your quota. Here, the user has used 1222M of the available 5120M and has enough space for our analysis.</p> <ul> <li>If you do not have 500M available, you may have space in a project directory for your lab. These are located in <code>/cluster/tufts</code> with names like <code>/cluster/tufts/labname/username/</code>. If you don't know whether you have project space, please email tts-research@tufts.edu.</li> </ul>"},{"location":"omics/intro-to-ngs/01_Setup/#download-the-data","title":"Download the data","text":"<ul> <li>Get an interaction session on a compute node (3 hours, 16 Gb memory, 4 cpu on 1 node) on the default partition (<code>batch</code>) by typing:</li> </ul> <p><code>srun --pty -t 3:00:00  --mem 16G  -N 1 --cpus 4 bash</code></p> <p>Notes:  If wait times are very long, you can try a different partitions by adding, e.g. <code>-p preempt</code> or <code>-p interactive</code> before <code>bash</code>. If you go through this workshop in multiple steps, you will have to rerun this step each time you log in.</p> <ul> <li>Change to your home directory</li> </ul> <p><code>cd</code></p> <p>Or, if you are using a project directory:</p> <p><code>cd /cluster/tufts/labname/username/</code></p> <ul> <li>Copy the course directory and all files in the directory (-R is for recursive):   </li> </ul> <p><code>cp -R /cluster/tufts/bio/tools/training/intro-to-ngs/ .</code> </p> <p>(Also available via: \u00a0<code>git clone https://gitlab.tufts.edu/rbator01/intro-to-ngs.git</code>)</p> <ul> <li>Take a look at the contents using the <code>tree</code> command:</li> </ul> <p><code>tree intro-to-ngs</code></p> <p>You'll see a list of all files <pre><code>intro-to-ngs\n\u251c\u2500\u2500 all_commands.sh          &lt;-- Bash script with all commands\n\u251c\u2500\u2500 raw_data                 &lt;-- Folder with paired end fastq files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 na12878_1.fq         \n\u2502\u00a0\u00a0 \u2514\u2500\u2500 na12878_2.fq\n\u251c\u2500\u2500 README.md                &lt;-- Contents description\n\u2514\u2500\u2500 ref_data                 &lt;-- Folder with reference sequence\n\u00a0 \u00a0 \u2514\u2500\u2500 chr10.fa\n2 directories, 5 files\n</code></pre></p>"},{"location":"omics/intro-to-ngs/01_Setup/#data-for-the-class","title":"Data for the class","text":"<p>Genome In a Bottle (GIAB) was initiated in 2011 by the National Institute of Standards and Technology \"to develop the technical infrastructure (reference standards, reference methods, and reference data) to enable translation of whole human genome sequencing to clinical practice\" (Zook et al 2012).  We'll be using a DNA Whole Exome Sequencing (WES) dataset released by GIAB for the purposes of benchmarking bioinformatics tools.</p> <p></p> <p>The source DNA, known as NA12878, was taken from a single person: the daughter in a father-mother-child 'trio'. She is also mother to 11 children of her own, for whom sequence data is also available. (HBC Training). Father-mother-child 'trios' are often sequenced to study genetic links between family members.</p> <p>As mentioned in the introduction, WES is a method to concentrate the sequenced DNA fragments in coding regions (exons) of the genome.</p> <p></p> <p>For this class, we've created a small dataset of reads that align to a single gene that will allow our commands to finish quickly.</p> <p>Sample: NA12878</p> <p>Gene: Cyp2c19 on chromosome 10</p> <p>Sequencing: Illumina, Paired End, Exome</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/","title":"Quality Control","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#goals","title":"Goals","text":"<ul> <li>Understand FASTQ file format</li> <li>Run FastQC to asses data quality</li> </ul>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#assess-the-quality-of-the-raw-data","title":"Assess the quality of the raw data","text":""},{"location":"omics/intro-to-ngs/02_Quality_Control/#fastq-format","title":"FASTQ format","text":"<p>FASTQ files is the most common way to store biological sequence data. Depending on the sequencing protocol, a single FASTQ file can represent an entire flow cell, a single lane, a single sample, or a portion of a sample. We have two FASTQ files in our <code>raw_data</code> folder, which are the paired end data of a single sample.</p> <p>From our course directory <code>into-to-ngs</code> change into the <code>raw data</code> directory: <pre><code>cd raw_data\n</code></pre></p> <p>Use the command <code>head</code> to look at the first few lines of our first FASTQ file.</p> <pre><code>head na12878_1.fq\n</code></pre> <p>Each read in our file is represented are by four lines: An identifier, the nucleotide sequence, an optional second identifier and a quality string. Below is an example, the  arrows on the right show explanation of each line:</p> <pre><code>@SRR098401.109756285/1                   &lt;-- Sequence identifier: @ReadID / 1 or 2 of pair\nGACTCACGTAACTTTAAACTCTAACAGAAATATACTA\u2026   &lt;-- Sequence\n+                                        &lt;-- + (optionally lists the sequence identifier again)\nCAEFGDG?BCGGGEEDGGHGHGDFHEIEGGDDDD\u2026      &lt;-- Quality String\n</code></pre>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#base-quality-scores","title":"Base Quality Scores","text":"<p>The fourth line of each read is called the quality string. Each symbol in the string is an encoding of the quality score, representing the inferred base call accuracy at that  position in the read.  The manufacturer of the sequencing instrument has performed calibration of quality score by sequencing many  well-characterized samples from multiple organisms  and studying the correspondence between properties of the signal  generated by the cluster being sequenced and the accuracy of the resulting base call.</p> <p>The following two images explain this encoding. The first image shows the mapping of the encoded quality score to the quality score:</p> <p></p> <p>The second image shows the mapping of the quality score to the inferred base call accuracy:</p> <p></p> <p>Looking back at our sample read, we can see that the first base has an encoded quality score of <code>C</code>. Using the first image above, we see that C encodes a quality of 34. Using the second table, we see that the probability is &lt; 1/1000 of that base being an error. In the next section, we'll see how quality scores and other quality control metrics are used to evaluate the quality of  a sequenced sample. </p> <p>More information on Quality scores from Illumina</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#fastqc","title":"FastQC","text":"<p>FastQC is widely used tool for both DNA and RNA sequencing data in order to evaluate the quality of the sequencing data.</p> <p>To use, load the module:  <pre><code>module load fastqc/0.11.8\n</code></pre></p> <p>To see the input options, type: <pre><code>fastqc --help\n</code></pre></p> <p>Result: <pre><code>fastqc --help\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n    fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN\n...\n</code></pre></p> <p>FastQC is run on each FASTQ file separately in order to be sensitive to the variation in quality over lanes, samples,  and paired-end files.</p> <p>Since FastQC can run on multiple files at once, we'll use a wildcard <code>*</code> to indicate each file in the folder <code>raw_data</code>,  and we specify that the output should be placed in the directory we created called <code>fastqc</code>:</p> <pre><code>cd ..\nmkdir fastqc\nfastqc raw_data/* -o fastqc\n</code></pre> <p>Result: <pre><code>Started analysis of na12878_1.fq\nApprox 20% complete for na12878_1.fq\nApprox 40% complete for na12878_1.fq\nApprox 65% complete for na12878_1.fq\nApprox 85% complete for na12878_1.fq\nAnalysis complete for na12878_1.fq\nStarted analysis of na12878_2.fq\nApprox 20% complete for na12878_2.fq\nApprox 40% complete for na12878_2.fq\nApprox 65% complete for na12878_2.fq\nApprox 85% complete for na12878_2.fq\nAnalysis complete for na12878_2.fq\n</code></pre></p> <p>To view the resulting files: <pre><code>ls fastqc\n</code></pre></p> <p>The result shows an <code>html</code> file showing graphical results and a <code>zip</code> file containing the raw data for each input FASTQ file. The easist way to view the result is to open the <code>html</code> files in a web browser. <pre><code>na12878_1_fastqc.html  na12878_1_fastqc.zip  na12878_2_fastqc.html  na12878_2_fastqc.zip\n</code></pre></p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#view-results-in-the-on-demand-browser","title":"View results in the On Demand browser","text":"<p>To view the graphical results, return to the tab ondemand.cluster.tufts.edu</p> <p>On the top menu bar choose <code>Files-&gt;Home Directory</code></p> <p></p> <p>Navigate to the <code>fastqc</code> folder in course directory, e.g.: <code>/home/username/intro-to-ngs/fastqc/</code> Right click on the file <code>na12878_1_fastqc.html</code> and select <code>Open in new tab</code>.</p> <p></p> <p>The new tab that opens in the browser has the results of FastQC for the first reads in the sample. We'll go through each plot. Note that the plots shown are representative results for WES data of varying quality, rather than those generated  on the course data.</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#per-base-sequence-quality","title":"Per base sequence quality","text":"<p>The first plot shows the quality scores vs. position in the read, for all reads in the file.</p> <p></p> <p>For each position a Box and Whisker type plot is drawn. The elements of the plot are as follows: - The central red line is the median value - The yellow box represents the inter-quartile range (25-75%) - The upper and lower whiskers represent the 10% and 90% points - The blue line represents the mean quality</p> <p>The background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red).  It is typical for reads generated by Illumina platforms to show reduced quality at the ends of reads due to fragments in  a cluster becoming out-of-sync (Fuller et al 2009).</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#per-sequence-quality-scores","title":"Per sequence quality scores","text":"<p>The Per Sequence Quality Score plots the distribution of mean sequence quality. This plot allows will show a peak toward lower mean quality if there is a subset of sequences with  low quality values. </p> <p></p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#per-base-sequence-content","title":"Per base sequence content","text":"<p>The Per Base Sequence Content plot shows the the proportion of each base called at each position in the read,  for all reads in the file.</p> <p></p> <p>In a random library you would expect that bases would be present in equal proportions. In any given genome, however, the relative amount of each base will reflect the overall amount of these bases in your genome. In any case, we would expect the lines to run parallel to each other. If you see strong biases which change in different bases then this usually indicates an overrepresented sequence which is contaminating your library. Below is an example of a library that was contaminated  with adapter dimers (from sequencing.qcfail.com)[https://sequencing.qcfail.com/articles/contamination-with-adapter-dimers/].</p> <p></p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#per-sequence-gc-content","title":"Per sequence GC content","text":"<p>This plot displays the fraction of G and C bases across for all sequences in the file. and compares it to a modelled normal distribution of GC content.</p> <p></p> <p>In a normal random library you would expect to see a roughly normal distribution of GC content where the central peak corresponds to the overall GC content of the underlying genome. The expected GC content is calculated from the observed data and used to build a reference distribution. An unusually shaped distribution could indicate a contaminated library or some other kinds of biased subset. A normal distribution which is shifted indicates some systematic bias which is independent of base position.</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#per-base-n-content","title":"Per base N content","text":"<p>If a sequencer is unable to make a base call with sufficient confidence then it will normally substitute an N rather than a conventional base.  This plot shows the percentage of base calls at each position for which an N was substituted.</p> <p></p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#sequence-length-distribution","title":"Sequence Length Distribution","text":"<p>This plot shows the distribution of read sizes in the file. Depending on the sequencing method and whether reads have been post-processed, it may be expected to have reads of a  uniform length or varying lengths. For our raw WES dataset we see a sharp peak at 76 bases, as expected.</p> <p></p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#sequence-duplication-levels","title":"Sequence Duplication Levels","text":"<p>This plot shows the distribution of sequence duplicates in the file. For example, in the below plot, over 80% of the total sequences are present only once and 10% are present twice.</p> <p></p> <p>In a diverse library most sequences will occur only once in the final set. A high level of duplication may indicate low library complexity or an enrichment bias (e.g. PCR over amplification).</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#overrepresented-sequences","title":"Overrepresented sequences","text":"<p>This plot shows the sequences in the file which make up more than 0.1% of the total. A normal high-throughput library will contain a diverse set of sequences, with no individual sequence making up a more than a tiny fraction of the whole. Finding that a single sequence is very overrepresented in the set either means that it is highly biologically significant, or indicates that the library is contaminated, or not as diverse as you expected.</p> <p>For each overrepresented sequence the program will look for matches in a database of common contaminants and will report the best hit it finds. Hits must be at least 20bp in length and have no more than 1 mismatch. Finding a hit doesn't necessarily mean that this is the source of the contamination, but may point you in the right direction. It's also worth pointing out that many adapter sequences are very similar to each other so you may get a hit reported which isn't technically correct, but which has very similar sequence to the actual match.</p> <p>If overrepresented sequences are found but not identified by FastQC, try a  BLAST search.</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#adapter-content","title":"Adapter Content","text":"<p>This module looks for common adapters in the sequence.</p> <p></p> <p>Explanations adapted from [https://dnacore.missouri.edu/PDF/FastQC_Manual.pdf][https://dnacore.missouri.edu/PDF/FastQC_Manual.pdf]</p>"},{"location":"omics/intro-to-ngs/02_Quality_Control/#optional-read-trimming","title":"Optional: Read trimming","text":"<p>In our <code>Per base sequence quality</code> we saw that the read quality dropped towards the end of the read. In order to ensure alignment and variant calling are as accurate as possible, we can perform quality trimming of reads.</p> <p>Trim Galore is a popular tool  that in the default mode performs two types of trimming: Quality trimming: Trims low quality bases from the 3' end of the read Adapter trimming: Automatically detects and removes known Illumina adapters that may be present in the data</p> <p>To perform trimming on the data, we first load the software which is installed as an a conda environment. For more information on using anaconda on the HPC, see this tutorial.</p> <pre><code>module load anaconda/3\nsource activate /cluster/tufts/bio/tools/conda_envs/trim_galore/\n</code></pre> <p>To run: <pre><code>mkdir trim\ntrim_galore -o trim raw_data/*\n</code></pre></p> <p>Result: <pre><code>...\n=== Summary ===\n\nTotal reads processed:                   4,652\nReads with adapters:                     1,606 (34.5%)\nReads written (passing filters):         4,652 (100.0%)\n\nTotal basepairs processed:       353,552 bp\nQuality-trimmed:                  24,906 bp (7.0%)\nTotal written (filtered):        326,448 bp (92.3%)\n...\n</code></pre></p> <p>Note that Trim Galore may trim adapters even in the case where FastQC found no adapters. This is because Trim Galore will remove partial adapters at the ends of reads.</p> <p>The result after trimming is much improved:</p> <p></p>"},{"location":"omics/intro-to-ngs/03_Alignment/","title":"Alignment","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-ngs/03_Alignment/#goals","title":"Goals","text":"<ul> <li>Align short reads to a references genome with BWA</li> <li>View alignment using IGV</li> </ul>"},{"location":"omics/intro-to-ngs/03_Alignment/#bwa-overview","title":"BWA Overview","text":"<p>Burrows-Wheeler Aligner (BWA) is a software package for mapping low-divergent  sequences against a large reference genome, such as the human genome.  The naive approach to read alignment is to compare a read to every position in the reference genome until a good match  is found is far too slow.  BWA solves this problem by creating an \"index\" of our reference sequence for faster lookup.</p> <p>The following figure shows a short read with a red segment followed by a blue segment that  we seek to align to a genome containing many blue and red segments. The table keeps track of all the locations where a given pattern of red and blue segments (seed sequence) occurs in the  reference genome. When BWA encounters a new read, it looks up the seed sequence at the beginning of the read in the table  and retrieves a set of positions that are potential alignment positions for that read.  This speeds up the search by reducing the number of positions to check for a good match.</p> <p></p> <p>BWA has three algorithms:</p> <ul> <li>BWA-backtrack:\u00a0designed for Illumina sequence reads up to 100bp (3-step)</li> <li>BWA-SW:\u00a0 designed for longer sequences ranging from 70bp to 1Mbp, long-read support and split alignment</li> <li>BWA-MEM:\u00a0optimized for 70-100bp Illumina reads</li> </ul> <p>We'll use BWA-MEM.  Underlying the BWA index is the Burrows-Wheeler Transform This is beyond the scope of this course but is an widely used data compression algorithm.</p>"},{"location":"omics/intro-to-ngs/03_Alignment/#bwa-index","title":"BWA Index","text":"<p>In the following steps we'll create the BWA index. </p> <ol> <li> <p>Change to our reference data directory <code>cd intro-to-ngs/ref_data</code></p> </li> <li> <p>Preview our genome using the command <code>head</code> by typing:</p> </li> </ol> <p><code>head chr10.fa</code> </p> <p>You'll see the first 10 lines of the file <code>chr10.fa</code>: <pre><code>&gt;chr10 AC:CM000672.2 gi:568336\u2026   &lt;-- '&gt;' charachter followed by sequence name\nNNNNNNNNNNNNNNNNNNNNN             &lt;-- sequence\n\u2026\n</code></pre> This is an example of FASTA format. FASTA format is similar to the first two lines of FASTQ format, storing only the  sequence name and sequence.</p> <ol> <li>Load the BWA module, which will give us access to the <code>bwa</code> program: <pre><code>module load bwa/0.7.17\n</code></pre></li> </ol> <p>Test it out without any arguments in order to view the help message. <pre><code>bwa\n</code></pre></p> <p>Result: <pre><code>Program: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1198-dirty\nContact: Heng Li &lt;lh3@sanger.ac.uk&gt;\n\nUsage: \u00a0 bwa &lt;command&gt; [options]\n\nCommand: index \u00a0 \u00a0 \u00a0 \u00a0 index sequences in the FASTA format\n\u2026\n</code></pre></p> <p>Use the <code>bwa index</code> command to see usage instructions for genome indexing</p> <pre><code>bwa index\n</code></pre> <p>Result <pre><code>Usage: \u00a0 bwa index [options] &lt;in.fasta&gt;\nOptions: -a STR\u00a0 \u00a0 BWT construction algorithm \u2026\n</code></pre></p> <p>Run the command as instructed, using the default options: <pre><code>bwa index chr10.fa\n</code></pre></p> <p>Result: <pre><code>[bwa_index] Pack FASTA... 0.93 sec\n[bwa_index] Construct BWT for the packed sequence...\n[BWTIncCreate] textLength=267594844, availableWord=30828588\n    [BWTIncConstructFromPacked] 10 iterations done. 50853228 characters processed.\n[BWTIncConstructFromPacked] 20 iterations done. 93947292 characters processed.\n[BWTIncConstructFromPacked] 30 iterations done. 132245372 characters processed.\n[BWTIncConstructFromPacked] 40 iterations done. 166280796 characters processed.\n[BWTIncConstructFromPacked] 50 iterations done. 196527516 characters processed.\n[BWTIncConstructFromPacked] 60 iterations done. 223406844 characters processed.\n[BWTIncConstructFromPacked] 70 iterations done. 247293244 characters processed.\n[BWTIncConstructFromPacked] 80 iterations done. 267594844 characters processed.\n[bwt_gen] Finished constructing BWT in 80 iterations.\n[bwa_index] 59.13 seconds elapse.\n[bwa_index] Update BWT... 0.67 sec\n[bwa_index] Pack forward-only FASTA... 0.59 sec\n[bwa_index] Construct SA from BWT and Occ... 24.98 sec\n[main] Version: 0.7.17-r1198-dirty\n[main] CMD: bwa index chr10.fa\n[main] Real time: 87.087 sec; CPU: 86.306 sec\n</code></pre></p> <p>When it's done, take a look at the files produced by typing <code>ls</code>. The following is the result, with arrows and text on the right giving an explanation of each file.</p> <pre><code>chr10.fa\u00a0     &lt;-- Original sequence\nchr10.fa.amb\u00a0 &lt;-- Location of ambiguous (non-ATGC) nucleotides\nchr10.fa.ann\u00a0 &lt;-- Sequence names, lengths\nchr10.fa.bwt\u00a0 &lt;-- BWT suffix array\nchr10.fa.pac\u00a0 &lt;-- Binary encoded sequence\nchr10.fa.sa   &lt;-- Suffix array index\n</code></pre>"},{"location":"omics/intro-to-ngs/03_Alignment/#bwa-alignment","title":"BWA alignment","text":"<p>Let's check the usage instructions for BWA mem by typing <code>bwa mem</code></p> <pre><code>Usage: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq]\n\nAlgorithm options:\n\n       -t INT        number of threads [1]\n       -k INT        minimum seed length [19]\n       -w INT        band width for banded alignment [100]\n       -d INT        off-diagonal X-dropoff [100]\n       -r FLOAT      look for internal seeds inside a seed longer than {-k} * FLOAT [1.5]\n       -y INT        seed occurrence for the 3rd round seeding [20]\n       -c INT        skip seeds with more than INT occurrences [500]\n       -D FLOAT      drop chains shorter than FLOAT fraction of the longest overlapping chain [0.50]\n       -W INT        discard a chain if seeded bases shorter than INT [0]\n       -m INT        perform at most INT rounds of mate rescues for each read [50]\n       -S            skip mate rescue\n       -P            skip pairing; mate rescue performed unless -S also in use\n...\n</code></pre> <p>Since our alignment command will have multiple arguments, it will be convenient to write a script.</p> <p>Go up one level to our main <code>intro-to-ngs</code> directory: <pre><code>cd ..\n</code></pre></p> <p>Make a new directory for our results <pre><code>mkdir results\n</code></pre></p> <p>Open a text editor with the program <code>nano</code> and create a new file called <code>bwa.sh</code>. <pre><code>nano bwa.sh\n</code></pre></p> <p>Enter the following text. Note that each line ends in a single backslash <code>\\</code>, which will be read as a line continuation. Be careful to put a space before the backslash and not after. This serves to make the script more readable.</p> <pre><code>module load bwa/0.7.17\n\nbwa mem \\\n-t 2 \\\n-M \\\n-R \"@RG\\tID:reads\\tSM:na12878\\tPL:illumina\" \\\n-o results/na12878.sam \\\nref_data/chr10.fa \\\nraw_data/na12878_1.fq \\\nraw_data/na12878_2.fq\n</code></pre> <p>Let's look line by line at the options we've given to BWA: 1. <code>-t 2</code> : BWA runs two parallel threads. Alignment is a task that is easy to parallelize  because alignment of a read is independent of other reads. Recall that in Setup we asked for a compute  node allocation with  <code>--cpus=4</code>, which can process up to 8 threads. Here we are using only 2 threads. </p> <ol> <li> <p><code>-M</code> : \"mark shorter split hits as secondary\". This option will change the SAM flag (discussed in next section) that  is assigned to short reads that have read segments mapped to distant locations. It optionn is needed for GATK/Picard compatibility, which are tools we use downstream. see this explanation on biostars</p> </li> <li> <p><code>-R \"@RG\\tID:reads\\tSM:na12878\\tPL:illumina\"</code>: Add a read group tag (RG), sample name (SM), and platform (PL) to our alignment file header.  We'll see where this appears in our output. In addition to being required for GATK, it's advisable to always add these  labels to make the origin of the reads clear.</p> </li> <li> <p><code>-o results/na12878.sam</code> :  Place the output in the results folder and give it a name</p> </li> <li> <p>The following arguments are our reference, read1 and read2 files, in the order required by BWA: <pre><code>ref_data/chr10.fa \\\nraw_data/na12878_1.fq \\\nraw_data/na12878_2.fq\n</code></pre></p> </li> </ol> <p>Exit nano by typing <code>^X</code> and follow prompts to save and name the file <code>bwa.sh</code>.</p> <p>Now we can run our script. <pre><code>sh bwa.sh\n</code></pre></p> <p>Result: <pre><code>[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 9304 sequences (707104 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 2256, 0, 0)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (120, 160, 216)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 408)\n[M::mem_pestat] mean and std.dev: (172.35, 67.15)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 504)\n[M::mem_pestat] skip orientation RF as there are not enough pairs\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_process_seqs] Processed 9304 reads in 1.034 CPU sec, 0.518 real sec\n</code></pre></p> <p>List the files in the results directory by typing <code>ls results</code>. Result: <pre><code>na12878.sam\n</code></pre></p>"},{"location":"omics/intro-to-ngs/03_Alignment/#sequence-alignment-map-sam","title":"Sequence Alignment Map (SAM)","text":"<p>Take a look at the output file: <pre><code>cd results\nhead na12878.sam\n</code></pre> The file has two sections</p> <p>Header: <pre><code>@SQ     SN:chr10        LN:133797422        &lt;-- Reference sequence name (SN) and length (LN)\n@RG     ID:reads        SM:na12878          &lt;-- Read group (ID) and sample (SM) information that we provided\n@PG ID:bwa PN:bwa VN:0.7.17\u2026 CL:bwa mem     &lt;-- Programs and arguments used in processing\n</code></pre></p> <p>Alignment:</p> 1 2 3 4 5 6 7 8 9 10 11 SRR098401.109756285 83 chr10 94760653 60 76M = 94760647 -82 CTAA\u2026 D?@A... SRR098401.109756285 163 chr10 94760647 60 76M = 94760653 82 ATTA\u2026 ?&gt;@@... <p>The fields: 1. Read ID 2. Flag: indicates alignment information e.g. paired, aligned, etc. Here is a useful site to decode flags. 3. Reference sequence name 4. Position on the reference sequence where mapping starts 5. Mapping Quality 6. CIGAR string: summary of alignment, e.g. match (M), insertion (I), deletion (D) 7. RNEXT: Name of reference sequence where the other read in the pair aligns 8. PNEXT: Position in the reference sequence where the other read in the pair aligns 9. TLEN: Template length, size of the original DNA or RNA fragment 10. Read Sequence 11. Read Quality</p> <p>More information on SAM format.</p>"},{"location":"omics/intro-to-ngs/03_Alignment/#alignment-quality-control","title":"Alignment Quality Control","text":"<p>Next, we'd like to know how well our reads aligned to the reference genome? We'll use a tool called <code>Samtools</code> to summarize the SAM Flags.</p> <p>To load the module: <pre><code>module load samtools/1.9\n</code></pre></p> <p>To run the <code>flagstat</code> program on our <code>SAM</code> file: <pre><code>samtools flagstat na12878.sam\n</code></pre></p> <p>Result: <pre><code>9306 + 0 in total (QC-passed reads + QC-failed reads)        &lt;-- We have only QC pass reads\n2 + 0 secondary                                              &lt;-- 2 reads have &gt;1 alignment position \n0 + 0 supplementary                                          &lt;-- for reads that align to multiple chromosomes\n0 + 0 duplicates                                             \n9271 + 0 mapped (99.62% : N/A)                               &lt;-- For exome data, &gt;90% alignment is expected    \n9304 + 0 paired in sequencing\n4652 + 0 read1\n4652 + 0 read2\n9226 + 0 properly paired (99.16% : N/A)\n9240 + 0 with itself and mate mapped\n29 + 0 singletons (0.31% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre></p> <p>Samtools flagstat is a great way to check to make sure that the aligment meets the quality expected. In this case, &gt;99% properly paired and mapped indicates a high quality alignment.</p>"},{"location":"omics/intro-to-ngs/03_Alignment/#summary","title":"Summary","text":""},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/","title":"Alignment Cleanup","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Sort and Index SAM/BAM files</li> <li>Mark duplicate reads in BAM file</li> </ul>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#sort-sam-file","title":"Sort SAM file","text":"<p>Downstream applications require that reads in SAM files be sorted by reference genome coordinates (fields 3 and 4 in each line of our SAM file). This will assist in fast search, display and other functions.</p> 1 2 3 4 5 6 7 8 9 10 11 SRR098401.109756285 83 chr10 94760653 60 76M = 94760647 -82 CTAA\u2026 D?@A... <p>We\u2019ll use the Picard toolkit for this and othter SAM file manipulations.</p> <p>Open another script in our course directory called picard.sh <pre><code>cd ..\nnano picard.sh\n</code></pre></p> <p>Enter the following text: <pre><code>module load picard/2.8.0\n\npicard SortSam \\\nINPUT=results/na12878.sam \\\nOUTPUT=results/na12878.srt.bam \\\nSORT_ORDER=coordinate\n</code></pre></p> <p>We have input our SAM file and we will output a Binary Alignment Map (BAM) file, which is a compressed version of SAM format.</p> <p>Exit nano by typing <code>^X</code> and follow prompts to save the file <code>picard.sh</code>.</p> <p>To run the script: <pre><code>sh picard.sh\n</code></pre></p> <p>Result: <pre><code>[Fri May 08 15:38:55 EDT 2020] picard.sam.SortSam INPUT=results/na12878.sam OUTPUT=results/na12878.srt.bam SORT_ORDER=coordinate    VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 15:38:55 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\nINFO    2020-05-08 15:38:56 SortSam Finished reading inputs, merging and writing to output now.\n[Fri May 08 15:38:57 EDT 2020] picard.sam.SortSam done. Elapsed time: 0.02 minutes.\nRuntime.totalMemory()=2058354688\n</code></pre></p> <p>Take a look at the results directory: <pre><code>ls results\n</code></pre></p> <p>The result shows that the sorted BAM file has been created: <pre><code>na12878.sam\u00a0 na12878.srt.bam\u00a0\n</code></pre></p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#mark-duplicates-in-bam-file","title":"Mark Duplicates in BAM file","text":"<p>Many copies are made of a single DNA fragment during the sequencing process. The amount of duplication may not be the same for all sequences and this can cause biases in variant calling. Therefore, we mark the duplicates so the variant caller can focus on the unique reads.</p> <p>Duplicate reads are identified based on their alignment coordinates and CIGAR string. For example, the below alignment appears to have a G to A mutation in the majority of reads:</p> <p></p> <p>However, when the duplicates are removed, the number of reads supporting the mutation drops to one.</p> <p></p> <p>Let's add this step to our <code>picard.sh</code> script in order to illustrate how to include multiple steps in a single script. Note that when we run it, we'll rerun our previous steps as well.</p> <p><pre><code>nano picard.sh\n</code></pre> Add the following lines to the end of our script: <pre><code>printf  \"..... Starting Mark Duplicates ....\\n\\n\"\n\npicard MarkDuplicates \\\nINPUT=results/na12878.srt.bam \\\nOUTPUT=results/na12878.srt.markdup.bam \\\nREAD_NAME_REGEX=null \\\nMETRICS_FILE=results/na12878.markdup.txt\n</code></pre></p> <p>The first line is a formatted print (<code>printf</code>) statement that will display useful log lines when our script is running. The option <code>READ_NAME_REGEX=null</code> is added because our read names, downloaded from <code>GIAB</code> do not contain information about the position on the flowcell. When present this information can help with estimating optical duplicated. Typically, datasets do contain this information and it is best to omit this line when processing your data.</p> <p>To run our script (Note this will rerun the first step as well. This is only for demonstration purposes. If you were developing this for your own use, you would instead write all commands and run the script once): <pre><code>sh picard.sh\n</code></pre></p> <p>In addition to our previous log, we'll see our log line, followed by the output from Mark Duplicates: <pre><code>\u2026\n.... Starting Mark Duplicates ....\n\n[Fri May 08 16:03:52 EDT 2020] picard.sam.markduplicates.MarkDuplicates INPUT=[results/na12878.srt.sam] OUTPUT=results/na12878.srt.markdup.sam METRICS_FILE=results/na12878.markdup.txt READ_NAME_REGEX=null    MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 16:03:52 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\nINFO    2020-05-08 16:03:52 MarkDuplicates  Start of doWork freeMemory: 2042614304; totalMemory: 2058354688; maxMemory: 28631367680\nINFO    2020-05-08 16:03:52 MarkDuplicates  Reading input file and constructing read end information.\nINFO    2020-05-08 16:03:52 MarkDuplicates  Will retain up to 110120644 data points before spilling to disk.\nINFO    2020-05-08 16:04:03 MarkDuplicates  Read 9300 records. 0 pairs never matched.\nINFO    2020-05-08 16:04:09 MarkDuplicates  After buildSortedReadEndLists freeMemory: 2020313280; totalMemory: 2915041280; maxMemory: 28631367680\nINFO    2020-05-08 16:04:09 MarkDuplicates  Will retain up to 894730240 duplicate indices before spilling to disk.\nINFO    2020-05-08 16:04:11 MarkDuplicates  Traversing read pair information and detecting duplicates.\nINFO    2020-05-08 16:04:11 MarkDuplicates  Traversing fragment information and detecting duplicates.\nINFO    2020-05-08 16:04:11 MarkDuplicates  Sorting list of duplicate records.\nINFO    2020-05-08 16:04:14 MarkDuplicates  After generateDuplicateIndexes freeMemory: 3340626880; totalMemory: 10530324480; maxMemory: 28631367680\nINFO    2020-05-08 16:04:14 MarkDuplicates  Marking 864 records as duplicates.\nWARNING 2020-05-08 16:04:14 MarkDuplicates  Skipped optical duplicate cluster discovery; library size estimation may be inaccurate!\nINFO    2020-05-08 16:04:14 MarkDuplicates  Reads are assumed to be ordered by: coordinate\nINFO    2020-05-08 16:04:16 MarkDuplicates  Before output close freeMemory: 10507885056; totalMemory: 10530324480; maxMemory: 28631367680\nINFO    2020-05-08 16:04:16 MarkDuplicates  After output close freeMemory: 10507907720; totalMemory: 10530324480; maxMemory: 28631367680\n[Fri May 08 16:04:16 EDT 2020] pic\n</code></pre></p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#mark-duplicates-metrics-file","title":"Mark Duplicates Metrics file","text":"<p>The following is the metrics file <code>na12878.markdup.txt</code> generated by Picard Mark Duplicates:</p> <p>| LIBRARY | UNPAIRED_READS_EXAMINED | READ_PAIRS_EXAMINED | SECONDARY_OR_SUPPLEMENTARY_RDS | UNMAPPED_READS | UNPAIRED_READ_DUPLICATES | READ_PAIR_DUPLICATES | READ_PAIR_OPTICAL_DUPLICATES | PERCENT_DUPLICATION | ESTIMATED_LIBRARY_SIZE |</p> <p>|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:| | Unknown | 29 | 4620 | 2 | 35 | 14 | 425 | 0 | 0.093214 | 23546 |</p> <p>Normal % duplication for exome sequencing data is 10-30%. By scrolling to the left in this table we see that our percent duplication is <code>0.093214%</code>.</p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#index-the-bam-file","title":"Index the BAM file","text":"<p>In order to view the alignment with the Integrated Genomics Viewer (IGV) we are required to create an index files for our BAM file. This facilitates fast lookup of genomics coordinates.</p> <p>Let's continue editing our script: <pre><code>nano picard.sh\n</code></pre></p> <p>Add the following lines at the end of the script: <pre><code>printf  '.... Start BAM Indexing ....\\n\\n'\n\npicard BuildBamIndex \\\nINPUT=results/na12878.srt.markdup.bam\n</code></pre></p> <p>Run our script: <pre><code>sh picard.sh\n</code></pre></p> <p>Result, in addition to previous output: <pre><code>.... Start BAM Indexing ....\n\n[Fri May 08 16:24:17 EDT 2020] picard.sam.BuildBamIndex INPUT=results/na12878.srt.markdup.bam    VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 16:24:17 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\nWARNING: BAM index file /cluster/home/rbator01/intro-to-ngs/results/na12878.srt.markdup.bai is older than BAM /cluster/home/rbator01/intro-to-ngs/results/na12878.srt.markdup.bam\nINFO    2020-05-08 16:24:18 BuildBamIndex   Successfully wrote bam index file /cluster/home/rbator01/intro-to-ngs/results/na12878.srt.markdup.bai\n[Fri May 08 16:24:18 EDT 2020] picard.sam.BuildBamIndex done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=2058354688\n</code></pre></p> <p>We can see the files that were generated by typing <code>ls results</code> <pre><code>na12878.sam\nna12878.srt.bam\nna12878.srt.markdup.bam\nna12878.markdup.txt\nna12878.srt.markdup.bai     &lt;--- Index file\n</code></pre></p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#bam-visualization-with-igv","title":"BAM Visualization with IGV","text":"<ol> <li>With a Chrome web browser, visit https://ondemand.cluster.tufts.edu</li> <li>Login with your Tufts credentials</li> <li> <p>Choose Interactive Apps-&gt;IGV. Set parameters, click \u201cLaunch\u201d</p> </li> <li> <p>Choose <code>Interactive Apps-&gt;IGV</code>. Set parameters below and , click <code>Launch</code> </p> </li> <li>Choose the following compute resource parameters: 1 hour, 2 cores, 4 GB memory, Default Batch Parition, Default Reservation</li> </ol> <p></p> <ol> <li>Click the blue button <code>Launch NoVNC in New Tab</code> when it appears</li> </ol> <p>After this the IGV window will appear, probably as a small window on a grey background. Click the square icon in the top right corner to maximize the window.</p> <p></p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#load-reference-genome-and-bam-file","title":"Load reference genome and BAM file","text":"<ol> <li>Choose reference genome by clicking the <code>Genomes</code> menu and selecting <code>Load Genome from Server...</code></li> </ol> <ol> <li>Scroll down to <code>Human hg38</code></li> </ol> <ol> <li> <p>DO NOT check <code>Download Sequnence</code></p> </li> <li> <p>Click <code>OK</code></p> </li> <li> <p>Load the BAM file by clicking the <code>File</code> menu and select <code>Load from File...</code></p> </li> </ol> <p></p> <ol> <li> <p>Navigate to the results folder in the course directory, e.g. <code>/cluster/home/your-user-name/intro-to-ngs/results</code>.  </p> </li> <li> <p>Select <code>na12878.srt.markdup.bam</code></p> </li> </ol> <p></p> <p>You will have the following view:</p> <p></p> <p>Each row of data is called a track. There are five tracks visible: the top track shows the pq bands of the entire chromosome, followed by the reference genome coordinate track, followed by two tracks of our alignment (coverage and reads, respectively) which don't yet show data, followed by a reference genome annotation track called \"Genes\".</p>"},{"location":"omics/intro-to-ngs/04_Alignment_Cleanup/#examining-a-gene","title":"Examining a gene","text":"<ol> <li>In the box indicated in green below, type gene name \"Cyp2c19\" and hit enter. You will see the gene model display in the \u201cGenes\u201d track, showing vertical bars where exons are located</li> </ol> <p>Troubleshooting tip: At times IGV on demand will stop allowing the user to type input. If that happens, close the tab, go back to the on demand window, rejoin the session by clicking <code>Launch NoVNC in New Tab</code>.</p> <ol> <li>Let's zoom in on exon 7. You can hover over exons in the <code>Genes</code> track to get information such as exon number. Click and drag over a region in the reference coordinate track to zoom in on exon 7 (highlighted in green below.)</li> </ol> <p></p> <ol> <li>We can see that there is a variant in this exon.</li> </ol> <p></p> <ol> <li>Zoom in even further until the nucleotide letters are clear. Then, hover with your mouse over the coverage track to find out more information about this variant.</li> </ol> <p></p> <p>It appear there are two variants next to each other: heterozygous<code>C&gt;T</code> at position <code>chr10:94,842,865</code> and homozygous <code>A&gt;G</code> at position <code>chr10:94,842,866</code>. Next, we'll explore the meaning of these variants.</p> <p></p> <p>This lesson adapted from HBC NGS Data Analysis</p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/","title":"Variant Calling","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#learning-objectives","title":"Learning Objectives","text":"<p>Use Genome Analysis Tool Kit (GATK) to call variants</p> <p></p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#prepare-the-reference-sequence-for-gatk","title":"Prepare the reference sequence for GATK","text":"<p>GATK requires a Sequence Dictionary for reference genomes used in variant calling. The sequence dictionary contains names and lengths of all chromosomes in the reference genome. The information in this file is transferred to the Variant Call File (VCF) when it is produced, so that there is no ambiguity about which reference was used to produce the file.</p> <p>Let's open a new script <pre><code>nano prepare.sh\n</code></pre></p> <p>Add these lines: <pre><code>module load samtools/1.9\nmodule load picard/2.8.0\n\nsamtools faidx ref_data/chr10.fa\n\npicard CreateSequenceDictionary \\\nREFERENCE=ref_data/chr10.fa \\\nOUTPUT=ref_data/chr10.dict\n</code></pre></p> <p>With these steps, we load the necessary modules, created a FASTA index for our reference sequence, and use Picard to create our Sequence Dictionary.</p> <p>Run our script: <pre><code>sh prepare.sh\n</code></pre></p> <p>Result: <pre><code>[Fri May 08 16:52:35 EDT 2020] picard.sam.CreateSequenceDictionary REFERENCE=ref_data/chr10.fa OUTPUT=ref_data/chr10.dict    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 16:52:35 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\n[Fri May 08 16:52:35 EDT 2020] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=2058354688\n</code></pre></p> <p>Two new files are created in the folder <code>ref_data</code>, our FASTA index (fai) and sequence dictionary (dict): <pre><code>chr10.fa.fai\nchr10.dict\n</code></pre></p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#variant-calling-with-gatk-haplotypecaller","title":"Variant Calling with GATK HaplotypeCaller","text":"<p>GATK has two main goals: - Separate true variants from sequencing error - Establish which variants co-exist on a single DNA strand (haplotype)</p> <p>The figure below described the 4 stages of the GATK HaplotypeCaller algorithm (from software.broadinstitute.org ).</p> <p></p> <p>Paraphrasing from the GATK documentation, the four stages are as follows:</p> <ol> <li> <p>Define active regions. The program determines which regions of the genome it needs to operate on, based on the presence of significant evidence for variation.</p> </li> <li> <p>Determine haplotypes by re-assembly of the active region. For each active region, the program builds a graph to represent all possible read sequences spanning the region. For example, the top first read starts in the <code>TATG</code> bubble that is common to all reads, then takes the top path to the <code>A</code> bubble, continues through the <code>AAT</code>, etc. The program then realigns each haplotype (path through the graph) against the reference sequence in order to identify potentially variant sites.</p> </li> <li> <p>Determine likelihoods of the haplotypes given the read data. The goal of this stage is to evaluate which haplotypes have the most read support. For each active region, the program performs a pairwise alignment of each read against each haplotype using the PairHMM algorithm, which takes into account other information about the data, such as quality scores. This produces a matrix of likelihoods of haplotypes given the read data. These likelihoods are then used to calculate how much evidence there is for individual alleles at each variant site (marginalization over alleles).</p> </li> <li> <p>Assign sample genotypes. The final step is to determine which sequences were most likely present in the data. This step uses Bayes' rule to find the most likely genotype, given the allele likelihoods calculated in the last step.</p> </li> </ol>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#run-gatk-on-our-bam-file","title":"Run GATK on our BAM file","text":"<p>To load the module on our system, we'll type:</p> <p><pre><code>module load GATK/3.7\n</code></pre> We can check the usage for GATK, which has many tools in addition to HaplotypeCaller: <pre><code>gatk --help\n</code></pre></p> <p>The result shows the many different tools inside GATK. The relevant lines for HaplotypeCaller are: <pre><code>\u2026\nusage: java -jar GenomeAnalysisTK.jar -T &lt;analysis_type&gt; \u2026\n\u2026\n\u00a0haplotypecaller\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n\u00a0\u00a0 HaplotypeCaller \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Call germline SNPs and indels via local re-assembly of haplotypes\n\u00a0\u00a0 HaplotypeResolver \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Haplotype-based resolution of variants in separate callsets.\n</code></pre></p> <p>For tool specific help, we type: <pre><code>gatk -T HaplotypeCaller --help\n</code></pre></p> <p>Let's write a new script: <pre><code>nano gatk.sh\n</code></pre></p> <p>Add these lines, which specify the reference file, input BAM, and output VCF. <pre><code>module load GATK/3.7\n\ngatk -T HaplotypeCaller \\\n-R ref_data/chr10.fa \\\n-I results/na12878.srt.markdup.bam \\\n-o results/na12878.vcf\n</code></pre></p> <p>Run our script: <pre><code>sh gatk.sh\n</code></pre></p> <p>Result: <pre><code>INFO\u00a0 17:17:41,656 HelpFormatter - -----------\nINFO\u00a0 17:17:41,660 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18\u00a0\n\u2026\n</code></pre></p> <p>Two new files have appeared in our results folder, the variant call file (VCF) and index file, respectively: <pre><code>na12878.vcf\u00a0\nna12878.vcf.idx\n</code></pre></p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#vcf-format","title":"VCF format","text":"<p>We can take a look at the first few lines of our vcf file: <pre><code>cd results\nhead na12878.vcf\n</code></pre></p> <p>VCF, like BAM files, files contain two sections: A header section, indicated by the presence of <code>#</code> at the beginning of the line, followed by data lines for each variant that was called. <pre><code>##fileformat=VCFv4.2\n##FILTER=&lt;ID=LowQual,Description=\"Low quality\"&gt;\n##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Allelic depths for the ref and alt alleles in the order listed\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth \u2026\n##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=PL,Number=G,Type=Integer,Description=\"Normalized, Phred-scaled likelihoods for genotypes \u2026\n##GATKCommandLine.HaplotypeCaller=&lt;ID=HaplotypeCaller,...\n\u2026.\n##contig=&lt;ID=chr10,length=135534747&gt;\n##reference=file:///cluster/home/tutln01/intro-to-ngs/ref_data/chr10.fa\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO            FORMAT      NA12878\nchr10   96521422    .   A   G   60.28   .         AC=2;AF=1.00; \u2026.  GT:AD:DP:GQ:PL  1/1:0,3:3:9:88,9,0\nchr10   96522365    .   T   C   1134.77 .         AC=1;AF=0.500;\u2026.  GT:AD:DP:GQ:PL  0/1:47,37:84:99:1163,0,1502\n</code></pre></p> <p>The header lines explain the meaning of notation found in the body section of the VCF, as well as information about the reference and software used to produce the VCF. The last header line lists the column titles for information, and the last column has the sample name. VCF can be used to represent multiple samples, and in that case, each sample would have it's own subsequent column.</p> <p>Let's look at the body section in table format:</p> CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878 chr10 96521422 . A G 60.28 . AC=2;AF=1.00; \u2026. GT:AD:DP:GQ:PL 1/1:0,3:3:9:88,9,0 <p>Fixed fields (same for all samples in the VCF) CHROM - Chromosome POS - Position ID - Identifier. May be present if the VCF was annotated with known variants, for example, rs numbers from dbSNP. REF - Reference sequence base ALT - Alternate base, comma separated list of non-reference alleles (usually) found in the samples represented by the VCF QUAL - Phred scaled quality score for the variant, i.e. \\(10log_{10}\\) prob(call is wrong). FILTER - PASS if this position has passed all filters, otherwise the name of the filter islisted. INFO - Additional information</p> <p>Genotype fields (one per sample): FORMAT - This field specifies the format that will be used to give information in each sample column. VCF can represent In this case, we see <code>GT:AD:DP:GQ:PL</code>, which corresponds to the values <code>1/1:0,3:3:9:88,9,0</code>. GT - Genotype, encoded as allele values separated by either '/' (unphaseD) or '|' (phased - known to be on the same chromosome arm). The allele values are 0 for the reference allele and 1 for the first allele listed. AD - Allele depth at this position for the same, reference first followed by first allele listed DP - Read depth at this position for the sample GQ - Genotype quality PL - Genotype liklihoods</p> <p>For more on the rich VCF format, see the VCF format specification from Samtools</p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#vcf-quality-control","title":"VCF Quality Control","text":"<p>It's always a good idea when writing a new pipeline, to ask: How well did our variant calling perform? In this case, the best way to check the performance would be to compare the variants we called in this exercise matched the \"known\" variants for NA12878 in the NIST callset. That exercise is beyond the scope of this workshop.</p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#add-our-vcf-to-igv","title":"Add our VCF to IGV","text":"<p>We can add a VCF track to our IGV windows.</p> <ol> <li>Go to back to IGV on demand</li> <li>Click the <code>File</code> menu and select <code>Load from File</code></li> <li>Select the file <code>na12878.vcf</code></li> </ol> <p></p> <ol> <li>We'll see a variant track appear above the coverage track. Hover over the colored blocks on the variant track in order to see the information in the VCF.</li> </ol> <p></p>"},{"location":"omics/intro-to-ngs/05_Variant_Calling/#summary","title":"Summary","text":""},{"location":"omics/intro-to-ngs/06_Variant_Annotation/","title":"Variant Annotation","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Use the Variant Effect Predictor (VEP) online web server to annotate variants  </li> <li>Identify amino acid changing substitutions in our VCF</li> </ul>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#vep-overview","title":"VEP overview","text":"<p>VEP will add annotation from a number of sources for each variant that we upload. Below is a subset of the most commonly used annotations annotations.</p> <ul> <li> <p>Identifiers: Gene, transcript, protein, etc.</p> </li> <li> <p>Frequency data: Allele frequency information from multiple public databases. 1000 Genomes, (gnomAD)[https://gnomad.broadinstitute.org/], (ESP)[https://evs.gs.washington.edu/EVS/]  Allele frequency information is helpful to understand whether the input variant is common or rare in different geographical populations.</p> </li> </ul> <p>-Pathogenicity predictions: Computational predictions of whether a variant will affect the protein function. Various algorithms are available (SIFT, PolyPhen2, CADD, etc)</p> <ul> <li> <p>Disease Association: Clinical significance and disease association as reported in ClinVar. ClinVar is a widely used database that aggregates and curates clinical reports of variants with clinical determinations. The clinical significances reported in VEP range from <code>Benign</code> to <code>Pathogenic</code> and usually have a disease annotation.</p> </li> <li> <p>Consequence: For each variant, VEP identifies all transcripts in the selected database (Ensembl or Refseq) that overlaps with the variant coordinates. The consequence of the variant with respect to the transcript is then evaluated based on the following diagram.</p> </li> </ul> <p></p> <p>These consequences are then binned into impact groups: LOW, MODERATE, MODIFIER, HIGH. For a full mapping to consequence to impact, see VEP</p> <p>We'll run VEP on the VCF that we produced and analyze the variant consequences.</p>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#download-the-vcf","title":"Download the VCF","text":"<p>First, we'll download the VCF from the cluster to our local computer.</p> <ol> <li>Go back to https://ondemand.cluster.tufts.edu</li> <li>In the top grey menu, click <code>Files</code> and select <code>Home Directory</code>.</li> </ol> <p></p> <ol> <li>Select <code>intro-to-ngs/results/na12878.vcf</code></li> </ol> <p></p> <ol> <li>Click <code>Download</code></li> </ol>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#run-vep","title":"Run VEP","text":"<ol> <li> <p>In web browser tab, navigate to to https://useast.ensembl.org/Tools/VEP Note that VEP can also be run on the command line on our HPC, resulting in a text file (txt or vcf). You are welcome to ask for instructions to run the command line VEP. For single VCF analysis, the web server is recommended in order to take advantage of the visualization tools.</p> </li> <li> <p>In the <code>Species</code> section choose <code>Human (Homo sapiens)</code> (should be the default)</p> </li> <li> <p>In the <code>Input data</code> section choose <code>Or upload file:</code> and navigate to the downloaded file <code>na12878.vcf</code></p> </li> <li> <p>Under <code>Transcript database to use</code> select <code>RefSeq transcripts</code></p> </li> </ol> <p></p> <ol> <li>Click <code>Run</code></li> </ol>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#viewing-vep-results","title":"Viewing VEP results","text":"<p>When your job is done, click <code>View Results</code></p> <p></p> <p>Our goal is to identify variants that change the coding sequence. We can see in the <code>Coding Consequences</code> box on the right that 20% of the variants are <code>missense</code>, which means that they change the coding sequence of the transcript.</p>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#filtering-vep-consequences","title":"Filtering VEP consequences","text":"<p>Under <code>Filters</code> choose <code>Consequence</code> + <code>is</code> + <code>missense_variant</code> and click <code>Add</code> You should see 1 row - here are a subset of interesting columns:</p> Location Allele Consequence IMPACT SYMBOL BIOTYPE Amino_acids 10:94842866-94842866 G missense_variant MODERATE CYP2C19 protein_coding I/V <p>| Existing_variation | SIFT | PolyPhen | AF | Clinical Significance | |:---:|:---:|:---:|:---:| | rs3758581,CM983294 | tolerated(0.38) | benign(0.05) | 0.9515 | |</p> <p>Based on the annotations, one can conclude that this variant unlikely to cause disease. This is consistent with what we know about <code>NA12878</code> being a healthy individual.</p> <p>Though the vatiant does change the amino acid from <code>I</code> to <code>V</code>, both SIFT, PolyPhen both suggest that this change does not alter protein function. Furthermore, there is no ClinVar report associated with this variant. Finally, the maximum allele frequency found for this variant in the <code>1000 Genomes</code> database is <code>0.95</code>, meaning it is a very common variant and unlikely to be pathogenic.</p>"},{"location":"omics/intro-to-ngs/06_Variant_Annotation/#summary","title":"summary","text":""},{"location":"omics/intro-to-ngs/background/","title":"Background","text":""},{"location":"omics/intro-to-ngs/background/#background","title":"Background","text":"<p>Sequencing data analysis typically focuses on either assessing DNA or RNA. As a reminder here is the interplay between DNA, RNA, and protein:</p> <p></p>"},{"location":"omics/intro-to-ngs/background/#dna-sequencing","title":"DNA Sequencing","text":"<ul> <li>Fixed copy of a gene per cell </li> <li>Analysis goal: Variant calling and interpretation</li> </ul>"},{"location":"omics/intro-to-ngs/background/#rna-sequencing","title":"RNA Sequencing","text":"<ul> <li>Copy of a transcript per cell depends on gene expression</li> <li>Analysis goal: Differential expression and interpretation</li> </ul> <p>Note</p> <p>Here we are working with DNA sequencing</p>"},{"location":"omics/intro-to-ngs/background/#next-generation-sequencing","title":"Next Generation Sequencing","text":"<p>Here we will analyze a DNA sequence using next generation sequencing data. Here are the steps to get that data:</p> <ul> <li>Library Preparation: DNA is fragmented and adapters are added to these fragments</li> </ul> <p></p> <ul> <li>Cluster Amplification: This library is loaded onto a flow cell, where the adapters help hybridize the fragments to the flow cell. Each fragment is then amplified to form a clonal cluster</li> </ul> <p></p> <ul> <li>Sequencing: Fluorescently labelled nucleotides are added to this flow cell and each time a base in the fragment bonds a light signal is emmitted telling the sequencer which base is which in the sequence.</li> </ul> <p></p> <ul> <li>Alignment &amp; Data Analysis: These sequenced fragments, or reads, can then be aligned to a reference sequence to determine differences.</li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/background/#singe-end-v-paired-end-data","title":"Singe End v. Paired End Data","text":"<ul> <li>single-end sequence each DNA fragement from one end only</li> <li>paired-end sequence each DNA fragement from both sides. Paired-end data is useful when sequencing highly repetitive sequences.</li> </ul>"},{"location":"omics/intro-to-ngs/background/#variant-calling","title":"Variant Calling","text":""},{"location":"omics/intro-to-ngs/background/#ploidy","title":"Ploidy","text":"<ul> <li> <p>When discussing variant calling it is worth mentioning an organism's ploidy. Ploidy is the number of copies of each chromosomes.</p> <ul> <li>Humans cells are diploid for autosomal chromosome and haploid for sex chromosomes</li> <li>Bacteria are haploid</li> <li>Viruses and Yeast can by haploid or diploid</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Variant callers can use ploidy to improve specificity (avoid false positives) because there are expected variant frequencies, e.g. for a diploid:</p> <ul> <li>Homozygous</li> <li>both copies contain variant</li> <li> <p>fraction of the reads ~1</p> </li> <li> <p>Heterozygous</p> </li> <li>one copy of variant</li> <li>fraction of reads with variant  ~0.5</li> </ul> </li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/background/#references","title":"References","text":"<ul> <li>Illumina</li> <li>Ploidy</li> </ul>"},{"location":"omics/intro-to-ngs/combined/","title":"Combined","text":""},{"location":"omics/intro-to-ngs/combined/#background","title":"Background","text":"<p>Sequencing data analysis typically focuses on either assessing DNA or RNA. As a reminder here is the interplay between DNA, RNA, and protein:</p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#types-of-sequencing","title":"Types of Sequencing","text":"<p>DNA Sequencing</p> <ul> <li>Fixed copy of a gene per cell </li> <li>Analysis goal: Variant calling and interpretation</li> </ul> <p>RNA Sequencing</p> <ul> <li>Copy of a transcript per cell depends on gene expression</li> <li>Analysis goal: Differential expression and interpretation</li> </ul> <p>Note</p> <p>Here we are working with DNA sequencing</p>"},{"location":"omics/intro-to-ngs/combined/#next-generation-sequencing","title":"Next Generation Sequencing","text":"<p>Here we will analyze a DNA sequence using next generation sequencing data. Here are the steps to get that data:</p> <ul> <li>Library Preparation: DNA is fragmented and adapters are added to these fragments</li> </ul> <p></p> <ul> <li>Cluster Amplification: This library is loaded onto a flow cell, where the adapters help hybridize the fragments to the flow cell. Each fragment is then amplified to form a clonal cluster</li> </ul> <p></p> <ul> <li>Sequencing: Fluorescently labelled nucleotides are added to this flow cell and each time a base in the fragment bonds a light signal is emmitted telling the sequencer which base is which in the sequence.</li> </ul> <p></p> <ul> <li>Alignment &amp; Data Analysis: These sequenced fragments, or reads, can then be aligned to a reference sequence to determine differences.</li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/combined/#singe-end-v-paired-end-data","title":"Singe End v. Paired End Data","text":"<ul> <li>single-end sequence each DNA fragement from one end only</li> <li>paired-end sequence each DNA fragement from both sides. Paired-end data is useful when sequencing highly repetitive sequences.</li> </ul>"},{"location":"omics/intro-to-ngs/combined/#variant-calling","title":"Variant Calling","text":""},{"location":"omics/intro-to-ngs/combined/#ploidy","title":"Ploidy","text":"<ul> <li> <p>When discussing variant calling it is worth mentioning an organism's ploidy. Ploidy is the number of copies of each chromosomes.</p> <ul> <li>Humans cells are diploid for autosomal chromosome and haploid for sex chromosomes</li> <li>Bacteria are haploid</li> <li>Viruses and Yeast can by haploid or diploid</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Variant callers can use ploidy to improve specificity (avoid false positives) because there are expected variant frequencies, e.g. for a diploid:</p> <ul> <li>Homozygous</li> <li>both copies contain variant</li> <li> <p>fraction of the reads ~1</p> </li> <li> <p>Heterozygous</p> </li> <li>one copy of variant</li> <li>fraction of reads with variant  ~0.5</li> </ul> </li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/combined/#setup","title":"Setup","text":"<p>Goals</p> <ul> <li>Connect to the HPC cluster via On Demand Interface</li> <li>Download data</li> </ul>"},{"location":"omics/intro-to-ngs/combined/#log-into-the-hpc-clusters-on-demand-interface","title":"Log into the HPC cluster's On Demand interface","text":"<ul> <li>Open a Chrome browser and enter the URL https://ondemand.cluster.tufts.edu</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <ul> <li>Type your password at the prompt (the password will be hidden for security purposes): <code>tutln01@login.cluster.tufts.edu's password:</code></li> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <p><code>[tutln01@login001 ~]$</code></p> <p>This indicates you are logged in to the login node of the cluster. - Type <code>clear</code> to clear the screen</p>"},{"location":"omics/intro-to-ngs/combined/#set-up-for-the-analysis","title":"Set up for the analysis","text":"<p>Find 500M storage space</p> <ul> <li>Check how much available storage you have in your home directory by typing <code>showquota</code>.</li> </ul> <p>Result: <pre><code>Home Directory Quota\nDisk quotas for user tutln01 (uid 31394):\n     Filesystem  blocks   quota   limit   grace   files   quota   limit   grace\nhpcstore03:/hpc_home/home\n                  1222M   5120M   5120M            2161   4295m   4295m        \n\n\nListing quotas for all groups you are a member of\nGroup: facstaff Usage: 16819478240KB    Quota: 214748364800KB   Percent Used: 7.00%\n</code></pre></p> <p>Under <code>blocks</code> you will see the amount of storage you are using, and under quota you see your quota. Here, the user has used 1222M of the available 5120M and has enough space for our analysis.</p> <ul> <li>If you do not have 500M available, you may have space in a project directory for your lab. These are located in <code>/cluster/tufts</code> with names like <code>/cluster/tufts/labname/username/</code>. If you don't know whether you have project space, please email tts-research@tufts.edu.</li> </ul>"},{"location":"omics/intro-to-ngs/combined/#download-the-data","title":"Download the data","text":"<ul> <li>Get an interaction session on a compute node (3 hours, 16 Gb memory, 4 cpu on 1 node) on the default partition (<code>batch</code>) by typing:</li> </ul> <p><code>srun --pty -t 3:00:00  --mem 16G  -N 1 --cpus 4 bash</code></p> <p>Notes:  If wait times are very long, you can try a different partitions by adding, e.g. <code>-p preempt</code> or <code>-p interactive</code> before <code>bash</code>. If you go through this workshop in multiple steps, you will have to rerun this step each time you log in.</p> <ul> <li>Change to your home directory</li> </ul> <p><code>cd</code></p> <p>Or, if you are using a project directory:</p> <p><code>cd /cluster/tufts/labname/username/</code></p> <ul> <li>Copy the course directory and all files in the directory (-R is for recursive):   </li> </ul> <p><code>cp -R /cluster/tufts/bio/tools/training/intro-to-ngs/ .</code> </p> <p>(Also available via:  <code>git clone https://gitlab.tufts.edu/rbator01/intro-to-ngs.git</code>)</p> <ul> <li>Take a look at the contents using the <code>tree</code> command:</li> </ul> <p><code>tree intro-to-ngs</code></p> <p>You'll see a list of all files <pre><code>intro-to-ngs\n\u251c\u2500\u2500 all_commands.sh          &lt;-- Bash script with all commands\n\u251c\u2500\u2500 raw_data                 &lt;-- Folder with paired end fastq files\n\u2502   \u251c\u2500\u2500 na12878_1.fq         \n\u2502   \u2514\u2500\u2500 na12878_2.fq\n\u251c\u2500\u2500 README.md                &lt;-- Contents description\n\u2514\u2500\u2500 ref_data                 &lt;-- Folder with reference sequence\n    \u2514\u2500\u2500 chr10.fa\n2 directories, 5 files\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#data-for-the-class","title":"Data for the class","text":"<p>Genome In a Bottle (GIAB) was initiated in 2011 by the National Institute of Standards and Technology \"to develop the technical infrastructure (reference standards, reference methods, and reference data) to enable translation of whole human genome sequencing to clinical practice\" (Zook et al 2012).  We'll be using a DNA Whole Exome Sequencing (WES) dataset released by GIAB for the purposes of benchmarking bioinformatics tools.</p> <p></p> <p>The source DNA, known as NA12878, was taken from a single person: the daughter in a father-mother-child 'trio'. She is also mother to 11 children of her own, for whom sequence data is also available. (HBC Training). Father-mother-child 'trios' are often sequenced to study genetic links between family members.</p> <p>As mentioned in the introduction, WES is a method to concentrate the sequenced DNA fragments in coding regions (exons) of the genome.</p> <p></p> <p>For this class, we've created a small dataset of reads that align to a single gene that will allow our commands to finish quickly.</p> <p>Sample: NA12878</p> <p>Gene: Cyp2c19 on chromosome 10</p> <p>Sequencing: Illumina, Paired End, Exome</p>"},{"location":"omics/intro-to-ngs/combined/#quality-control","title":"Quality Control","text":"<p>Goals</p> <ul> <li>Understand FASTQ file format</li> <li>Run FastQC to asses data quality</li> </ul>"},{"location":"omics/intro-to-ngs/combined/#assess-the-quality-of-the-raw-data","title":"Assess the quality of the raw data","text":"<p>FASTQ format</p> <p>FASTQ files is the most common way to store biological sequence data. Depending on the sequencing protocol, a single FASTQ file can represent an entire flow cell, a single lane, a single sample, or a portion of a sample. We have two FASTQ files in our <code>raw_data</code> folder, which are the paired end data of a single sample.</p> <p>From our course directory <code>into-to-ngs</code> change into the <code>raw data</code> directory: <pre><code>cd raw_data\n</code></pre></p> <p>Use the command <code>head</code> to look at the first few lines of our first FASTQ file.</p> <pre><code>head na12878_1.fq\n</code></pre> <p>Each read in our file is represented are by four lines: An identifier, the nucleotide sequence, an optional second identifier and a quality string. Below is an example, the  arrows on the right show explanation of each line:</p> <pre><code>@SRR098401.109756285/1                   &lt;-- Sequence identifier: @ReadID / 1 or 2 of pair\nGACTCACGTAACTTTAAACTCTAACAGAAATATACTA\u2026   &lt;-- Sequence\n+                                        &lt;-- + (optionally lists the sequence identifier again)\nCAEFGDG?BCGGGEEDGGHGHGDFHEIEGGDDDD\u2026      &lt;-- Quality String\n</code></pre>"},{"location":"omics/intro-to-ngs/combined/#base-quality-scores","title":"Base Quality Scores","text":"<p>The fourth line of each read is called the quality string. Each symbol in the string is an encoding of the quality score, representing the inferred base call accuracy at that  position in the read.  The manufacturer of the sequencing instrument has performed calibration of quality score by sequencing many  well-characterized samples from multiple organisms  and studying the correspondence between properties of the signal  generated by the cluster being sequenced and the accuracy of the resulting base call.</p> <p>The following two images explain this encoding. The first image shows the mapping of the encoded quality score to the quality score:</p> <p></p> <p>The second image shows the mapping of the quality score to the inferred base call accuracy:</p> <p></p> <p>Looking back at our sample read, we can see that the first base has an encoded quality score of <code>C</code>. Using the first image above, we see that C encodes a quality of 34. Using the second table, we see that the probability is &lt; 1/1000 of that base being an error. In the next section, we'll see how quality scores and other quality control metrics are used to evaluate the quality of  a sequenced sample. </p> <p>More information on Quality scores from Illumina</p>"},{"location":"omics/intro-to-ngs/combined/#fastqc","title":"FastQC","text":"<p>FastQC is widely used tool for both DNA and RNA sequencing data in order to evaluate the quality of the sequencing data.</p> <p>To use, load the module:  <pre><code>module load fastqc/0.11.8\n</code></pre></p> <p>To see the input options, type: <pre><code>fastqc --help\n</code></pre></p> <p>Result: <pre><code>fastqc --help\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n    fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN\n...\n</code></pre></p> <p>FastQC is run on each FASTQ file separately in order to be sensitive to the variation in quality over lanes, samples,  and paired-end files.</p> <p>Since FastQC can run on multiple files at once, we'll use a wildcard <code>*</code> to indicate each file in the folder <code>raw_data</code>,  and we specify that the output should be placed in the directory we created called <code>fastqc</code>:</p> <pre><code>cd ..\nmkdir fastqc\nfastqc raw_data/* -o fastqc\n</code></pre> <p>Result: <pre><code>Started analysis of na12878_1.fq\nApprox 20% complete for na12878_1.fq\nApprox 40% complete for na12878_1.fq\nApprox 65% complete for na12878_1.fq\nApprox 85% complete for na12878_1.fq\nAnalysis complete for na12878_1.fq\nStarted analysis of na12878_2.fq\nApprox 20% complete for na12878_2.fq\nApprox 40% complete for na12878_2.fq\nApprox 65% complete for na12878_2.fq\nApprox 85% complete for na12878_2.fq\nAnalysis complete for na12878_2.fq\n</code></pre></p> <p>To view the resulting files: <pre><code>ls fastqc\n</code></pre></p> <p>The result shows an <code>html</code> file showing graphical results and a <code>zip</code> file containing the raw data for each input FASTQ file. The easist way to view the result is to open the <code>html</code> files in a web browser. <pre><code>na12878_1_fastqc.html  na12878_1_fastqc.zip  na12878_2_fastqc.html  na12878_2_fastqc.zip\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#view-results-in-the-on-demand-browser","title":"View results in the On Demand browser","text":"<p>To view the graphical results, return to the tab ondemand.cluster.tufts.edu</p> <p>On the top menu bar choose <code>Files-&gt;Home Directory</code></p> <p></p> <p>Navigate to the <code>fastqc</code> folder in course directory, e.g.: <code>/home/username/intro-to-ngs/fastqc/</code> Right click on the file <code>na12878_1_fastqc.html</code> and select <code>Open in new tab</code>.</p> <p></p> <p>The new tab that opens in the browser has the results of FastQC for the first reads in the sample. We'll go through each plot. Note that the plots shown are representative results for WES data of varying quality, rather than those generated  on the course data.</p>"},{"location":"omics/intro-to-ngs/combined/#per-base-sequence-quality","title":"Per base sequence quality","text":"<p>The first plot shows the quality scores vs. position in the read, for all reads in the file.</p> <p></p> <p>For each position a Box and Whisker type plot is drawn. The elements of the plot are as follows: - The central red line is the median value - The yellow box represents the inter-quartile range (25-75%) - The upper and lower whiskers represent the 10% and 90% points - The blue line represents the mean quality</p> <p>The background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red).  It is typical for reads generated by Illumina platforms to show reduced quality at the ends of reads due to fragments in  a cluster becoming out-of-sync (Fuller et al 2009).</p>"},{"location":"omics/intro-to-ngs/combined/#per-sequence-quality-scores","title":"Per sequence quality scores","text":"<p>The Per Sequence Quality Score plots the distribution of mean sequence quality. This plot allows will show a peak toward lower mean quality if there is a subset of sequences with  low quality values. </p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#per-base-sequence-content","title":"Per base sequence content","text":"<p>The Per Base Sequence Content plot shows the the proportion of each base called at each position in the read,  for all reads in the file.</p> <p></p> <p>In a random library you would expect that bases would be present in equal proportions. In any given genome, however, the relative amount of each base will reflect the overall amount of these bases in your genome. In any case, we would expect the lines to run parallel to each other. If you see strong biases which change in different bases then this usually indicates an overrepresented sequence which is contaminating your library. Below is an example of a library that was contaminated  with adapter dimers (from sequencing.qcfail.com)[https://sequencing.qcfail.com/articles/contamination-with-adapter-dimers/].</p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#per-sequence-gc-content","title":"Per sequence GC content","text":"<p>This plot displays the fraction of G and C bases across for all sequences in the file. and compares it to a modelled normal distribution of GC content.</p> <p></p> <p>In a normal random library you would expect to see a roughly normal distribution of GC content where the central peak corresponds to the overall GC content of the underlying genome. The expected GC content is calculated from the observed data and used to build a reference distribution. An unusually shaped distribution could indicate a contaminated library or some other kinds of biased subset. A normal distribution which is shifted indicates some systematic bias which is independent of base position.</p>"},{"location":"omics/intro-to-ngs/combined/#per-base-n-content","title":"Per base N content","text":"<p>If a sequencer is unable to make a base call with sufficient confidence then it will normally substitute an N rather than a conventional base.  This plot shows the percentage of base calls at each position for which an N was substituted.</p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#sequence-length-distribution","title":"Sequence Length Distribution","text":"<p>This plot shows the distribution of read sizes in the file. Depending on the sequencing method and whether reads have been post-processed, it may be expected to have reads of a  uniform length or varying lengths. For our raw WES dataset we see a sharp peak at 76 bases, as expected.</p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#sequence-duplication-levels","title":"Sequence Duplication Levels","text":"<p>This plot shows the distribution of sequence duplicates in the file. For example, in the below plot, over 80% of the total sequences are present only once and 10% are present twice.</p> <p></p> <p>In a diverse library most sequences will occur only once in the final set. A high level of duplication may indicate low library complexity or an enrichment bias (e.g. PCR over amplification).</p>"},{"location":"omics/intro-to-ngs/combined/#overrepresented-sequences","title":"Overrepresented sequences","text":"<p>This plot shows the sequences in the file which make up more than 0.1% of the total. A normal high-throughput library will contain a diverse set of sequences, with no individual sequence making up a more than a tiny fraction of the whole. Finding that a single sequence is very overrepresented in the set either means that it is highly biologically significant, or indicates that the library is contaminated, or not as diverse as you expected.</p> <p>For each overrepresented sequence the program will look for matches in a database of common contaminants and will report the best hit it finds. Hits must be at least 20bp in length and have no more than 1 mismatch. Finding a hit doesn't necessarily mean that this is the source of the contamination, but may point you in the right direction. It's also worth pointing out that many adapter sequences are very similar to each other so you may get a hit reported which isn't technically correct, but which has very similar sequence to the actual match.</p> <p>If overrepresented sequences are found but not identified by FastQC, try a  BLAST search.</p>"},{"location":"omics/intro-to-ngs/combined/#adapter-content","title":"Adapter Content","text":"<p>This module looks for common adapters in the sequence.</p> <p></p> <p>Explanations adapted from [https://dnacore.missouri.edu/PDF/FastQC_Manual.pdf][https://dnacore.missouri.edu/PDF/FastQC_Manual.pdf]</p>"},{"location":"omics/intro-to-ngs/combined/#optional-read-trimming","title":"Optional: Read trimming","text":"<p>In our <code>Per base sequence quality</code> we saw that the read quality dropped towards the end of the read. In order to ensure alignment and variant calling are as accurate as possible, we can perform quality trimming of reads.</p> <p>Trim Galore is a popular tool  that in the default mode performs two types of trimming: Quality trimming: Trims low quality bases from the 3' end of the read Adapter trimming: Automatically detects and removes known Illumina adapters that may be present in the data</p> <p>To perform trimming on the data, we first load the software which is installed as an a conda environment. For more information on using anaconda on the HPC, see this tutorial.</p> <pre><code>module load anaconda/3\nsource activate /cluster/tufts/bio/tools/conda_envs/trim_galore/\n</code></pre> <p>To run: <pre><code>mkdir trim\ntrim_galore -o trim raw_data/*\n</code></pre></p> <p>Result: <pre><code>...\n=== Summary ===\n\nTotal reads processed:                   4,652\nReads with adapters:                     1,606 (34.5%)\nReads written (passing filters):         4,652 (100.0%)\n\nTotal basepairs processed:       353,552 bp\nQuality-trimmed:                  24,906 bp (7.0%)\nTotal written (filtered):        326,448 bp (92.3%)\n...\n</code></pre></p> <p>Note that Trim Galore may trim adapters even in the case where FastQC found no adapters. This is because Trim Galore will remove partial adapters at the ends of reads.</p> <p>The result after trimming is much improved:</p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#read-alignment","title":"Read Alignment","text":"<p>Goals</p> <ul> <li>Align short reads to a references genome with BWA</li> <li>View alignment using IGV</li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/combined/#bwa-overview","title":"BWA Overview","text":"<p>Burrows-Wheeler Aligner (BWA) is a software package for mapping low-divergent  sequences against a large reference genome, such as the human genome.  The naive approach to read alignment is to compare a read to every position in the reference genome until a good match  is found is far too slow.  BWA solves this problem by creating an \"index\" of our reference sequence for faster lookup.</p> <p>The following figure shows a short read with a red segment followed by a blue segment that  we seek to align to a genome containing many blue and red segments. The table keeps track of all the locations where a given pattern of red and blue segments (seed sequence) occurs in the  reference genome. When BWA encounters a new read, it looks up the seed sequence at the beginning of the read in the table  and retrieves a set of positions that are potential alignment positions for that read.  This speeds up the search by reducing the number of positions to check for a good match.</p> <p></p> <p>BWA has three algorithms:</p> <ul> <li>BWA-backtrack: designed for Illumina sequence reads up to 100bp (3-step)</li> <li>BWA-SW:  designed for longer sequences ranging from 70bp to 1Mbp, long-read support and split alignment</li> <li>BWA-MEM: optimized for 70-100bp Illumina reads</li> </ul> <p>We'll use BWA-MEM.  Underlying the BWA index is the Burrows-Wheeler Transform This is beyond the scope of this course but is an widely used data compression algorithm.</p>"},{"location":"omics/intro-to-ngs/combined/#bwa-index","title":"BWA Index","text":"<p>In the following steps we'll create the BWA index. </p> <ol> <li> <p>Change to our reference data directory <code>cd intro-to-ngs/ref_data</code></p> </li> <li> <p>Preview our genome using the command <code>head</code> by typing:</p> </li> </ol> <p><code>head chr10.fa</code> </p> <p>You'll see the first 10 lines of the file <code>chr10.fa</code>: <pre><code>&gt;chr10 AC:CM000672.2 gi:568336\u2026   &lt;-- '&gt;' charachter followed by sequence name\nNNNNNNNNNNNNNNNNNNNNN             &lt;-- sequence\n\u2026\n</code></pre> This is an example of FASTA format. FASTA format is similar to the first two lines of FASTQ format, storing only the  sequence name and sequence.</p> <ol> <li>Load the BWA module, which will give us access to the <code>bwa</code> program: <pre><code>module load bwa/0.7.17\n</code></pre></li> </ol> <p>Test it out without any arguments in order to view the help message. <pre><code>bwa\n</code></pre></p> <p>Result: <pre><code>Program: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1198-dirty\nContact: Heng Li &lt;lh3@sanger.ac.uk&gt;\n\nUsage:   bwa &lt;command&gt; [options]\n\nCommand: index         index sequences in the FASTA format\n\u2026\n</code></pre></p> <p>Use the <code>bwa index</code> command to see usage instructions for genome indexing</p> <pre><code>bwa index\n</code></pre> <p>Result <pre><code>Usage:   bwa index [options] &lt;in.fasta&gt;\nOptions: -a STR    BWT construction algorithm \u2026\n</code></pre></p> <p>Run the command as instructed, using the default options: <pre><code>bwa index chr10.fa\n</code></pre></p> <p>Result: <pre><code>[bwa_index] Pack FASTA... 0.93 sec\n[bwa_index] Construct BWT for the packed sequence...\n[BWTIncCreate] textLength=267594844, availableWord=30828588\n    [BWTIncConstructFromPacked] 10 iterations done. 50853228 characters processed.\n[BWTIncConstructFromPacked] 20 iterations done. 93947292 characters processed.\n[BWTIncConstructFromPacked] 30 iterations done. 132245372 characters processed.\n[BWTIncConstructFromPacked] 40 iterations done. 166280796 characters processed.\n[BWTIncConstructFromPacked] 50 iterations done. 196527516 characters processed.\n[BWTIncConstructFromPacked] 60 iterations done. 223406844 characters processed.\n[BWTIncConstructFromPacked] 70 iterations done. 247293244 characters processed.\n[BWTIncConstructFromPacked] 80 iterations done. 267594844 characters processed.\n[bwt_gen] Finished constructing BWT in 80 iterations.\n[bwa_index] 59.13 seconds elapse.\n[bwa_index] Update BWT... 0.67 sec\n[bwa_index] Pack forward-only FASTA... 0.59 sec\n[bwa_index] Construct SA from BWT and Occ... 24.98 sec\n[main] Version: 0.7.17-r1198-dirty\n[main] CMD: bwa index chr10.fa\n[main] Real time: 87.087 sec; CPU: 86.306 sec\n</code></pre></p> <p>When it's done, take a look at the files produced by typing <code>ls</code>. The following is the result, with arrows and text on the right giving an explanation of each file.</p> <pre><code>chr10.fa      &lt;-- Original sequence\nchr10.fa.amb  &lt;-- Location of ambiguous (non-ATGC) nucleotides\nchr10.fa.ann  &lt;-- Sequence names, lengths\nchr10.fa.bwt  &lt;-- BWT suffix array\nchr10.fa.pac  &lt;-- Binary encoded sequence\nchr10.fa.sa   &lt;-- Suffix array index\n</code></pre>"},{"location":"omics/intro-to-ngs/combined/#bwa-alignment","title":"BWA alignment","text":"<p>Let's check the usage instructions for BWA mem by typing <code>bwa mem</code></p> <pre><code>Usage: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq]\n\nAlgorithm options:\n\n       -t INT        number of threads [1]\n       -k INT        minimum seed length [19]\n       -w INT        band width for banded alignment [100]\n       -d INT        off-diagonal X-dropoff [100]\n       -r FLOAT      look for internal seeds inside a seed longer than {-k} * FLOAT [1.5]\n       -y INT        seed occurrence for the 3rd round seeding [20]\n       -c INT        skip seeds with more than INT occurrences [500]\n       -D FLOAT      drop chains shorter than FLOAT fraction of the longest overlapping chain [0.50]\n       -W INT        discard a chain if seeded bases shorter than INT [0]\n       -m INT        perform at most INT rounds of mate rescues for each read [50]\n       -S            skip mate rescue\n       -P            skip pairing; mate rescue performed unless -S also in use\n...\n</code></pre> <p>Since our alignment command will have multiple arguments, it will be convenient to write a script.</p> <p>Go up one level to our main <code>intro-to-ngs</code> directory: <pre><code>cd ..\n</code></pre></p> <p>Make a new directory for our results <pre><code>mkdir results\n</code></pre></p> <p>Open a text editor with the program <code>nano</code> and create a new file called <code>bwa.sh</code>. <pre><code>nano bwa.sh\n</code></pre></p> <p>Enter the following text. Note that each line ends in a single backslash <code>\\</code>, which will be read as a line continuation. Be careful to put a space before the backslash and not after. This serves to make the script more readable.</p> <pre><code>module load bwa/0.7.17\n\nbwa mem \\\n-t 2 \\\n-M \\\n-R \"@RG\\tID:reads\\tSM:na12878\\tPL:illumina\" \\\n-o results/na12878.sam \\\nref_data/chr10.fa \\\nraw_data/na12878_1.fq \\\nraw_data/na12878_2.fq\n</code></pre> <p>Let's look line by line at the options we've given to BWA: 1. <code>-t 2</code> : BWA runs two parallel threads. Alignment is a task that is easy to parallelize  because alignment of a read is independent of other reads. Recall that in Setup we asked for a compute  node allocation with  <code>--cpus=4</code>, which can process up to 8 threads. Here we are using only 2 threads. </p> <ol> <li> <p><code>-M</code> : \"mark shorter split hits as secondary\". This option will change the SAM flag (discussed in next section) that  is assigned to short reads that have read segments mapped to distant locations. It optionn is needed for GATK/Picard compatibility, which are tools we use downstream. see this explanation on biostars</p> </li> <li> <p><code>-R \"@RG\\tID:reads\\tSM:na12878\\tPL:illumina\"</code>: Add a read group tag (RG), sample name (SM), and platform (PL) to our alignment file header.  We'll see where this appears in our output. In addition to being required for GATK, it's advisable to always add these  labels to make the origin of the reads clear.</p> </li> <li> <p><code>-o results/na12878.sam</code> :  Place the output in the results folder and give it a name</p> </li> <li> <p>The following arguments are our reference, read1 and read2 files, in the order required by BWA: <pre><code>ref_data/chr10.fa \\\nraw_data/na12878_1.fq \\\nraw_data/na12878_2.fq\n</code></pre></p> </li> </ol> <p>Exit nano by typing <code>^X</code> and follow prompts to save and name the file <code>bwa.sh</code>.</p> <p>Now we can run our script. <pre><code>sh bwa.sh\n</code></pre></p> <p>Result: <pre><code>[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 9304 sequences (707104 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 2256, 0, 0)\n[M::mem_pestat] skip orientation FF as there are not enough pairs\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (120, 160, 216)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 408)\n[M::mem_pestat] mean and std.dev: (172.35, 67.15)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 504)\n[M::mem_pestat] skip orientation RF as there are not enough pairs\n[M::mem_pestat] skip orientation RR as there are not enough pairs\n[M::mem_process_seqs] Processed 9304 reads in 1.034 CPU sec, 0.518 real sec\n</code></pre></p> <p>List the files in the results directory by typing <code>ls results</code>. Result: <pre><code>na12878.sam\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#sequence-alignment-map-sam","title":"Sequence Alignment Map (SAM)","text":"<p>Take a look at the output file: <pre><code>cd results\nhead na12878.sam\n</code></pre> The file has two sections</p> <p>Header: <pre><code>@SQ     SN:chr10        LN:133797422        &lt;-- Reference sequence name (SN) and length (LN)\n@RG     ID:reads        SM:na12878          &lt;-- Read group (ID) and sample (SM) information that we provided\n@PG ID:bwa PN:bwa VN:0.7.17\u2026 CL:bwa mem     &lt;-- Programs and arguments used in processing\n</code></pre></p> <p>Alignment:</p> 1 2 3 4 5 6 7 8 9 10 11 SRR098401.109756285 83 chr10 94760653 60 76M = 94760647 -82 CTAA\u2026 D?@A... SRR098401.109756285 163 chr10 94760647 60 76M = 94760653 82 ATTA\u2026 ?&gt;@@... <p>The fields: 1. Read ID 2. Flag: indicates alignment information e.g. paired, aligned, etc. Here is a useful site to decode flags. 3. Reference sequence name 4. Position on the reference sequence where mapping starts 5. Mapping Quality 6. CIGAR string: summary of alignment, e.g. match (M), insertion (I), deletion (D) 7. RNEXT: Name of reference sequence where the other read in the pair aligns 8. PNEXT: Position in the reference sequence where the other read in the pair aligns 9. TLEN: Template length, size of the original DNA or RNA fragment 10. Read Sequence 11. Read Quality</p> <p>More information on SAM format.</p>"},{"location":"omics/intro-to-ngs/combined/#alignment-quality-control","title":"Alignment Quality Control","text":"<p>Next, we'd like to know how well our reads aligned to the reference genome? We'll use a tool called <code>Samtools</code> to summarize the SAM Flags.</p> <p>To load the module: <pre><code>module load samtools/1.9\n</code></pre></p> <p>To run the <code>flagstat</code> program on our <code>SAM</code> file: <pre><code>samtools flagstat na12878.sam\n</code></pre></p> <p>Result: <pre><code>9306 + 0 in total (QC-passed reads + QC-failed reads)        &lt;-- We have only QC pass reads\n2 + 0 secondary                                              &lt;-- 2 reads have &gt;1 alignment position \n0 + 0 supplementary                                          &lt;-- for reads that align to multiple chromosomes\n0 + 0 duplicates                                             \n9271 + 0 mapped (99.62% : N/A)                               &lt;-- For exome data, &gt;90% alignment is expected    \n9304 + 0 paired in sequencing\n4652 + 0 read1\n4652 + 0 read2\n9226 + 0 properly paired (99.16% : N/A)\n9240 + 0 with itself and mate mapped\n29 + 0 singletons (0.31% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre></p> <p>Samtools flagstat is a great way to check to make sure that the aligment meets the quality expected. In this case, &gt;99% properly paired and mapped indicates a high quality alignment.</p>"},{"location":"omics/intro-to-ngs/combined/#summary","title":"Summary","text":""},{"location":"omics/intro-to-ngs/combined/#alignment-cleanup","title":"Alignment Cleanup","text":"<p>Goals</p> <ul> <li>Sort and Index SAM/BAM files</li> <li>Mark duplicate reads in BAM file</li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/combined/#sort-sam-file","title":"Sort SAM file","text":"<p>Downstream applications require that reads in SAM files be sorted by reference genome coordinates (fields 3 and 4 in each line of our SAM file). This will assist in fast search, display and other functions.</p> 1 2 3 4 5 6 7 8 9 10 11 SRR098401.109756285 83 chr10 94760653 60 76M = 94760647 -82 CTAA\u2026 D?@A... <p>We\u2019ll use the Picard toolkit for this and othter SAM file manipulations.</p> <p>Open another script in our course directory called picard.sh <pre><code>cd ..\nnano picard.sh\n</code></pre></p> <p>Enter the following text: <pre><code>module load picard/2.8.0\n\npicard SortSam \\\nINPUT=results/na12878.sam \\\nOUTPUT=results/na12878.srt.bam \\\nSORT_ORDER=coordinate\n</code></pre></p> <p>We have input our SAM file and we will output a Binary Alignment Map (BAM) file, which is a compressed version of SAM format.</p> <p>Exit nano by typing <code>^X</code> and follow prompts to save the file <code>picard.sh</code>.</p> <p>To run the script: <pre><code>sh picard.sh\n</code></pre></p> <p>Result: <pre><code>[Fri May 08 15:38:55 EDT 2020] picard.sam.SortSam INPUT=results/na12878.sam OUTPUT=results/na12878.srt.bam SORT_ORDER=coordinate    VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 15:38:55 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\nINFO    2020-05-08 15:38:56 SortSam Finished reading inputs, merging and writing to output now.\n[Fri May 08 15:38:57 EDT 2020] picard.sam.SortSam done. Elapsed time: 0.02 minutes.\nRuntime.totalMemory()=2058354688\n</code></pre></p> <p>Take a look at the results directory: <pre><code>ls results\n</code></pre></p> <p>The result shows that the sorted BAM file has been created: <pre><code>na12878.sam  na12878.srt.bam \n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#mark-duplicates-in-bam-file","title":"Mark Duplicates in BAM file","text":"<p>Many copies are made of a single DNA fragment during the sequencing process. The amount of duplication may not be the same for all sequences and this can cause biases in variant calling. Therefore, we mark the duplicates so the variant caller can focus on the unique reads.</p> <p>Duplicate reads are identified based on their alignment coordinates and CIGAR string. For example, the below alignment appears to have a G to A mutation in the majority of reads:</p> <p></p> <p>However, when the duplicates are removed, the number of reads supporting the mutation drops to one.</p> <p></p> <p>Let's add this step to our <code>picard.sh</code> script in order to illustrate how to include multiple steps in a single script. Note that when we run it, we'll rerun our previous steps as well.</p> <p><pre><code>nano picard.sh\n</code></pre> Add the following lines to the end of our script: <pre><code>printf  \"..... Starting Mark Duplicates ....\\n\\n\"\n\npicard MarkDuplicates \\\nINPUT=results/na12878.srt.bam \\\nOUTPUT=results/na12878.srt.markdup.bam \\\nREAD_NAME_REGEX=null \\\nMETRICS_FILE=results/na12878.markdup.txt\n</code></pre></p> <p>The first line is a formatted print (<code>printf</code>) statement that will display useful log lines when our script is running. The option <code>READ_NAME_REGEX=null</code> is added because our read names, downloaded from <code>GIAB</code> do not contain information about the position on the flowcell. When present this information can help with estimating optical duplicated. Typically, datasets do contain this information and it is best to omit this line when processing your data.</p> <p>To run our script (Note this will rerun the first step as well. This is only for demonstration purposes. If you were developing this for your own use, you would instead write all commands and run the script once): <pre><code>sh picard.sh\n</code></pre></p> <p>In addition to our previous log, we'll see our log line, followed by the output from Mark Duplicates: <pre><code>\u2026\n.... Starting Mark Duplicates ....\n\n[Fri May 08 16:03:52 EDT 2020] picard.sam.markduplicates.MarkDuplicates INPUT=[results/na12878.srt.sam] OUTPUT=results/na12878.srt.markdup.sam METRICS_FILE=results/na12878.markdup.txt READ_NAME_REGEX=null    MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 16:03:52 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\nINFO    2020-05-08 16:03:52 MarkDuplicates  Start of doWork freeMemory: 2042614304; totalMemory: 2058354688; maxMemory: 28631367680\nINFO    2020-05-08 16:03:52 MarkDuplicates  Reading input file and constructing read end information.\nINFO    2020-05-08 16:03:52 MarkDuplicates  Will retain up to 110120644 data points before spilling to disk.\nINFO    2020-05-08 16:04:03 MarkDuplicates  Read 9300 records. 0 pairs never matched.\nINFO    2020-05-08 16:04:09 MarkDuplicates  After buildSortedReadEndLists freeMemory: 2020313280; totalMemory: 2915041280; maxMemory: 28631367680\nINFO    2020-05-08 16:04:09 MarkDuplicates  Will retain up to 894730240 duplicate indices before spilling to disk.\nINFO    2020-05-08 16:04:11 MarkDuplicates  Traversing read pair information and detecting duplicates.\nINFO    2020-05-08 16:04:11 MarkDuplicates  Traversing fragment information and detecting duplicates.\nINFO    2020-05-08 16:04:11 MarkDuplicates  Sorting list of duplicate records.\nINFO    2020-05-08 16:04:14 MarkDuplicates  After generateDuplicateIndexes freeMemory: 3340626880; totalMemory: 10530324480; maxMemory: 28631367680\nINFO    2020-05-08 16:04:14 MarkDuplicates  Marking 864 records as duplicates.\nWARNING 2020-05-08 16:04:14 MarkDuplicates  Skipped optical duplicate cluster discovery; library size estimation may be inaccurate!\nINFO    2020-05-08 16:04:14 MarkDuplicates  Reads are assumed to be ordered by: coordinate\nINFO    2020-05-08 16:04:16 MarkDuplicates  Before output close freeMemory: 10507885056; totalMemory: 10530324480; maxMemory: 28631367680\nINFO    2020-05-08 16:04:16 MarkDuplicates  After output close freeMemory: 10507907720; totalMemory: 10530324480; maxMemory: 28631367680\n[Fri May 08 16:04:16 EDT 2020] pic\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#mark-duplicates-metrics-file","title":"Mark Duplicates Metrics file","text":"<p>The following is the metrics file <code>na12878.markdup.txt</code> generated by Picard Mark Duplicates:</p> <p>| LIBRARY | UNPAIRED_READS_EXAMINED | READ_PAIRS_EXAMINED | SECONDARY_OR_SUPPLEMENTARY_RDS | UNMAPPED_READS | UNPAIRED_READ_DUPLICATES | READ_PAIR_DUPLICATES | READ_PAIR_OPTICAL_DUPLICATES | PERCENT_DUPLICATION | ESTIMATED_LIBRARY_SIZE |</p> <p>|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:| | Unknown | 29 | 4620 | 2 | 35 | 14 | 425 | 0 | 0.093214 | 23546 |</p> <p>Normal % duplication for exome sequencing data is 10-30%. By scrolling to the left in this table we see that our percent duplication is <code>0.093214%</code>.</p>"},{"location":"omics/intro-to-ngs/combined/#index-the-bam-file","title":"Index the BAM file","text":"<p>In order to view the alignment with the Integrated Genomics Viewer (IGV) we are required to create an index files for our BAM file. This facilitates fast lookup of genomics coordinates.</p> <p>Let's continue editing our script: <pre><code>nano picard.sh\n</code></pre></p> <p>Add the following lines at the end of the script: <pre><code>printf  '.... Start BAM Indexing ....\\n\\n'\n\npicard BuildBamIndex \\\nINPUT=results/na12878.srt.markdup.bam\n</code></pre></p> <p>Run our script: <pre><code>sh picard.sh\n</code></pre></p> <p>Result, in addition to previous output: <pre><code>.... Start BAM Indexing ....\n\n[Fri May 08 16:24:17 EDT 2020] picard.sam.BuildBamIndex INPUT=results/na12878.srt.markdup.bam    VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 16:24:17 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\nWARNING: BAM index file /cluster/home/rbator01/intro-to-ngs/results/na12878.srt.markdup.bai is older than BAM /cluster/home/rbator01/intro-to-ngs/results/na12878.srt.markdup.bam\nINFO    2020-05-08 16:24:18 BuildBamIndex   Successfully wrote bam index file /cluster/home/rbator01/intro-to-ngs/results/na12878.srt.markdup.bai\n[Fri May 08 16:24:18 EDT 2020] picard.sam.BuildBamIndex done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=2058354688\n</code></pre></p> <p>We can see the files that were generated by typing <code>ls results</code> <pre><code>na12878.sam\nna12878.srt.bam\nna12878.srt.markdup.bam\nna12878.markdup.txt\nna12878.srt.markdup.bai     &lt;--- Index file\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#bam-visualization-with-igv","title":"BAM Visualization with IGV","text":"<ol> <li>With a Chrome web browser, visit https://ondemand.cluster.tufts.edu</li> <li>Login with your Tufts credentials</li> <li> <p>Choose Interactive Apps-&gt;IGV. Set parameters, click \u201cLaunch\u201d</p> </li> <li> <p>Choose <code>Interactive Apps-&gt;IGV</code>. Set parameters below and , click <code>Launch</code> </p> </li> <li>Choose the following compute resource parameters: 1 hour, 2 cores, 4 GB memory, Default Batch Parition, Default Reservation</li> </ol> <p></p> <ol> <li>Click the blue button <code>Launch NoVNC in New Tab</code> when it appears</li> </ol> <p>After this the IGV window will appear, probably as a small window on a grey background. Click the square icon in the top right corner to maximize the window.</p> <p></p>"},{"location":"omics/intro-to-ngs/combined/#load-reference-genome-and-bam-file","title":"Load reference genome and BAM file","text":"<ol> <li>Choose reference genome by clicking the <code>Genomes</code> menu and selecting <code>Load Genome from Server...</code></li> </ol> <ol> <li>Scroll down to <code>Human hg38</code></li> </ol> <ol> <li> <p>DO NOT check <code>Download Sequnence</code></p> </li> <li> <p>Click <code>OK</code></p> </li> <li> <p>Load the BAM file by clicking the <code>File</code> menu and select <code>Load from File...</code></p> </li> </ol> <p></p> <ol> <li> <p>Navigate to the results folder in the course directory, e.g. <code>/cluster/home/your-user-name/intro-to-ngs/results</code>.  </p> </li> <li> <p>Select <code>na12878.srt.markdup.bam</code></p> </li> </ol> <p></p> <p>You will have the following view:</p> <p></p> <p>Each row of data is called a track. There are five tracks visible: the top track shows the pq bands of the entire chromosome, followed by the reference genome coordinate track, followed by two tracks of our alignment (coverage and reads, respectively) which don't yet show data, followed by a reference genome annotation track called \"Genes\".</p>"},{"location":"omics/intro-to-ngs/combined/#examining-a-gene","title":"Examining a gene","text":"<ol> <li>In the box indicated in green below, type gene name \"Cyp2c19\" and hit enter. You will see the gene model display in the \u201cGenes\u201d track, showing vertical bars where exons are located</li> </ol> <p>Troubleshooting tip: At times IGV on demand will stop allowing the user to type input. If that happens, close the tab, go back to the on demand window, rejoin the session by clicking <code>Launch NoVNC in New Tab</code>.</p> <ol> <li>Let's zoom in on exon 7. You can hover over exons in the <code>Genes</code> track to get information such as exon number. Click and drag over a region in the reference coordinate track to zoom in on exon 7 (highlighted in green below.)</li> </ol> <p></p> <ol> <li>We can see that there is a variant in this exon.</li> </ol> <p></p> <ol> <li>Zoom in even further until the nucleotide letters are clear. Then, hover with your mouse over the coverage track to find out more information about this variant.</li> </ol> <p></p> <p>It appear there are two variants next to each other: heterozygous<code>C&gt;T</code> at position <code>chr10:94,842,865</code> and homozygous <code>A&gt;G</code> at position <code>chr10:94,842,866</code>. Next, we'll explore the meaning of these variants.</p> <p></p> <p>This lesson adapted from HBC NGS Data Analysis</p>"},{"location":"omics/intro-to-ngs/combined/#variant-calling_1","title":"Variant Calling","text":"<p>Goals</p> <ul> <li>Use Genome Analysis Tool Kit (GATK) to call variants</li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/combined/#prepare-the-reference-sequence-for-gatk","title":"Prepare the reference sequence for GATK","text":"<p>GATK requires a Sequence Dictionary for reference genomes used in variant calling. The sequence dictionary contains names and lengths of all chromosomes in the reference genome. The information in this file is transferred to the Variant Call File (VCF) when it is produced, so that there is no ambiguity about which reference was used to produce the file.</p> <p>Let's open a new script <pre><code>nano prepare.sh\n</code></pre></p> <p>Add these lines: <pre><code>module load samtools/1.9\nmodule load picard/2.8.0\n\nsamtools faidx ref_data/chr10.fa\n\npicard CreateSequenceDictionary \\\nREFERENCE=ref_data/chr10.fa \\\nOUTPUT=ref_data/chr10.dict\n</code></pre></p> <p>With these steps, we load the necessary modules, created a FASTA index for our reference sequence, and use Picard to create our Sequence Dictionary.</p> <p>Run our script: <pre><code>sh prepare.sh\n</code></pre></p> <p>Result: <pre><code>[Fri May 08 16:52:35 EDT 2020] picard.sam.CreateSequenceDictionary REFERENCE=ref_data/chr10.fa OUTPUT=ref_data/chr10.dict    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n[Fri May 08 16:52:35 EDT 2020] Executing as rbator01@pcomp31 on Linux 2.6.32-696.1.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27; Picard version: 2.8.0-SNAPSHOT\n[Fri May 08 16:52:35 EDT 2020] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=2058354688\n</code></pre></p> <p>Two new files are created in the folder <code>ref_data</code>, our FASTA index (fai) and sequence dictionary (dict): <pre><code>chr10.fa.fai\nchr10.dict\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#variant-calling-with-gatk-haplotypecaller","title":"Variant Calling with GATK HaplotypeCaller","text":"<p>GATK has two main goals: - Separate true variants from sequencing error - Establish which variants co-exist on a single DNA strand (haplotype)</p> <p>The figure below described the 4 stages of the GATK HaplotypeCaller algorithm (from software.broadinstitute.org ).</p> <p></p> <p>Paraphrasing from the GATK documentation, the four stages are as follows:</p> <ol> <li> <p>Define active regions. The program determines which regions of the genome it needs to operate on, based on the presence of significant evidence for variation.</p> </li> <li> <p>Determine haplotypes by re-assembly of the active region. For each active region, the program builds a graph to represent all possible read sequences spanning the region. For example, the top first read starts in the <code>TATG</code> bubble that is common to all reads, then takes the top path to the <code>A</code> bubble, continues through the <code>AAT</code>, etc. The program then realigns each haplotype (path through the graph) against the reference sequence in order to identify potentially variant sites.</p> </li> <li> <p>Determine likelihoods of the haplotypes given the read data. The goal of this stage is to evaluate which haplotypes have the most read support. For each active region, the program performs a pairwise alignment of each read against each haplotype using the PairHMM algorithm, which takes into account other information about the data, such as quality scores. This produces a matrix of likelihoods of haplotypes given the read data. These likelihoods are then used to calculate how much evidence there is for individual alleles at each variant site (marginalization over alleles).</p> </li> <li> <p>Assign sample genotypes. The final step is to determine which sequences were most likely present in the data. This step uses Bayes' rule to find the most likely genotype, given the allele likelihoods calculated in the last step.</p> </li> </ol>"},{"location":"omics/intro-to-ngs/combined/#run-gatk-on-our-bam-file","title":"Run GATK on our BAM file","text":"<p>To load the module on our system, we'll type:</p> <p><pre><code>module load GATK/3.7\n</code></pre> We can check the usage for GATK, which has many tools in addition to HaplotypeCaller: <pre><code>gatk --help\n</code></pre></p> <p>The result shows the many different tools inside GATK. The relevant lines for HaplotypeCaller are: <pre><code>\u2026\nusage: java -jar GenomeAnalysisTK.jar -T &lt;analysis_type&gt; \u2026\n\u2026\n haplotypecaller                 \n   HaplotypeCaller               Call germline SNPs and indels via local re-assembly of haplotypes\n   HaplotypeResolver             Haplotype-based resolution of variants in separate callsets.\n</code></pre></p> <p>For tool specific help, we type: <pre><code>gatk -T HaplotypeCaller --help\n</code></pre></p> <p>Let's write a new script: <pre><code>nano gatk.sh\n</code></pre></p> <p>Add these lines, which specify the reference file, input BAM, and output VCF. <pre><code>module load GATK/3.7\n\ngatk -T HaplotypeCaller \\\n-R ref_data/chr10.fa \\\n-I results/na12878.srt.markdup.bam \\\n-o results/na12878.vcf\n</code></pre></p> <p>Run our script: <pre><code>sh gatk.sh\n</code></pre></p> <p>Result: <pre><code>INFO  17:17:41,656 HelpFormatter - -----------\nINFO  17:17:41,660 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18 \n\u2026\n</code></pre></p> <p>Two new files have appeared in our results folder, the variant call file (VCF) and index file, respectively: <pre><code>na12878.vcf \nna12878.vcf.idx\n</code></pre></p>"},{"location":"omics/intro-to-ngs/combined/#vcf-format","title":"VCF format","text":"<p>We can take a look at the first few lines of our vcf file: <pre><code>cd results\nhead na12878.vcf\n</code></pre></p> <p>VCF, like BAM files, files contain two sections: A header section, indicated by the presence of <code>#</code> at the beginning of the line, followed by data lines for each variant that was called. <pre><code>##fileformat=VCFv4.2\n##FILTER=&lt;ID=LowQual,Description=\"Low quality\"&gt;\n##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Allelic depths for the ref and alt alleles in the order listed\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth \u2026\n##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=PL,Number=G,Type=Integer,Description=\"Normalized, Phred-scaled likelihoods for genotypes \u2026\n##GATKCommandLine.HaplotypeCaller=&lt;ID=HaplotypeCaller,...\n\u2026.\n##contig=&lt;ID=chr10,length=135534747&gt;\n##reference=file:///cluster/home/tutln01/intro-to-ngs/ref_data/chr10.fa\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO            FORMAT      NA12878\nchr10   96521422    .   A   G   60.28   .         AC=2;AF=1.00; \u2026.  GT:AD:DP:GQ:PL  1/1:0,3:3:9:88,9,0\nchr10   96522365    .   T   C   1134.77 .         AC=1;AF=0.500;\u2026.  GT:AD:DP:GQ:PL  0/1:47,37:84:99:1163,0,1502\n</code></pre></p> <p>The header lines explain the meaning of notation found in the body section of the VCF, as well as information about the reference and software used to produce the VCF. The last header line lists the column titles for information, and the last column has the sample name. VCF can be used to represent multiple samples, and in that case, each sample would have it's own subsequent column.</p> <p>Let's look at the body section in table format:</p> CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878 chr10 96521422 . A G 60.28 . AC=2;AF=1.00; \u2026. GT:AD:DP:GQ:PL 1/1:0,3:3:9:88,9,0 <p>Fixed fields (same for all samples in the VCF) CHROM - Chromosome POS - Position ID - Identifier. May be present if the VCF was annotated with known variants, for example, rs numbers from dbSNP. REF - Reference sequence base ALT - Alternate base, comma separated list of non-reference alleles (usually) found in the samples represented by the VCF QUAL - Phred scaled quality score for the variant, i.e. \\(10log_{10}\\) prob(call is wrong). FILTER - PASS if this position has passed all filters, otherwise the name of the filter islisted. INFO - Additional information</p> <p>Genotype fields (one per sample): FORMAT - This field specifies the format that will be used to give information in each sample column. VCF can represent In this case, we see <code>GT:AD:DP:GQ:PL</code>, which corresponds to the values <code>1/1:0,3:3:9:88,9,0</code>. GT - Genotype, encoded as allele values separated by either '/' (unphaseD) or '|' (phased - known to be on the same chromosome arm). The allele values are 0 for the reference allele and 1 for the first allele listed. AD - Allele depth at this position for the same, reference first followed by first allele listed DP - Read depth at this position for the sample GQ - Genotype quality PL - Genotype liklihoods</p> <p>For more on the rich VCF format, see the VCF format specification from Samtools</p>"},{"location":"omics/intro-to-ngs/combined/#vcf-quality-control","title":"VCF Quality Control","text":"<p>It's always a good idea when writing a new pipeline, to ask: How well did our variant calling perform? In this case, the best way to check the performance would be to compare the variants we called in this exercise matched the \"known\" variants for NA12878 in the NIST callset. That exercise is beyond the scope of this workshop.</p>"},{"location":"omics/intro-to-ngs/combined/#add-our-vcf-to-igv","title":"Add our VCF to IGV","text":"<p>We can add a VCF track to our IGV windows.</p> <ol> <li>Go to back to IGV on demand</li> <li>Click the <code>File</code> menu and select <code>Load from File</code></li> <li>Select the file <code>na12878.vcf</code></li> </ol> <p></p> <ol> <li>We'll see a variant track appear above the coverage track. Hover over the colored blocks on the variant track in order to see the information in the VCF.</li> </ol> <p></p>"},{"location":"omics/intro-to-ngs/combined/#summary_1","title":"Summary","text":""},{"location":"omics/intro-to-ngs/combined/#variant-annotation","title":"Variant Annotation","text":"<p>Goals</p> <ul> <li>Use the Variant Effect Predictor (VEP) online web server to annotate variants  </li> <li>Identify amino acid changing substitutions in our VCF</li> </ul> <p></p>"},{"location":"omics/intro-to-ngs/combined/#vep-overview","title":"VEP overview","text":"<p>VEP will add annotation from a number of sources for each variant that we upload. Below is a subset of the most commonly used annotations annotations.</p> <ul> <li> <p>Identifiers: Gene, transcript, protein, etc.</p> </li> <li> <p>Frequency data: Allele frequency information from multiple public databases. 1000 Genomes, (gnomAD)[https://gnomad.broadinstitute.org/], (ESP)[https://evs.gs.washington.edu/EVS/]  Allele frequency information is helpful to understand whether the input variant is common or rare in different geographical populations.</p> </li> </ul> <p>-Pathogenicity predictions: Computational predictions of whether a variant will affect the protein function. Various algorithms are available (SIFT, PolyPhen2, CADD, etc)</p> <ul> <li> <p>Disease Association: Clinical significance and disease association as reported in ClinVar. ClinVar is a widely used database that aggregates and curates clinical reports of variants with clinical determinations. The clinical significances reported in VEP range from <code>Benign</code> to <code>Pathogenic</code> and usually have a disease annotation.</p> </li> <li> <p>Consequence: For each variant, VEP identifies all transcripts in the selected database (Ensembl or Refseq) that overlaps with the variant coordinates. The consequence of the variant with respect to the transcript is then evaluated based on the following diagram.</p> </li> </ul> <p></p> <p>These consequences are then binned into impact groups: LOW, MODERATE, MODIFIER, HIGH. For a full mapping to consequence to impact, see VEP</p> <p>We'll run VEP on the VCF that we produced and analyze the variant consequences.</p>"},{"location":"omics/intro-to-ngs/combined/#download-the-vcf","title":"Download the VCF","text":"<p>First, we'll download the VCF from the cluster to our local computer.</p> <ol> <li>Go back to https://ondemand.cluster.tufts.edu</li> <li>In the top grey menu, click <code>Files</code> and select <code>Home Directory</code>.</li> </ol> <p></p> <ol> <li>Select <code>intro-to-ngs/results/na12878.vcf</code></li> </ol> <p></p> <ol> <li>Click <code>Download</code></li> </ol>"},{"location":"omics/intro-to-ngs/combined/#run-vep","title":"Run VEP","text":"<ol> <li> <p>In web browser tab, navigate to to https://useast.ensembl.org/Tools/VEP Note that VEP can also be run on the command line on our HPC, resulting in a text file (txt or vcf). You are welcome to ask for instructions to run the command line VEP. For single VCF analysis, the web server is recommended in order to take advantage of the visualization tools.</p> </li> <li> <p>In the <code>Species</code> section choose <code>Human (Homo sapiens)</code> (should be the default)</p> </li> <li> <p>In the <code>Input data</code> section choose <code>Or upload file:</code> and navigate to the downloaded file <code>na12878.vcf</code></p> </li> <li> <p>Under <code>Transcript database to use</code> select <code>RefSeq transcripts</code></p> </li> </ol> <p></p> <ol> <li>Click <code>Run</code></li> </ol>"},{"location":"omics/intro-to-ngs/combined/#viewing-vep-results","title":"Viewing VEP results","text":"<p>When your job is done, click <code>View Results</code></p> <p></p> <p>Our goal is to identify variants that change the coding sequence. We can see in the <code>Coding Consequences</code> box on the right that 20% of the variants are <code>missense</code>, which means that they change the coding sequence of the transcript.</p>"},{"location":"omics/intro-to-ngs/combined/#filtering-vep-consequences","title":"Filtering VEP consequences","text":"<p>Under <code>Filters</code> choose <code>Consequence</code> + <code>is</code> + <code>missense_variant</code> and click <code>Add</code> You should see 1 row - here are a subset of interesting columns:</p> Location Allele Consequence IMPACT SYMBOL BIOTYPE Amino_acids 10:94842866-94842866 G missense_variant MODERATE CYP2C19 protein_coding I/V <p>| Existing_variation | SIFT | PolyPhen | AF | Clinical Significance | |:---:|:---:|:---:|:---:| | rs3758581,CM983294 | tolerated(0.38) | benign(0.05) | 0.9515 | |</p> <p>Based on the annotations, one can conclude that this variant unlikely to cause disease. This is consistent with what we know about <code>NA12878</code> being a healthy individual.</p> <p>Though the vatiant does change the amino acid from <code>I</code> to <code>V</code>, both SIFT, PolyPhen both suggest that this change does not alter protein function. Furthermore, there is no ClinVar report associated with this variant. Finally, the maximum allele frequency found for this variant in the <code>1000 Genomes</code> database is <code>0.95</code>, meaning it is a very common variant and unlikely to be pathogenic.</p>"},{"location":"omics/intro-to-ngs/combined/#summary_2","title":"summary","text":""},{"location":"omics/intro-to-ngs/combined/#references","title":"References","text":"<ul> <li>Illumina</li> <li>Ploidy</li> </ul>"},{"location":"omics/intro-to-proteomics/00_background/","title":"Background","text":""},{"location":"omics/intro-to-proteomics/00_background/#background","title":"Background","text":"<p>Proteins are composed of amino acids and the way in which they are arranged determines the final shape of the protein. Proteins have the following levels of organization:</p> <ul> <li>Primary Structure: amino acid sequence</li> <li>Secondary Structure: amino acid sequences linked by hydrogen bonds</li> <li>Tertiary Structure: organization of secondary structures</li> <li>Quaternary Structure: organization of multiple amino acid chains</li> </ul> <p></p>"},{"location":"omics/intro-to-proteomics/00_background/#proteomics-data","title":"Proteomics Data","text":"<p>In a tissue sample, whether it be from your skin, liver, heart, etc. - there are millions of protein molecules. So how does one determine which proteins are present in a particular sample?</p> <p>Currently a popular high-throughput stategy to accomplish this is called shotgun proteomics, where:</p> <ul> <li>A protein sample is digested (broken apart) resulting in a peptide mixture</li> <li>Those peptides are separated with Liquid Chromatography/Tandem Mass Spectra (LC-MS/MS)</li> <li>These spectra are compared to a protein sequence database to identify the peptides</li> </ul> <p></p>"},{"location":"omics/intro-to-proteomics/00_background/#protein-digestion","title":"Protein Digestion","text":"<p>Proteins are often complex 3-dimensional structures, with variable sequences. To assess what proteins are present, we need a way of breaking up these proteins into something we can identify. In shotgun proteomics:</p> <ul> <li>Proteins are denatured (a process in which protein folding is undone)</li> <li>The resulting polymer is digested with a serine protease called Trypsin. </li> </ul> <p>Info</p> <p>Trypsin will cleaves peptide sequences at the c-terminal of Ariginine (R) and Lysine (K) residues (Unless the next residue is a Proline (P))</p> <p></p>"},{"location":"omics/intro-to-proteomics/00_background/#liquid-chromatography","title":"Liquid Chromatography","text":"<p>The next step in this process is to separate these peptides and we can accomplish this with liquid chromatography. Currently, high-pressure liquid chromatography is often used to achieve separation where-in:</p> <ul> <li> <p>A solvent (with our peptides) is pumped through, at high pressure, a column with packing material</p> <ul> <li>This packing material ensures that peptides (of different sizes, charges, affinity etc.) move at different rates, separating them into bands</li> </ul> </li> <li> <p>As each band passes through a detector at the end of the column, and a peak is created with a height proportional to the concentration of dye in that band.</p> </li> </ul> <p></p>"},{"location":"omics/intro-to-proteomics/00_background/#mass-spetrometry","title":"Mass Spetrometry","text":"<p>Now that our peptides have been separated, we need to gather more information to be able to identify these peptides. Mass Spectrometry (MS) can be used to gain information about the composition of these peptides, where-in:</p> <ul> <li>Analytes are ionized into gas-phase ions</li> <li>The ionized analyates are then separated based on their mass (m) to charge (z) ratio</li> <li>The detector marks the number of ions at each m/z value</li> </ul> <p></p> <p>Tandem MS/MS</p> <p>Often, this process is repeated in what is called Tandem MS/MS. After the first round of MS, the precursor ions are heated with neutral molecules (helium, nitrogen or argon) to achieve a more sensitive detection.</p>"},{"location":"omics/intro-to-proteomics/00_background/#protein-identification","title":"Protein Identification","text":"<p>Now that we have spectra assoicated with our peptides we need some way of matching these back to a protein. Luckily, there are several databases that can help! The basic principle is as follows:</p> <ul> <li>Experimental spectrum are correlated to the theoretical spectrum for a protein in a database</li> <li>A scoring algorithm is used to assess how well each experimental ion matches the theortical one in the database</li> <li>The highest scoring hit is used to then label the peptide</li> </ul> <p></p> <p>Protein Identification Considerations</p> <p>It should be noted that when identifying peptides, most algorithms will attempt to match non-unique peptides (shared/razor peptides) to a protein that has a unique peptide/ or the most other peptides. In this way the algorithm attempts to safe-gaurd against false positives. In the following example, the second peptide would be assigned to protein 1:</p> <p></p>"},{"location":"omics/intro-to-proteomics/00_background/#labelling","title":"Labelling","text":"<p>When quantifying proteins with mass spectrometry, there are a few options:</p> <p></p> <p>Targeted methods assess a narrow, pre-defined, set of proteins. Alternatively, one could take an explorative approach and assess all possible proteins. Exploratory methods can include labelled and label-free methods. Labelling invovles adding a specific mass tag to peptides. In this way it is easier to trace the origins of different peptides:</p> <p></p> <p>The following table briefly covers the pros and cons of labelled v. label-free techniques. For a more in-depth explanation, check out the following Galaxy Article:</p> category label-free labelled machine time more less wet lab complexity &amp; time little medium comparability of samples difficult easy data analysis complex complex study design flexible fixed"},{"location":"omics/intro-to-proteomics/00_background/#references","title":"References","text":"<ol> <li>https://rformassspectrometry.github.io/docs/sec-msintro.html</li> <li>http://sepsis-omics.github.io/tutorials/modules/xtandem/#references</li> <li>https://link.springer.com/protocol/10.1385/1-59745-275-0:87</li> <li>https://www.bioconductor.org/packages/release/data/experiment/vignettes/RforProteomics/inst/doc/RforProteomics.html</li> <li>https://medical-dictionary.thefreedictionary.com/Denaturation+(biochemistry)</li> <li>https://febs.onlinelibrary.wiley.com/doi/abs/10.1016/0014-5793%2871%2980373-3</li> <li>https://www.waters.com/nextgen/au/en/education/primers/beginner-s-guide-to-liquid-chromatography/how-does-high-performance-liquid-chromatography-work.html</li> <li>https://www.nature.com/articles/nature01511</li> <li>https://www.sciencedirect.com/science/article/abs/pii/S0076687905020057?via%3Dihub</li> <li>https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/introduction/slides-plain.html</li> </ol>"},{"location":"omics/intro-to-proteomics/01_setup/","title":"Setup","text":""},{"location":"omics/intro-to-proteomics/01_setup/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>8GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshop</code>---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p>"},{"location":"omics/intro-to-proteomics/01_setup/#project-setup","title":"Project Setup","text":"<p>We are going to create a new project to begin:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>intro-to-proteomics</code>)</li> <li><code>Create Project</code></li> </ol>"},{"location":"omics/intro-to-proteomics/01_setup/#file-organization","title":"File Organization","text":"<p>In our project we will need some folders to contain our scripts, data and results:</p> <ul> <li>Click the New Folder icon</li> <li>Create a folder called data and click ok</li> <li>Following the same process, create a scripts folder and a results folder</li> </ul>"},{"location":"omics/intro-to-rnaseq/01_Setup/","title":"Setup","text":""},{"location":"omics/intro-to-rnaseq/01_Setup/#setup","title":"Setup","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-rnaseq/01_Setup/#goals","title":"Goals","text":"<ul> <li>Connect to the HPC cluster via On Demand Interface</li> <li>Download data</li> </ul>"},{"location":"omics/intro-to-rnaseq/01_Setup/#log-into-the-hpc-clusters-on-demand-interface","title":"Log into the HPC cluster's On Demand interface","text":"<ul> <li>Open a Chrome browser visit ondemand.cluster.tufts.edu</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose Clusters-&gt;HPC Shell Access</li> </ul> <ul> <li>Type your password at the prompt (the password will be hidden for security purposes):</li> </ul> <p><code>whuo01@login.cluster.tufts.edu's password:</code></p> <ul> <li>You'll see a welcome message and a bash prompt, for example for user <code>whuo01</code>:</li> </ul> <p><code>[whuo01@login001 ~]$</code> This indicates you are logged in to the login node.</p> <ul> <li>Type <code>clear</code> to clear the screen</li> </ul>"},{"location":"omics/intro-to-rnaseq/01_Setup/#compute-node-allocation","title":"Compute node allocation","text":"<ul> <li>Get an interactive session on a compute node by typing:</li> </ul> <p><code>srun --pty -t 3:00:00  --mem 16G  -N 1 -n 4 bash</code></p> <p>Once you hit enter, you will see something like below showing that the job is queued: <pre><code>[whuo01@login001 ~]$ srun --pty -t 3:00:00  --mem 16G  -N 1 -n 4 bash\nsrun: job 55918493 queued and waiting for resources\n</code></pre> If wait times are very long, you can try a different partitions by adding, e.g. <code>-p interactive</code> before <code>bash</code>. Or, if you are you registered for the workshop, you can use following option before <code>bash</code>: <code>-p preempt --reservation=bioworkshop</code>. This reservation will be available for one week after the workshop start. You can press <code>Ctrl-C</code> to cancel your request and try again with different options, e.g.: <pre><code>[whuo01@login001 ~]$ srun --pty -t 3:00:00  --mem 16G  -N 1 -n 4 -p interactive bash\n[whuo01@pcomp45 ~]$\n</code></pre></p> <p>The success is indicated by the change of node name after your username. Here it was changed from <code>login001</code> to <code>pcomp45</code>.  This is an indication that you may proceed to the next step. Note: If you go through this workshop in multiple steps, you will have to rerun this step each time you log in.</p>"},{"location":"omics/intro-to-rnaseq/01_Setup/#course-data","title":"Course data","text":"<ul> <li>Since our home directory will likely not have enough space for the analysis (&gt; 3Gb), we'll work in a course directory.  Your work will be saved here for 30 days.**  Change to the course directory</li> </ul> <pre><code>cd /cluster/tufts/bio/tools/training/intro-to-rnaseq/users/\n</code></pre> <p>**Note: If you have a project directory for your lab, you may use this instead. These are located in <code>/cluster/tufts</code> with names like <code>/cluster/tufts/labname/username/</code>. If you don't know whether you have project space, please email tts-research@tufts.edu.</p> <ul> <li> <p>Make a directory for your work (replace <code>whuo01</code> in the below commands with your username) <pre><code>mkdir whuo01\ncd whuo01\n</code></pre></p> </li> <li> <p>Copy the course files into your own directory: <pre><code>cp /cluster/tufts/bio/tools/training/intro-to-rnaseq/intro-to-RNA-seq-May-2020.tar.gz .\n</code></pre></p> </li> <li> <p>Unzip the course directory: <pre><code>tar -xvzf intro-to-RNA-seq-May-2020.tar.gz\n</code></pre></p> </li> <li> <p>Take a look at the contents of the unzipped directory by typing: <code>tree intro-to-RNA-seq</code></p> </li> </ul> <p>Result: <pre><code>intro-to-RNA-seq/\n\u251c\u2500\u2500 ERP004763_info.txt                 &lt;-- sample description\n\u251c\u2500\u2500 raw_data                           &lt;-- Folder with fastq files\n\u2502   \u251c\u2500\u2500 sample_info.txt\n\u2502   \u251c\u2500\u2500 SNF2\n\u2502   \u2502   \u251c\u2500\u2500 ERR458500.fastq.gz         &lt;-- gzip compressed fastq files\n\u2502   \u2502   \u251c\u2500\u2500 ERR458501.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ERR458502.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ERR458503.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ERR458504.fastq.gz\n\u2502   \u2502   \u251c\u2500\u2500 ERR458505.fastq.gz\n\u2502   \u2502   \u2514\u2500\u2500 ERR458506.fastq.gz\n\u2502   \u2514\u2500\u2500 WT\n\u2502       \u251c\u2500\u2500 ERR458493.fastq.gz\n\u2502       \u251c\u2500\u2500 ERR458494.fastq.gz\n\u2502       \u251c\u2500\u2500 ERR458495.fastq.gz\n\u2502       \u251c\u2500\u2500 ERR458496.fastq.gz\n\u2502       \u251c\u2500\u2500 ERR458497.fastq.gz\n\u2502       \u251c\u2500\u2500 ERR458498.fastq.gz\n\u2502       \u2514\u2500\u2500 ERR458499.fastq.gz\n\u2514\u2500\u2500 scripts                           &lt;-- Folder with all commands\n    \u251c\u2500\u2500 fastqc.sh\n    \u251c\u2500\u2500 featurecounts.sh\n    \u251c\u2500\u2500 intro.R\n    \u251c\u2500\u2500 sbatch_star_align_individual.sh\n    \u251c\u2500\u2500 sbatch_star_align.sh\n    \u2514\u2500\u2500 sbatch_star_align_SNF2.sh\n\n4 directories, 22 files\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/01_Setup/#data-for-the-class","title":"Data for the class","text":"<p>Publication: Statistical Models for RNA-seq Data Derived From a Two-Condition 48-replicate Experiment.</p> <p>Purpose: The experiment seeks to compare a wild type Saccharomyces cerevisiae with a mutant that contains a knock-out in the gene SNF2. The purpose of the study is to analyze variability in sequencing replicates.</p> <p>Project access number: PRJEB5348</p> <p>Samples: The <code>WT</code> folder contains 7 sequencing files from a wild type yeast sample, <code>SNF2</code> contains 7 sequencing files from a yeast sample with a knock-out mutation in the gene SNF2. Note that for the workshop purposes we are treating the 7 sequencing files as if they originate from separate biological replicates.</p> <p>Organism: Saccharomyces cerevisiae</p> <p>Sequencing: Illumina HiSeq, Single End, 50bp read length</p>"},{"location":"omics/intro-to-rnaseq/02_Quality_Control/","title":"Quality Control","text":""},{"location":"omics/intro-to-rnaseq/02_Quality_Control/#quality-control","title":"Quality control","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-rnaseq/02_Quality_Control/#goals","title":"Goals","text":"<ul> <li>Understand FastQ file format</li> <li>Run FastQC to asses data quality</li> </ul>"},{"location":"omics/intro-to-rnaseq/02_Quality_Control/#take-a-look-at-our-raw-data","title":"Take a look at our raw data","text":"<p>Usually RNA sequencing is performed on Illumina machines.  If you need to refresh your memory about Illumina sequencing technology, please take a look at this video by Illumina.</p> <ul> <li>FASTQ format</li> </ul> <p>Change into the<code>raw_data</code> directory inside our main course directory <code>intro-to-RNA-seq</code>: <pre><code>cd intro-to-RNA-seq/raw_data\n</code></pre></p> <p>First, we'll take a look at our raw data. Since our data it gzip compressed, we'll use the command <code>zcat</code> to read the file without decompressing. We'll follow that command by the bash pip <code>|</code> and <code>head</code> in order to show only the first few lines</p> <pre><code>zcat WT/ERR458493.fastq.gz | head\n</code></pre> <p>The result that you see contains the first few FASTQ sequences. Each sequence has the following 4-line format: <pre><code>@ERR458493.1 DHKW5DQ1:219:D0PT7ACXX:1:1101:1724:2080/1       &lt;-- Sequence identifier: @Read ID\nCGCAAGACAAGGCCCAAACGAGAGATTGAGCCCAATCGGCAGTGTAGTGAA          &lt;-- Sequence\n+                                                            &lt;-- + (optionally lists the sequence identifier again)\nB@@FFFFFHHHGHJJJJJJIJJGIGIIIGI9DGGIIIEIGIIFHHGGHJIB          &lt;-- Quality String\n</code></pre></p> <ul> <li>Base Quality Scores</li> </ul> <p>The fourth line of each read is called the quality string. Each symbol in the string is an encoding of the quality score, representing the inferred base call accuracy at that position in the read. The manufacturer of the sequencing instrument has performed calibration of quality score by sequencing many well-characterized samples from multiple organisms  and studying the correspondence between properties of the signal generated by the cluster being sequenced and the accuracy of the resulting base call.</p> <p>The following two images explain this encoding. The first image shows the mapping of the encoded quality score to the quality score:</p> <p></p> <p>The second image shows the mapping of the quality score to the inferred base call accuracy:</p> <p></p> <p>Looking back at our sample read, we can see that the first base has an encoded quality score of <code>C</code>. Using the first image above, we see that C encodes a quality of 34. Using the second table, we see that the probability is &lt; 1/1000 of that base being an error. In the next section, we'll see how quality scores and other quality control metrics are used to evaluate the quality of a sequenced sample.</p> <p>More information on Quality scores from Illumina</p>"},{"location":"omics/intro-to-rnaseq/02_Quality_Control/#perform-quality-control-checks-using-fastqc-toolkit","title":"Perform quality control checks using FastQC toolkit","text":"<p>FastQC is widely used tool for both DNA and RNA sequencing data that is run on each fastq file.</p> <ul> <li> <p>To use FastQC on HPC, first load the module: <pre><code>module load fastqc/0.11.8\n</code></pre></p> </li> <li> <p>To see the input options, type: <pre><code>fastqc --help\n</code></pre></p> </li> </ul> <p>Result: <pre><code>fastqc --help\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n    fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam]\n[-c contaminant file] seqfile1 .. seqfileN\n...\n</code></pre></p> <p>FastQC is run on each FASTQ file separately in order to be sensitive to the variation in quality over lanes, samples, and paired-end files.</p> <ul> <li>To see an example of FASTQC output, we'll run on one file.</li> </ul> <p>We add extra arguments <code>-o fastqc</code> to specify that the output should be placed in the directory we created and <code>--extract</code> to indicate that the input files are gzip compressed.</p> <p><pre><code>cd ..\nmkdir fastqc\nfastqc raw_data/WT/ERR458493.fastq.gz -o fastqc --extract\n</code></pre> The extra argument <code>--extract</code> is used when the input files are gzip compressed.</p> <p>Note that FastQC can run on multiple files at once, using a wildcard <code>*</code> instead of the filename <code>ERR458493.fastq.gz</code> to indicate each file in the folder <code>raw_data</code>.</p> <p>Result: <pre><code>Started analysis of ERR458493.fastq.gz\nApprox 5% complete for ERR458493.fastq.gz\nApprox 10% complete for ERR458493.fastq.gz\nApprox 15% complete for ERR458493.fastq.gz\nApprox 20% complete for ERR458493.fastq.gz\nApprox 25% complete for ERR458493.fastq.gz\nApprox 30% complete for ERR458493.fastq.gz\nApprox 35% complete for ERR458493.fastq.gz\nApprox 40% complete for ERR458493.fastq.gz\nApprox 45% complete for ERR458493.fastq.gz\nApprox 50% complete for ERR458493.fastq.gz\nApprox 55% complete for ERR458493.fastq.gz\nApprox 60% complete for ERR458493.fastq.gz\nApprox 65% complete for ERR458493.fastq.gz\nApprox 70% complete for ERR458493.fastq.gz\nApprox 75% complete for ERR458493.fastq.gz\nApprox 80% complete for ERR458493.fastq.gz\nApprox 85% complete for ERR458493.fastq.gz\nApprox 90% complete for ERR458493.fastq.gz\nApprox 95% complete for ERR458493.fastq.gz\nAnalysis complete for ERR458493.fastq.gz\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/02_Quality_Control/#view-results-in-the-ondemand-browser","title":"View results in the OnDemand browser","text":"<p>Return to the tab ondemand.cluster.tufts.edu</p> <p>On the top menu bar choose <code>Files-&gt;Projects</code></p> <p></p> <p>A new tab will open listing all the project folders in <code>/cluster/tufts/</code>. Navigate to the <code>fastqc</code> folder in course directory, e.g.: <code>bio/tools/training/intro-to-rnaseq/users/username/intro-to-RNA-seq/fastqc/</code> Right click on the file <code>ERR458493_fastqc.html</code> and select <code>Open in new tab</code>.</p> <p></p> <p>The new tab that opens in the browser has the results of FastQC for the sample.</p>"},{"location":"omics/intro-to-rnaseq/02_Quality_Control/#understand-fastqc-report","title":"Understand FASTQC report","text":"<p>A video tutorial on understanding FASTQC report is strongly recommended and can be found on Babraham bioinformatics.</p> <ul> <li>Per base sequence quality</li> </ul> <p>Explanations adapted from https://dnacore.missouri.edu/PDF/FastQC_Manual.pdf.</p> <p>This view shows an overview of the range of quality values across all bases at each position in the FastQ file</p> <p></p> <p>For each position a BoxWhisker type plot is drawn. The elements of the plot are as follows: - The central red line is the median value - The yellow box represents the inter-quartile range (25-75%) - The upper and lower whiskers represent the 10% and 90% points - The blue line represents the mean quality</p> <p>The y-axis on the graph shows the quality scores. The higher the score the better the base call.  The background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red).  The quality of calls on most platforms will degrade as the run progresses, so it is common to see base calls falling into the orange area towards the end of a read.</p> <ul> <li>Per sequence quality scores</li> </ul> <p>The Per Sequence Quality Score plots the distribution of mean sequence quality. This plot allows will show a peak toward lower mean quality if there is a subset of sequences with  low quality values. </p> <p></p> <ul> <li>Per base sequence content</li> </ul> <p>The Per Base Sequence Content plot shows the the proportion of each base called at each position in the read,  for all reads in the file.</p> <p></p> <p>In a random library you would expect that there would be little to no difference between the different bases of a sequence run, so the lines in this plot should run parallel with each other.  In our results, you can clearly see the biased sequence in the first ~12 bases of the run.  This bias then dissipates over the rest of the run which shows the expected parallel tracks in the base content for each base. This happens in pretty much all RNA-Seq libraries to a greater or lesser extent.</p> <p>The cause of this bias is the random priming step in library production. The priming should be driven by a selection of random hexamers which in theory should all be present with equal frequency in the priming mix and should all prime with equal efficiency. In the real world it turns out that this isn\u2019t the case and that certain hexamers are favoured during the priming step, resulting in the based composition over the region of the library primed by the random primers.</p> <p>The biased selection though doesn\u2019t appear to be strong enough to cause major headaches in downstream quantitation of data. A strong bias would result in a very uneven coverage of different parts of a transcript based on its sequence content, and most RNA-Seq libraries do not show these types of localised biases (excepting biases from mappability and other factors beyond this effect). Also the biases are very similar between libraries, so any artifacts which were introduced should cancel out when doing any kind of differential analysis.</p> <p>This article has more details.</p> <ul> <li>Per sequence GC content</li> </ul> <p>This module measures the GC content across the whole length of each sequence in a file and compares it to a modelled normal distribution of GC content.</p> <p></p> <p>In a normal random library you would expect to see a roughly normal distribution of GC content where the central peak corresponds to the overall GC content of the underlying genome. Since we don't know the the GC content of the genome the modal GC content is calculated from the observed data and used to build a reference distribution. An unusually shaped distribution could indicate a contaminated library or some other kinds of biased subset. A normal distribution which is shifted indicates some systematic bias which is independent of base position. If there is a systematic bias which creates a shifted normal distribution then this won't be flagged as an error by the module since it doesn't know what your genome's GC content should be.</p> <ul> <li>Per base N content</li> </ul> <p>If a sequencer is unable to make a base call with sufficient confidence then it will normally substitute an N rather than a conventional base call .  This plot shows the percentage of base calls at each position for which an N was substituted.</p> <p></p> <ul> <li>Sequence Length Distribution</li> </ul> <p>This plot shows the distribution of read sizes in the file. Depending on the sequencing method and whether reads have been post-processed, it may be expected to have reads of a  uniform length or varying lengths. For our data we see a sharp peak at 51 basepairs.</p> <p></p> <ul> <li>Sequence Duplication Levels</li> </ul> <p>This plot shows the distribution of sequence duplicates in the file. We see that &lt;60% of Total sequences are present exactly 1 time, and &gt;10% are present twice. This level of duplication is expected for RNAseq, due to the vastly different levels of transcripts in the starting population. RNAseq libraries tend to have higher levels of duplication than DNA libraries, due to the presence of highly expressed transcripts. Deduplication is not recommended for RNAseq sequencing files.</p> <p></p> <ul> <li>Overrepresented sequences</li> </ul> <p>This plot shows the sequences in the file which make up more than 0.1% of the total. In our sample, no overrepresented sequences are found.</p> <p>A normal high-throughput library will contain a diverse set of sequences, with no individual sequence making up a more than a tiny fraction of the whole. Finding that a single sequence is very overrepresented in the set either means that it is highly biologically significant, or indicates that the library is contaminated, or not as diverse as you expected.</p> <p>For each overrepresented sequence the program will look for matches in a database of common contaminants and will report the best hit it finds. Hits must be at least 20bp in length and have no more than 1 mismatch. Finding a hit doesn't necessarily mean that this is the source of the contamination, but may point you in the right direction. It's also worth pointing out that many adapter sequences are very similar to each other so you may get a hit reported which isn't technically correct, but which has very similar sequence to the actual match.</p> <p>If overrepresented sequences are found but not identified by FastQC, try a  BLAST search.</p> <ul> <li>Adapter Content</li> </ul> <p>This module looks for common adapters in the sequence. In this example, there are no adapters found.</p> <p></p> <p>Explanations adapted from tthe FastQC Manual</p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/","title":"Read Alignment","text":""},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#read-alignment","title":"Read alignment","text":"<p>Approximate time: 60 minutes</p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#goals","title":"Goals","text":"<ul> <li>Align short reads (one sample at a time) to a references genome using Spliced Transcripts Alignment to a Reference (STAR) aligner</li> <li>Review Gene annotation and GTF format</li> <li>View alignment using Integrative Genome Viewer (IGV)</li> </ul>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#star-aligner","title":"STAR Aligner","text":"<p>The alignment process consists of choosing an appropriate reference genome to map our reads against and performing the read alignment using one of several splice-aware alignment tools such as STAR or HISAT2. The choice of aligner is often a personal preference and also dependent on the computational resources that are available to you.</p> <p>STAR is an aligner designed to specifically address many of the challenges of RNA-seq data mapping using a strategy to account for spliced alignments. STAR is shown to have high accuracy and outperforms other aligners by more than a factor of 50 in mapping speed, but it is memory intensive.</p> <p>STAR algorithm consists of two major steps: - seed searching step: Find Maximum Mappable Prefixes (MMP) in a read. MMP can be extended by a. mismatches b. indels or c. soft-clipping - clustering/stitching/scoring step: determine finl read location and deal with a large number of mismatches, indels and splice junctions, as well as scalable with the read length.</p> <p></p> <p>More information can be found on the publication: \"STAR: ultrafast universal RNA-seq aligner\".</p> <p>Aligning reads using STAR is a two step process: - Create a genome index - Map reads to the genome</p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#step-1-build-star-index-for-reference-genome","title":"Step 1. Build STAR Index for reference genome.","text":"<p>If using the same reference, the index step only needs to be done once.</p> <p>Tufts HPC hosts genome reference data from UCSC at the following location <pre><code>/cluster/tufts/bio/data/genomes\n</code></pre> We will need reference files from Saccharomyces_cerevisiae genome version sacCer3, stored here: <pre><code>/cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/\n</code></pre></p> <p>In that directory there are both genome sequence, genome indicies for various aligners, and some annotation data.</p> <p>We'll re-create the STAR genome index in our own directory in order to practice:</p> <ul> <li>Get an interaction session on a compute node if you haven't done so</li> </ul> <p><code>srun --pty -t 3:00:00  --mem 16G  -N 1 -n 4 bash</code></p> <ul> <li>load the module</li> </ul> <p><code>module load STAR/2.7.0a</code></p> <ul> <li>Create a directory to store the index (directory should be created in the top level of the course directory <code>intro-to-RNA-seq</code>:</li> </ul> <pre><code>mkdir genome\n</code></pre> <ul> <li>You can take a peak at the first 10 lines of the file <code>genome.fa</code> using the <code>head</code> command <pre><code>head /cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/Sequence/WholeGenomeFasta/genome.fa\n</code></pre></li> </ul> <p>Result below shows an example of FASTA file. Arrows on the right explain format: <pre><code>&gt;chrI                                                          &lt;-- '&gt;' charachter followed by sequence name\nCCACACCACACCCACACACCCACACACCACACCACACACCACACCACACC\nCACACACACACATCCTAACACTACCCTAACACAGCCCTAATCTAACCCTG\nGCCAACCTGTCTCTCAACTTACCCTCCATTACCCTGCCTCCACTCGTTAC\nCCTGTCCCATTCAACCATACCACTCCGAACCACCATCCATCCCTCTACTT\nACTACCACTCACCCACCGTTACCCTCCAATTACCCATATCCAACCCACTG             &lt;-- sequence\n\u2026\n</code></pre></p> <ul> <li>Run STAR index in \"genomeGenerate\" mode <pre><code>STAR --runMode genomeGenerate --genomeDir ./genome --genomeFastaFiles /cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/Sequence/WholeGenomeFasta/genome.fa --runThreadN 4\n</code></pre></li> </ul> <p>The STAR program will start running and show the process as below: <pre><code>Apr 18 17:45:35 ..... started STAR run\nApr 18 17:45:36 ... starting to generate Genome files\nApr 18 17:45:37 ... starting to sort Suffix Array. This may take a long time...\nApr 18 17:45:37 ... sorting Suffix Array chunks and saving them to disk...\nApr 18 17:45:39 ... loading chunks from disk, packing SA...\nApr 18 17:45:39 ... finished generating suffix array\nApr 18 17:45:39 ... generating Suffix Array index\nApr 18 17:45:44 ... completed Suffix Array index\nApr 18 17:45:44 ... writing Genome to disk ...\nApr 18 17:45:44 ... writing Suffix Array to disk ...\nApr 18 17:45:45 ... writing SAindex to disk\nApr 18 17:45:46 ..... finished successfully\n</code></pre></p> <p>When it's done, take a look at the files produced by typing <code>ls genome</code>: <pre><code>-rw-rw---- 1 whuo01 isberg  107 Apr 18 17:45 chrName.txt\n-rw-rw---- 1 whuo01 isberg  122 Apr 18 17:45 chrLength.txt\n-rw-rw---- 1 whuo01 isberg  142 Apr 18 17:45 chrStart.txt\n-rw-rw---- 1 whuo01 isberg  229 Apr 18 17:45 chrNameLength.txt\n-rw-rw---- 1 whuo01 isberg  660 Apr 18 17:45 genomeParameters.txt\n-rw-rw---- 1 whuo01 isberg  14M Apr 18 17:45 Genome\n-rw-rw---- 1 whuo01 isberg  96M Apr 18 17:45 SA\n-rw-rw---- 1 whuo01 isberg 1.5G Apr 18 17:45 SAindex\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#step-2-find-gene-annotation-file","title":"Step 2. Find gene annotation file.","text":"<p>STAR can use an annotation file gives the location and structure of genes in order to improve alignment in known splice junctions. Annotation is dynamic and there are at least three major sources of annotation: RefGene, Ensembl, and UCSC.</p> <p>The intersection among the three sources is shown in the figure below. RefGene has the fewest unique genes, while more than 50% of genes in Ensembl are unique</p> <p></p> <p>(Figure references Zhao et al Bioinformatics 2015)</p> <p>It is important to be consistent with your choice of annotation source throughout an analysis.</p> <p>Tufts HPC hosts genome reference data from various sources. The annotation information for <code>sacCer3</code> from <code>UCSCC</code> source can be found at the following location: <pre><code>/cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/\n</code></pre></p> <p>Gene Transfer Format (GTF) is the most widely used file format to store information about gene location and structure with respect to a given reference genome.</p> <p>We will use the GTF file located here <pre><code>/cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/Annotation/Genes\n</code></pre></p> <p>An example of the format is shown below, where header line is added for clarity.</p> SequenceName Source Feature Start End Score Strand Frame Group chrI sacCer3.genepred transcript 335 649 . + . gene_id \"YAL069W\"; transcript_id \"YAL069W\"; chrI sacCer3.genepred exon 335 649 . + . gene_id \"YAL069W\"; transcript_id \"YAL069W\"; exon_number \"1\"; exon_id \"YAL069W.1\"; chrI sacCer3.genepred CDS 335 646 . + 0 gene_id \"YAL069W\"; transcript_id \"YAL069W\"; exon_number \"1\"; exon_id \"YAL069W.1\"; chrI sacCer3.genepred start_codon 335 337 . + 0 gene_id \"YAL069W\"; transcript_id \"YAL069W\"; exon_number \"1\"; exon_id \"YAL069W.1\"; chrI sacCer3.genepred stop_codon 647 649 . + 0 gene_id \"YAL069W\"; transcript_id \"YAL069W\"; exon_number \"1\"; exon_id \"YAL069W.1\"; <p>The table shows the location of all types of features in a Gene (transcript, exon, CDS, start_codon, and stop codons) It also uses the last four columns to provide annotation about the transcript and gene identifiers.</p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#step-3-align","title":"Step 3. Align.","text":"<p>Let's first check the usage instructions for STAR by typing <code>STAR</code></p> <pre><code>Usage: STAR  [options]... --genomeDir REFERENCE   --readFilesIn R1.fq R2.fq\nSpliced Transcripts Alignment to a Reference (c) Alexander Dobin, 2009-2015\n\n### versions\nversionSTAR             020201\n    int&gt;0: STAR release numeric ID. Please do not change this value!\nversionGenome           020101 020200\n    int&gt;0: oldest value of the Genome version compatible with this STAR release. Please do not change this value!\n\n### Parameter Files\nparametersFiles          -\n    string: name of a user-defined parameters file, \"-\": none. Can only be defined on the command line.\n\n### System\nsysShell            -\n    string: path to the shell binary, preferably bash, e.g. /bin/bash.\n                    - ... the default shell is executed, typically /bin/sh. This was reported to fail on some Ubuntu systems - then you need to specify path to bash.\n\n### Run Parameters\nrunMode                         alignReads\n...\n</code></pre> <p>Since our alignment command will have multiple arguments, it will be convenient to write a script.</p> <p>Make a new directory for our results <pre><code>mkdir STAR\n</code></pre></p> <p>Open the script ./scripts/star_align_practice.sh in a text editor, for example <code>nano scripts/star_align_practice.sh</code>:</p> <p><pre><code>## Load STAR aligner\nmodule load STAR/2.7.0a\nmkdir -p STAR\n\n## Assign the fastq file with its location. Extension with .fastq, .fq and .fastq.gz work the same\nFASTQ=\"raw_data/WT/ERR458493.fastq.gz\"\n\n## If you have multiple fastq files for one same sample, you can compile them together by typing below\n#FASTQ=\"reads1.fastq, reads2.fastq\"\n\n## Name the output file\nOUT=\"WT_ERR458493\"\n\n## Defing reference genome directory\nREF_DIR=\"/cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3\"\n\n## execute STAR in the runMode \"alignReads\"\nSTAR --genomeDir ${REF_DIR}/Sequence/STAR \\\n--readFilesIn ${FASTQ} \\\n--readFilesCommand zcat \\\n--outFileNamePrefix STAR/${OUT}_ \\\n--outFilterMultimapNmax 1 \\\n--outSAMtype BAM SortedByCoordinate \\\n--runThreadN 4 \\\n--alignIntronMin 1 \\\n--alignIntronMax 2500 \\\n--sjdbGTFfile ${REF_DIR}/Annotation/Genes/sacCer3.gtf \\\n--sjdbOverhang 49\n</code></pre> We have defined the following variables for convenience: - <code>FASTQ</code> to store the comma-separated list of names and locations of all input files for this sample - <code>OUT</code> to store the name of our output files - <code>REF_DIR</code> in order to give the location of the reference data.</p> <p>We've given the following arguments to <code>STAR</code>: 1. <code>--outFileNamePrefix</code>: specify output folder and prefix of the names 2. <code>--outFilterMultimapNmax 1</code>: max number of multiple alignments allowed for a read: if exceeded, the read is considered unmapped. Here it is set to 1. 3. <code>--outSAMtype BAM SortedByCoordinate</code>:  output sorted by coordinate Aligned.sortedByCoord.out.bam file, similar to samtools sort command 4. <code>--runThreadsN 4</code>: STAR runs four parallel threads. Alignment is a task that is easy to parallelize because alignment of a read is independent of other reads. 5. <code>--alignIntronMin/Max</code>: Minimal and Maximal intron length 6. <code>--sjdbGTFfile\"</code>: GTF annotation file for the gene expression calculation 7. <code>--sjdbOverhang</code>:  specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads. For instance, for Illumina 2x100b paired-end reads, the ideal value is 100-1=99. In case of reads of varying length, the ideal value is max(ReadLength)-1. In most cases, the default value of 100 will work as well as the ideal value.</p> <p>Exit nano by typing <code>^X</code>.</p> <p>Now we can run our script using sh. <pre><code>sh ./scripts/star_align_practice.sh\n</code></pre> Result: <pre><code>(base) [whuo01@m4lmem01 intro-to-RNA-seq]$ sh ./scripts/star_align_practice.sh\nMay 21 13:11:07 ..... started STAR run\nMay 21 13:11:07 ..... loading genome\nMay 21 13:11:09 ..... processing annotations GTF\nMay 21 13:11:09 ..... inserting junctions into the genome indices\nMay 21 13:11:17 ..... started mapping\nMay 21 13:11:28 ..... started sorting BAM\nMay 21 13:11:29 ..... finished successfully\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#step-4-view-results","title":"Step 4. View results.","text":"<p>View result by typing in: <code>ls -lh STAR/</code> <pre><code>(base) [whuo01@m4lmem01 intro-to-RNA-seq]$ ls -lh STAR/\ntotal 46M\n-rw-rw---- 1 whuo01 isberg  46M May 21 13:11 WT_ERR458493_Aligned.sortedByCoord.out.bam\n-rw-rw---- 1 whuo01 isberg 1.8K May 21 13:11 WT_ERR458493_Log.final.out\n-rw-rw---- 1 whuo01 isberg  21K May 21 13:11 WT_ERR458493_Log.out\n-rw-rw---- 1 whuo01 isberg  246 May 21 13:11 WT_ERR458493_Log.progress.out\n-rw-rw---- 1 whuo01 isberg  14K May 21 13:11 WT_ERR458493_SJ.out.tab\ndrwx--S--- 2 whuo01 isberg 4.0K May 21 13:11 WT_ERR458493__STARgenome\n</code></pre> The file <code>WT_ERR458493_Log.final.out</code> will give us a summary of the run. Take a look at the summary by running: <code>cat STAR/WT_ERR458493_Log.final.out</code></p> <pre><code>(base) [whuo01@m4lmem01 intro-to-RNA-seq]$ cat STAR/WT_ERR458493_Log.final.out\n                                 Started job on |       May 21 13:11:07\n                             Started mapping on |       May 21 13:11:17\n                                    Finished on |       May 21 13:11:29\n       Mapping speed, Million of reads per hour |       328.19\n\n                          Number of input reads |       1093957\n                      Average input read length |       51\n                                    UNIQUE READS:\n                   Uniquely mapped reads number |       938174\n                        Uniquely mapped reads % |       85.76%\n                          Average mapped length |       50.75\n                       Number of splices: Total |       8681\n            Number of splices: Annotated (sjdb) |       7530\n                       Number of splices: GT/AG |       8002\n                       Number of splices: GC/AG |       9\n                       Number of splices: AT/AC |       0\n               Number of splices: Non-canonical |       670\n                      Mismatch rate per base, % |       0.37%\n                         Deletion rate per base |       0.00%\n                        Deletion average length |       0.00\n                        Insertion rate per base |       0.00%\n                       Insertion average length |       1.05\n                             MULTI-MAPPING READS:\n        Number of reads mapped to multiple loci |       0\n             % of reads mapped to multiple loci |       0.00%\n        Number of reads mapped to too many loci |       123562\n             % of reads mapped to too many loci |       11.29%\n                                  UNMAPPED READS:\n       % of reads unmapped: too many mismatches |       0.00%\n                 % of reads unmapped: too short |       2.91%\n                     % of reads unmapped: other |       0.04%\n                                  CHIMERIC READS:\n                       Number of chimeric reads |       0\n                            % of chimeric reads |       0.00%                                \n</code></pre> <p>For well annotated genomes, it's expected that &gt;75% of the reads to be uniquely mapped and that most splice junctions are annotated. Further QC options are available with <code>RSEQC</code> and <code>samtools</code> packages (see scripts/bamqc.sh).</p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#step-5-create-index-for-bam-file","title":"Step 5. Create index for BAM file.","text":"<ul> <li>Bam format</li> </ul> <p>The BAM file is a binary compressed version of a Sequence Alignment Map (SAM) file.</p> <p></p> <p>Take a look at the output file: <pre><code>module load samtools/1.9\nsamtools view -h STAR/WT_ERR458493_Aligned.sortedByCoord.out.bam | less\n</code></pre> Press <code>space</code> to scroll down to the file, and press <code>q</code> to exit viewing the file. The file has two sections:</p> <p>Header: <pre><code>@HD VN:1.4 SO:coordinate          &lt;-- Format version (VN) and Sorting order of alignments (SO)\n@SQ SN:chrI LN:230218             &lt;-- Reference sequence name (SN) and length (LN)\n...\n</code></pre></p> <p>Alignment: <pre><code>ERR458493.243111  0 chrI 3873 255 51M * 0 0 TGAAAATATTCTGAGGTAAAAGCCATTAAGGTCCAGATAACCAAGGGACAA     &lt;-- Template name, FLAG, reference name, mapping position (start), mapping quality, CIGAR string, reference name of the paired read, position of the paired read, template length, read sequence\n...\n</code></pre> FLAG: The FLAG field is displayed as a single integer, but is the sum of bitwise flags to denote multiple attributes of a read alignment. For example, 16 means the read being reverse complemented. CIGAR: Concise Idiosyncratic Gapped Alignment Report (CIGAR) string. For example: M represents an alignment match</p> <p>More information on BAM format: samtools on github and wikipedia: SAM_(file_format).</p> <ul> <li>Create index for BAM file</li> </ul> <p>In order to visualize our BAM file in IGV (or any other visualization tool, such as Geneious) we will need a BAM index. This enables fast searching and display.</p> <p>We'll generate one using <code>samtools</code>.</p> <pre><code>module load samtools/1.9\nsamtools index STAR/WT_ERR458493_Aligned.sortedByCoord.out.bam\n</code></pre> <p>The result is a file with the extension <code>bai</code> in the same folder as our BAM file: <pre><code>WT_ERR458493_Aligned.sortedByCoord.out.bam.bai\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#visualizing-reads-using-igv","title":"Visualizing reads using IGV","text":"<ul> <li> <p>Return to On Demand Dashboard tab: <code>https://ondemand.cluster.tufts.edu</code></p> </li> <li> <p>On the top grey menu bar, choose <code>Interactive Apps-&gt;IGV</code></p> </li> <li> <p>Set the following parameters: <pre><code>hours: 1\ncores: 4\nmemory: 64 Gb\ndirectory: &lt; leave default &gt;`\n</code></pre></p> </li> <li> <p>Click: <code>Launch</code></p> </li> <li> <p>Click: <code>Launch noVNC in New Tab</code> when it appears.</p> </li> </ul> <p></p> <ul> <li>If the genome browser is cut off, resize using Chrome:</li> </ul> <p></p> <ul> <li>Enable RNA-seq-specific Splice Junction track by making the following selections in the IGV menu:</li> <li><code>View -&gt; Preferences</code></li> </ul> <p></p> <p>2.<code>Alignments -&gt; Track Display Options -&gt; Splice Junction Track -&gt; OK</code></p> <p></p> <ul> <li> <p>Choose reference genome by clicking the <code>Genomes</code> menu and selecting <code>Load Genome from Server...</code></p> </li> <li> <p>Scroll down to <code>Sacromyces ceerevicea (sacCer3)</code> -&gt; leave <code>Download Sequence</code> UNchecked -&gt; click <code>OK</code></p> </li> </ul> <p></p> <ul> <li>Load BAM file:</li> </ul> <p>Click <code>File-&gt; Load from File</code> Choose the sorted and indexed BAM files we generated: <code>/cluster/tufts/bio/tools/training/users/YOUR_USERNAME/intro-to-RNA-seq/STAR/WT_ERR458493_Aligned.sortedByCoord.out.bam</code></p> <p></p> <p>In the genome coordinate box (shown below) type the gene name <code>SUS1</code>. We can see that another name of this gene is <code>YBR111W-A</code>.</p> <p>Here is a summary of the fields and tracks present in IGV:</p> <p></p> <p>If we zoom in on the <code>YBR111W-A</code> gene, we see in the <code>Gene</code> track at the bottom that the gene contains two introns. We see in the <code>BAM</code> track that reads are spiced across the introns and that coverage track that read coverage in the area of the intron is missing as expected.  </p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#process-all-samples-align-all-reads-from-two-conditions-wt-and-snf2","title":"Process all samples: Align all reads from two conditions (WT and SNF2)","text":"<p>In the previous steps, we learned how to do quality control and read alignment using one WT fastq file as an example. Here, before we continue to the next step, we wanted to do the same procedure for all samples in both WT and SNF2 conditions so that we can performed the differential expression analysis. You can do so manually by creating a new version of the script <code>./scripts/star_align_practice.sh</code> for each sample.</p> <p>This can be done by creating a copy of the script, e.g.: <pre><code>cp ./scripts/star_align_practice.sh ./scripts/star_align_ERR458500.sh\n</code></pre> Then, change the name of the input <code>FASTQ</code> and output <code>OUT</code> to match the sample you are aligning, e.g. by using <code>nano ./scripts/star_align_ERR458500.sh</code> to create the modified lines: <pre><code>## Fastq files to align, separated by commas for multiple lanes of a single sample\nFASTQ=\"raw_data/SNF2/ERR458500.fastq.gz\"\n\n## Name the output file\nOUT=\"SNF2_ERR458500\"\n</code></pre> Exit by pressing Crtl and X and save the file by entering Y when prompt. Finally, run <code>sh ./scripts/star_align_ERR458500.sh</code>. Eventually, we want to align 7 WT samples and 7 SNF2 samples individually and generate 14 bam files in total.</p> <p>We have also prepared a script that will align all reads individually in a automatic manner.</p> <p>In order to use our pre-written scripts, first make sure you have an interaction session on a compute node by typing: <code>srun --pty -t 3:00:00  --mem 16G  -N 1 -n 4 bash</code></p> <p>Note: If wait times are very long, you can try a different partitions by adding, e.g. <code>-p interactive</code> before bash.</p> <p>After you get an interactive session, run the following commands: <pre><code>sh ./scripts/star_align_individual.sh\n</code></pre></p> <p>This step will automatically align individual fastq files to reference and use samtools to create indexes. This step will take about 15-30min to finish.</p> <p>After the alignment is finished, type in <code>tree ./STAR</code>  and you will see the aligned reads in STAR folder:  <code>[whuo01@pcomp45 intro-to-RNA-seq]$ tree STAR                       &lt;--command STAR                                                                &lt;--folder name: STAR \u251c\u2500\u2500 SNF2_ERR458500_Aligned.sortedByCoord.out.bam                    &lt;--Aligned bam file for SNF2 sample ERR458500 \u251c\u2500\u2500 SNF2_ERR458500_Aligned.sortedByCoord.out.bam.bai                &lt;--Indexed bam.bai file \u251c\u2500\u2500 SNF2_ERR458500_Log.final.out                                    &lt;--Alignment statistics \u251c\u2500\u2500 SNF2_ERR458500_Log.out                                          &lt;--STAR alignment setting \u251c\u2500\u2500 SNF2_ERR458500_Log.progress.out \u251c\u2500\u2500 SNF2_ERR458500_SJ.out.tab \u251c\u2500\u2500 SNF2_ERR458500__STARgenome \u2502   \u251c\u2500\u2500 exonGeTrInfo.tab \u2502   \u251c\u2500\u2500 exonInfo.tab \u2502   \u251c\u2500\u2500 geneInfo.tab \u2502   \u251c\u2500\u2500 sjdbInfo.txt \u2502   \u251c\u2500\u2500 sjdbList.fromGTF.out.tab \u2502   \u251c\u2500\u2500 sjdbList.out.tab \u2502   \u2514\u2500\u2500 transcriptInfo.tab ... \u251c\u2500\u2500 WT_ERR458494_Aligned.sortedByCoord.out.bam                    &lt;--Aligned bam file for WT sample ERR458494 \u251c\u2500\u2500 WT_ERR458494_Aligned.sortedByCoord.out.bam.bai                &lt;--Indexed bam.bai file \u251c\u2500\u2500 WT_ERR458494_Log.final.out \u251c\u2500\u2500 WT_ERR458494_Log.out \u251c\u2500\u2500 WT_ERR458494_Log.progress.out \u251c\u2500\u2500 WT_ERR458494_SJ.out.tab \u251c\u2500\u2500 WT_ERR458494__STARgenome \u2502   \u251c\u2500\u2500 exonGeTrInfo.tab \u2502   \u251c\u2500\u2500 exonInfo.tab \u2502   \u251c\u2500\u2500 geneInfo.tab \u2502   \u251c\u2500\u2500 sjdbInfo.txt \u2502   \u251c\u2500\u2500 sjdbList.fromGTF.out.tab \u2502   \u251c\u2500\u2500 sjdbList.out.tab \u2502   \u2514\u2500\u2500 transcriptInfo.tab ... <pre><code>## Optional step. Visualize number of mapped reads v.s. unmapped reads in all samples using barplot\nTo visualize the result, type:\n</code></pre> module load R/3.5.0 Rscript ./scripts/mapping_percentage.R <pre><code>If ran successfully, you will see the message below:\n</code></pre> [1] \"Processing file:  SNF2_ERR458500_Log.final.out\" [1] \"Processing file:  SNF2_ERR458501_Log.final.out\" [1] \"Processing file:  SNF2_ERR458502_Log.final.out\" [1] \"Processing file:  SNF2_ERR458503_Log.final.out\" [1] \"Processing file:  SNF2_ERR458504_Log.final.out\" [1] \"Processing file:  SNF2_ERR458505_Log.final.out\" [1] \"Processing file:  SNF2_ERR458506_Log.final.out\" [1] \"Processing file:  WT_ERR458493_Log.final.out\" [1] \"Processing file:  WT_ERR458494_Log.final.out\" [1] \"Processing file:  WT_ERR458495_Log.final.out\" [1] \"Processing file:  WT_ERR458496_Log.final.out\" [1] \"Processing file:  WT_ERR458497_Log.final.out\" [1] \"Processing file:  WT_ERR458498_Log.final.out\" [1] \"Processing file:  WT_ERR458499_Log.final.out\" null device           1</code> This code will generate a pdf file named <code>Mapping_stat.pdf</code>.</p> <p></p> <p>Now you are ready for the next step.</p>"},{"location":"omics/intro-to-rnaseq/03_Read_Alignment/#summary","title":"Summary","text":""},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/","title":"Gene Quantification","text":""},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#gene-quantification","title":"Gene quantification","text":"<p>Approximate time: 20 minutes</p>"},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Use the <code>featureCounts</code> function from the <code>Subread</code> package to perform feature quantification</li> </ul>"},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#introduction-to-featurecounts","title":"Introduction to featureCounts","text":"<p>The mapped coordinates of each read are compared with the features in the GTF file. Reads that overlap with a gene by &gt;=1 bp are counted as belonging to that feature. Ambiguous reads will be discarded and the output will be a matrix of genes and samples.</p> <p></p> <p>By default featurecounts will 1) count reads in features labeled as 'exon' in the GTF and 2) group all exons with a given 'gene_id'.</p> <p>An example of a transcript with multiple exons:</p> <p></p>"},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#step-1-write-and-run-the-script","title":"Step 1: write and run the script","text":"<p>Get an interaction session on a compute node by typing: <code>srun --pty -t 3:00:00  --mem 16G  -N 1 -n 4 bash</code></p> <p>Create a new script called <code>featureccounts.sh</code> using the <code>nano</code> text editor <code>nano featurecounts.sh</code> and enter the following content:</p> <pre><code>## load subread module\nmodule load subread/1.6.3\n\n## create output directory\nmkdir -p featurecounts\n\n## reference directory\nREF_DIR=/cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3\n\n## Run featurecounts\nfeatureCounts \\\n-a ${REF_DIR}/Annotation/Genes/sacCer3.gtf \\\n-o featurecounts/featurecounts_results.txt \\\nSTAR/*bam\n</code></pre> <p>Exit editor by typing <code>Control and X</code> and <code>Y</code> to save.</p> <p>To run the script, type in: <pre><code>sh featurecounts.sh\n</code></pre> If your script failed to run, try to run our prepared scripts by typing <code>sh scripts/featurecounts.sh</code>.  </p> <p>Result: <pre><code>(base) [whuo01@pcomp45 intro-to-RNA-seq]$ sh ./scripts/featurecounts.sh\n\n        ==========     _____ _    _ ____  _____  ______          _____  \n        =====         / ____| |  | |  _ \\|  __ \\|  ____|   /\\   |  __ \\\n          =====      | (___ | |  | | |_) | |__) | |__     /  \\  | |  | |\n            ====      \\___ \\| |  | |  _ &lt;|  _  /|  __|   / /\\ \\ | |  | |\n              ====    ____) | |__| | |_) | | \\ \\| |____ / ____ \\| |__| |\n        ==========   |_____/ \\____/|____/|_|  \\_\\______/_/    \\_\\_____/\n          v1.6.3\n\n//========================== featureCounts setting ===========================\\\\\n||                                                                            ||\n||             Input files : 14 BAM files                                     ||\n||                           S SNF2_ERR458500_Aligned.sortedByCoord.out.bam   ||\n||                           S SNF2_ERR458501_Aligned.sortedByCoord.out.bam   ||\n||                           S SNF2_ERR458502_Aligned.sortedByCoord.out.bam   ||\n||                           S SNF2_ERR458503_Aligned.sortedByCoord.out.bam   ||\n||                           S SNF2_ERR458504_Aligned.sortedByCoord.out.bam   ||\n||                           S SNF2_ERR458505_Aligned.sortedByCoord.out.bam   ||\n||                           S SNF2_ERR458506_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458493_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458494_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458495_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458496_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458497_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458498_Aligned.sortedByCoord.out.bam   ||\n||                           S WT_ERR458499_Aligned.sortedByCoord.out.bam   ||\n||                                                                            ||\n||             Output file : featurecounts_results.txt                        ||\n||                 Summary : featurecounts_results.txt.summary                ||\n||              Annotation : sacCer3.gtf (GTF)                                ||\n||      Dir for temp files : featurecounts                                    ||\n||                                                                            ||\n||                 Threads : 1                                                ||\n||                   Level : meta-feature level                               ||\n||              Paired-end : no                                               ||\n||      Multimapping reads : not counted                                      ||\n|| Multi-overlapping reads : not counted                                      ||\n||   Min overlapping bases : 1                                                ||\n||                                                                            ||\n\\\\===================== http://subread.sourceforge.net/ ======================//\n\n//================================= Running ==================================\\\\\n||                                                                            ||\n|| Load annotation file sacCer3.gtf ...                                       ||\n||    Features : 7050                                                         ||\n||    Meta-features : 6692                                                    ||\n||    Chromosomes/contigs : 17                                                ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458500_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1688319                                              ||\n||    Successfully assigned alignments : 1480922 (87.7%)                      ||\n||    Running time : 0.03 minutes                                             ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458501_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1674380                                              ||\n||    Successfully assigned alignments : 1468599 (87.7%)                      ||\n||    Running time : 0.03 minutes                                             ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458502_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1659719                                              ||\n||    Successfully assigned alignments : 1456027 (87.7%)                      ||\n||    Running time : 0.03 minutes                                             ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458503_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1519066                                              ||\n||    Successfully assigned alignments : 1331784 (87.7%)                      ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458504_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1311306                                              ||\n||    Successfully assigned alignments : 1150143 (87.7%)                      ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458505_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1313563                                              ||\n||    Successfully assigned alignments : 1151917 (87.7%)                      ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Process BAM file SNF2_ERR458506_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 1688061                                              ||\n||    Successfully assigned alignments : 1481499 (87.8%)                      ||\n||    Running time : 0.03 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458493_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 938174                                               ||\n||    Successfully assigned alignments : 842074 (89.8%)                       ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458494_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 924491                                               ||\n||    Successfully assigned alignments : 829144 (89.7%)                       ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458495_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 914201                                               ||\n||    Successfully assigned alignments : 820229 (89.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458496_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 844737                                               ||\n||    Successfully assigned alignments : 757537 (89.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458497_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 725805                                               ||\n||    Successfully assigned alignments : 651026 (89.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458498_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 730269                                               ||\n||    Successfully assigned alignments : 654642 (89.6%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file WT_ERR458499_Aligned.sortedByCoord.out.bam...           ||\n||    Single-end reads are included.                                          ||\n||    Assign alignments to features...                                        ||\n||    Total alignments : 937026                                               ||\n||    Successfully assigned alignments : 840493 (89.7%)                       ||\n||    Running time : 0.02 minutes                                             ||\n||                                                                            ||\n||                                                                            ||\n|| Summary of counting results can be found in file \"featurecounts/featureco  ||\n|| unts_results.txt.summary\"                                                  ||\n||                                                                            ||\n\\\\===================== http://subread.sourceforge.net/ ======================//\n</code></pre></p> <p>The output files will contain results and results summary. <pre><code>featurecounts/\n\u251c\u2500\u2500 featurecounts_results.txt\n\u2514\u2500\u2500 featurecounts_results.txt.summary\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#step-2-view-the-summary-statistics","title":"Step 2: View the summary statistics","text":"<p>To look at the summary statistics, type: <pre><code>cat featurecounts/featurecounts_results.txt.summary\n</code></pre></p> <p>Result:</p> <p></p> <p>The top line is the column names for this table. They are the name of each individule aligned file. The left most line is the row names, or the statistics of the alignment. From the table, we see that most of the reads fall within \"assigned\" features, meaning most of the aligned reads were transformed into gene expression value.</p>"},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#optional-step-visualize-number-of-assigned-reads-in-all-samples-using-barplot","title":"Optional step. Visualize number of assigned reads in all samples using barplot.","text":"<p>To visualize the result, type: <pre><code>module load R/3.5.0\nRscript ./scripts/featurecount_stat.R\n</code></pre> If ran successfully, you will see the message below: <pre><code>null device\n          1\n</code></pre> This code will generate a pdf file named <code>Featurecount_stat.pdf</code>.</p> <p></p>"},{"location":"omics/intro-to-rnaseq/04_Gene_Quantification/#step-3-view-the-feature-count-result","title":"Step 3: View the feature count result","text":"<p>To take a look at the calculated feature count, type: <pre><code>head featurecounts/featurecounts_results.txt\n</code></pre> Here, we are only looking at the top 10 lines in the featurecounts_results.txt file using <code>head</code> command. Result:</p> <p></p> <p>The very top line started with # shows the command that was used to run featurecount. Starting on the second line, it shows a table with sample names as column names and transcript names as row names. Each cell within the table shows how many reads from a sample were assigned to that transcript.</p> <p>You will notice the sample names are too long and contained unwanted strings. In order clean the file further for the DESeq2 analysis, we run the below command: <pre><code>cat featurecounts/featurecounts_results.txt |sed \"2s/STAR\\///g\" | sed \"2s/\\_Aligned.sortedByCoord.out.bam//g\" &gt; featurecounts/featurecounts_results.mod.txt\n</code></pre> This line of command will modify the column names to simplified names. Now you are ready for the next step.</p>"},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/","title":"Differential Expression","text":""},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/#differential-expression-using-rstudio","title":"Differential Expression using Rstudio","text":"<p>Approximate time: 60 minutes</p>"},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Use R to perform differential expression analysis</li> </ul>"},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/#step-1-setup-rstudio-on-the-tufts-hpc-cluster-via-on-demand","title":"Step 1. Setup Rstudio on the Tufts HPC cluster via \"On Demand\"","text":"<ol> <li>Open a Chrome browser and visit ondemand.cluster.tufts.edu</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose <code>Interactive Apps -&gt; Rstudio</code></li> </ol> <ol> <li>Choose:</li> </ol> <pre><code>Number of hours  : 4\nNumber of cores  : 1\nAmount of Memory : 32 Gb\nR version        : 3.5.0\nPartition        : Default\nReservation      : Default\n</code></pre> <p>Note if you are registered for the workshop, you may instead choose the following options in order to take advantage of  a reservation that will be available for one week after the workshop start date: <pre><code>Partition        : Preempt\nReservation      : Bioworkshop\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/#step-2-working-with-rstudio-interface","title":"Step 2. Working with Rstudio Interface","text":"<p>Go to the File menu -&gt; New File -&gt; R Script, you should see:</p> <p></p> <p>To save current session, click: File menu -&gt; Save your file as intro.R</p> <ul> <li>Set up working directory and library</li> </ul> <p>To run the code in the script editor, select a single line of code or highlight a block of code and click \"Run\". To see your current working directory: <pre><code>getwd()\n</code></pre> To change to our workshop directory: <pre><code>setwd('/cluster/tufts/bio/tools/training/intro-to-rnaseq/users/YOUR_USERNAME/into-to-RNA-seq/')\n</code></pre></p> <p>Check which paths on the cluster R will use to find library locations: <pre><code>.libPaths()\n</code></pre> Since we will be using a lot of R libraries today for differential expression analysis, instead of installing these libraries, you can use common library from Tufts bio tools by running: <pre><code>.libPaths('/cluster/tufts/bio/tools/R_libs/3.5')\n</code></pre> Now, you will be able to use all the libraries needed for this course. To load a library, type: <pre><code>library(tidyverse)\n</code></pre></p> <ul> <li>Read in Data</li> </ul> <p>To read in the metadata for our experiment: <pre><code>meta &lt;- read.table(\"./raw_data/sample_info.txt\", header=TRUE)\n</code></pre></p> <p>You can view the data by typing <code>meta</code> or <code>view(meta)</code></p> <p></p> <p>Load preprocessed data set of 7 WT replicates and 7 SNF2 knockouts <pre><code>feature_count &lt;- read.table(\"./featurecounts/featurecounts_results.mod.txt\",\n                            header=TRUE, row.names = 1)\n# First 6 columns contain information about the transcripts. Here we only need the feature count informat. So we will remove first 6 columns by select the column 6 to 19\ndata &lt;- feature_count[,6:19]\nview(data)\n</code></pre></p> <p>Similarly, you can view the data by typing <code>data</code> or <code>view(data)</code></p> <p></p>"},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/#deseq2-create-deseq2-dataset-object","title":"DESeq2: Create DESeq2 Dataset object","text":"<p>Before running DESeq2, load all required libraries by running below: <pre><code># Put HPC biotools R libraries on your R path\n.libPaths(c('', '/cluster/tufts/bio/tools/R_libs/3.5/'))\n# load required libraries\nlibrary(DESeq2)\nlibrary(vsn)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(DEGreport)\nlibrary(pheatmap)\n</code></pre> To run DESeq2 analysis, you have to check to make sure that all rows labels in meta are columns in data: <pre><code>all(colnames(data) == rownames(meta))\n</code></pre></p> <p>Create the dataset and run the analysis: <pre><code>dds &lt;- DESeqDataSetFromMatrix(countData = data, colData = meta, design = ~ condition)\ndds &lt;- DESeq(dds)\n</code></pre> Behind the scenes these steps were run: 1.estimating size factors 2.gene-wise dispersion estimates 3.mean-dispersion relationship 4.final dispersion estimates 5.fitting model and testing</p> <p>The design formula <code>design = ~condition</code> tells DESeq2 which factors in the metadata to test, such as control v.s. treatment. Here our condition is WT v.s. SNF2 as shown in the <code>meta</code>. The design can include multiple factors that are columns in the metadata. In this case, the factor that you are testing for comes last, and factors that you want to account for come first. E.g. To test for differences in condition while accounting for sex and age: <code>design = ~ sex + age + condition</code>. It's also possible to include time series data and interactions.</p> <ul> <li>Normalization</li> </ul> <p>The number of sequenced reads mapped to a gene depends on: Gene Length, Sequencing depth, The expression level of other genes in the sample and Its own expression level. Normalization using DESeq2 accounts for both sequencing depth and composition. Step 1: creates a pseudo-reference sample (row-wise geometric mean). For each gene, a pseudo-reference sample is created that is equal to the geometric mean across all samples.</p> <p></p> <p>Step 2: calculates ratio of each sample to the reference. Calculate the ratio of each sample to the pseudo-reference. Since most genes aren't differentially expressed, ratios should be similar.</p> <p></p> <p>Step 3: calculate the normalization factor for each sample (size factor). The median value of all ratios for a given sample is taken as the normalization factor (size factor) for that sample: <pre><code>normalization_factor_sampleA &lt;- median(c(1.00, 3.16)) = 2.08\nnormalization_factor_sampleB &lt;- median(c(1.00, 0.32)) = 0.66\n</code></pre></p> <p>Step 4: calculate the normalized count values using the normalization factor. This is performed by dividing each raw count value in a given sample by that sample's size factor to generate normalized count values. <pre><code>SampleA normalization factor = 2.08\nSampleB normalization factor = 0.66\n</code></pre></p> <p></p> <ul> <li>Unsupervised Clustering</li> </ul> <p>This step is to asses overall similarity between samples: 1.Which samples are similar to each other, which are different? 2.Does this fit to the expectation from the experiment\u2019s design? 3.What are the major sources of variation in the dataset?</p> <ul> <li>Principle Components Analysis</li> </ul> <p>This uses the built in function plotPCA from DESeq2 (built on top of ggplot). The regularized log transform (rlog) improves clustering by log transforming the data. <pre><code>rld &lt;- rlog(dds, blind=TRUE)\nplotPCA(rld, intgroup=\"condition\") + geom_text(aes(label=name))\n</code></pre></p> <p></p> <ul> <li>Creating contrasts and running a Wald test</li> </ul> <p>The null hypothesis: log fold change = 0 for across conditions. P-values are the probability of rejecting the null hypothesis for a given gene, and adjusted p values take into account that we've made many comparisons: <pre><code>contrast &lt;- c(\"condition\", \"SNF2\", \"WT\")\nres_unshrunken &lt;- results(dds, contrast=contrast)\nsummary(res_unshrunken)\n</code></pre> Here shows a summary of up- or down-regulated genes:</p> <p></p> <ul> <li>Shrinkage of the log2 fold changes</li> </ul> <p>One more step where information is used across genes to avoid overestimates of differences between genes with high dispersion. This is not done by default, so we run the code: <pre><code>res &lt;- lfcShrink(dds, contrast=contrast, res=res_unshrunken)\n</code></pre></p> <ul> <li>Exploring results</li> </ul> <p>The summary of results after shrinkage can be viewed by typing <code>summary(res)</code> or <code>head(res)</code>. If you used <code>head(res)</code> you will be viewing the top few lines of the result containing log2 fold change and p-value. log2FoldChange = log2(SNF2count/WTcount)Estimated from the model. padj - Adjusted pvalue for the probability that the log2FoldChange is not zero.</p> <p></p> <ol> <li>Plot single gene Now, you can explore individual genes that you might be interested in. A simple plot can be made to compare the expression level of a particular gene. For example, for gene \"YOR290C\": <pre><code>plotCounts(dds, gene=\"YOR290C\", intgroup=\"condition\")\n</code></pre></li> </ol> <p></p> <ol> <li>Saving the result Now you have the table with log2 fold change and you might want to save it for future analysis. A adj value cutoff can be applied here. For example, here p-adj 0.05 is used.</li> </ol> <p><pre><code>## Filtering to find significant genes\npadj.cutoff &lt;- 0.05 # False Discovery Rate cutoff\nsignificant_results &lt;- res[which(res$padj &lt; padj.cutoff),]\n\n## save results using customized file_name\nfile_name = 'significant_padj_0.05.txt'\nwrite.table(significant_results, file_name, quote=FALSE)\n</code></pre> Now you have your analyzed result saved in txt file, which can be imported to Excel.</p> <ul> <li>Exit R and save the work space</li> </ul> <p>If you want to take a break and exit R, type <code>q()</code>. The workspace will be automatically saved with the extension of <code>.Rproj</code>.</p>"},{"location":"omics/intro-to-rnaseq/05_Differential_Expression/#review-deseq2-workflow","title":"Review DeSeq2 workflow","text":"<p>These comprise the full workflow <pre><code># Setup DESeq2\ndds &lt;- DESeqDataSetFromMatrix(countData = data, colData = meta, design = ~ condition)\n# Run size factor estimation, dispersion estimation, dispersion shrinking, testing\ndds &lt;- DESeq(dds)\n# Tell DESeq2 which conditions you would like to output\ncontrast &lt;- c(\"condition\", \"SNF2\", \"WT\")\n# Output results to table\nres_unshrunken &lt;- results(dds, contrast=contrast)\n# Shrink log fold changes for genes with high dispersion\nres &lt;- lfcShrink(dds, contrast=contrast, res=res_unshrunken)\n# Regularized log transformation (rlog)\nrld &lt;- rlog(dds, blind=TRUE)\n</code></pre></p>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/","title":"Pathway Enrichment","text":""},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#pathway-enrichment-analysis","title":"Pathway enrichment analysis","text":"<p>Approximate time: 40 minutes</p>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Use R to visulize DESeq2 results</li> <li>A few recommendations for functional enrichment analysis</li> </ul>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#step-1-start-rstudio-on-the-tufts-hpc-cluster-via-on-demand","title":"Step 1. Start Rstudio on the Tufts HPC cluster via \"On Demand\"","text":"<ol> <li>Open a Chrome browser and visit ondemand.cluster.tufts.edu</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose Interactive Apps -&gt; Rstudio</li> </ol> <ol> <li>Choose:</li> </ol> <pre><code>Number of hours: 4\nNumber of cores: 1\nAmount of Memory: 32 Gb\nR version: 3.5.0\n</code></pre>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#step-2-load-the-previously-saved-r-project","title":"Step 2. Load the previously saved R project","text":"<p>You should automatically see the previous work. If not, you can load the previous session following these steps: Go to <code>File</code>, choose <code>Open Project...</code>, navigate to your folder and selected the previously saved file with extension of <code>.Rproj</code>. All previously saved variables and libraries will be loaded.</p> <ul> <li>Visualization of DeSeq2 result</li> </ul>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#use-volcano-plot-to-visualize-up-and-down-regulated-genes","title":"Use Volcano plot to visualize up- and down- regulated Genes","text":"<p><pre><code># load necessary library ggplot2\nlibrary(ggplot2)\n\n# add another column in the results table to label the significant genes using threshold of padj&lt;0.05 and absolute value of log2foldchange &gt;=1\nres_table &lt;- res %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") %&gt;%\n  as_tibble()\nres_table &lt;- res_table %&gt;%\n  mutate(threshold_OE =  padj &lt; 0.05 &amp; abs(log2FoldChange) &gt;= 1)\n# you can view the modified table\nview(res_table)\n# make volcano plot, the significant genes will be labeled in red\nggplot(res_table) +\n  geom_point(aes(x = log2FoldChange, y = -log10(padj), colour = threshold_OE)) +\n  scale_color_manual(values=c(\"black\", \"red\")) +  # black v.s. red dots\n  ggtitle(\"WT v.s. SNF2\") +                       # this line defines the title of the plot\n  xlab(\"log2 fold change\") +                      # this line defines the name of the x-axis\n  ylab(\"-log10 adjusted p-value\") +               # name of y-axis\n  scale_x_continuous(limits = c(-7.5,7.5)) +      # the axis range is set to be from -7.5 to 7.5\n  theme(legend.position = \"none\", #c(0.9, 0.9),\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))  \n</code></pre> The commands will generate a volcano plot as shown below</p> <p></p>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#plot-top-50-significant-genes-in-a-heatmap","title":"Plot top 50 significant genes in a heatmap","text":"<p>Sort the rows from smallest to largest padj and take the top 50 genes: <pre><code>significant_results_sorted &lt;- significant_results[order(significant_results$padj), ]\nsignificant_genes_50 &lt;- rownames(significant_results_sorted[1:50, ])\n</code></pre> We now have a list of 50 genes with most significant padj value. But we need to find the counts corresponding to these genes. To extract the counts from the rlog transformed object: <pre><code>rld_counts &lt;- assay(rld)\n</code></pre> Select by row name using the list of genes: <pre><code>rld_counts_sig &lt;- rld_counts[significant_genes_50, ]\n</code></pre> Plot multiple genes in a heatmap: <pre><code>pheatmap(rld_counts_sig,\ncluster_rows = T,\nshow_rownames = T,\nannotation = meta,\nborder_color = NA,\nfontsize = 10,\nscale = \"row\",\nfontsize_row = 8,\nheight = 20)\n</code></pre> </p>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#next-step-functional-enrichment","title":"Next step: Functional Enrichment","text":""},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#functional-enrichment-using-r-library-clusterprofiler","title":"Functional enrichment using R library clusterProfiler","text":"<p>To run the functional enrichment analysis, we first need to select genes of interest. Here we are interested in the 500 genes with lowest padj value (or the 500 most significantly differentially regulated genes). To do this, we first rank the previous result using padj value, then we select the gene names for the top 500. The list of 500 genes will be passed into enrichGO program and be analyzed for GO enrichment. Below are the codes needed to perform enrichment analysis.</p> <p><pre><code>## load required library\nlibrary(clusterProfiler)\n\n## load the proper database for your organism of interest\nlibrary(org.Sc.sgd.db)\n\n## Run GO enrichment analysis for the top 500 genes\nsignificant_results_sorted &lt;- res[order(res$padj), ]\nsignificant_genes_500 &lt;- rownames(significant_results_sorted[1:500, ])\nego &lt;- enrichGO(gene = significant_genes_500,\n                         keyType = \"ENSEMBL\",\n                         OrgDb = org.Sc.sgd.db)\n\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\n## Show a Dotplot\ndotplot(ego, showCategory=50)\n## Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms\nemapplot(ego, showCategory = 50)\n</code></pre> After you ran these codes, a dotplot and a emapplot will be generated.</p> <ul> <li>Dotplot: pathway enrichment</li> </ul> <p></p> <ul> <li>emapplot: pathway interaction</li> </ul> <p></p> <p>If your organism happens to be within the clusterprofiler database as shown below, you can easily use the code above for functional enrichment analysis.</p> <p></p> <p>A great tutorial to follow for functional enrichment can be found at https://hbctraining.github.io/DGE_workshop/lessons/09_functional_analysis.html</p>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#pathway-analysis-using-other-tools","title":"Pathway analysis using other tools","text":"<p>If your organism is not within the above database, you will have to pick your gene of interest (using log2 fold change cutoff and/or padj cutoff) and analyze the functional enrichment using String or Blast2Go.</p>"},{"location":"omics/intro-to-rnaseq/06_Pathway_Enrichment/#review-all-scripts","title":"Review all Scripts","text":"<ul> <li>Command line scripts</li> <li>R scripts</li> </ul>"},{"location":"omics/intro-to-rnaseq/07_dependencies/","title":"07 dependencies","text":""},{"location":"omics/intro-to-rnaseq/07_dependencies/#dependencies-for-rna-sequencing-workshop","title":"Dependencies for RNA sequencing workshop","text":""},{"location":"omics/intro-to-rnaseq/07_dependencies/#description","title":"Description:","text":"<p>To follow along with the practice in this course, you will use command line tools and R installed on your computer. Additionally, you will have to install the softwares and R libraries we will be using.</p>"},{"location":"omics/intro-to-rnaseq/07_dependencies/#full-list-of-dependencies","title":"Full list of Dependencies:","text":"<ol> <li> <p>Download Raw data for this course</p> </li> <li> <p>Command line tools for preprocessing reads and read alignment: FASTQC STAR samtools subread</p> </li> <li> <p>Download and install Interactive Genome Viewer (IGV)</p> </li> <li> <p>R and its libraries for differential expression analysis: R studio</p> <p>Bioconductr libraries (check out each library for installation guide): - DESeq2 - vsn - DEGreport - org.Sc.sgd.db - clusterProfiler</p> <p>Regular libraries (install by entering <code>install.packages(\"name\")</code>):</p> <ul> <li>tidyverse</li> <li>ggplot2</li> <li>dplyr</li> <li>ggrepel</li> <li>pheatmap</li> </ul> </li> </ol>"},{"location":"omics/intro-to-rnaseq/07_dependencies/#scripts-you-will-be-using","title":"Scripts you will be using:","text":"<ul> <li>Command line scripts</li> <li>R scripts</li> </ul>"},{"location":"omics/intro-to-rnaseq/07_dependencies/#workshop-schedule","title":"Workshop Schedule","text":"<ul> <li>Introduction</li> <li>Setup using Tufts HPC</li> <li>Process Raw Reads</li> <li>Read Alignment</li> <li>Gene Quantification</li> <li>Differential Expression</li> <li>Pathway Enrichment</li> </ul>"},{"location":"omics/intro-to-rnaseq/08_bash_scripts/","title":"08 bash scripts","text":""},{"location":"omics/intro-to-rnaseq/08_bash_scripts/#bash-scripts-for-rna-sequencing","title":"Bash scripts for RNA sequencing","text":""},{"location":"omics/intro-to-rnaseq/08_bash_scripts/#these-scripts-are-used-to-process-raw-reads-align-processed-reads-and-quantify-gene-expression-using-feature-counts","title":"These scripts are used to process raw reads, align processed reads and quantify gene expression using feature counts","text":"<pre><code># For Tufts HPC users who attended our 1-hr zoom workshop, please run following command to get a compute node\nsrun -t 3:00:00 --mem 16G -N 1 -n 4 -p preempt --reservation bioworkshop --pty bash\n\n# For other HPC users, please use below command\nsrun -t 3:00:00 --mem 16G -N 1 -n 4 -p preempt --pty bash\n\n# Navigate to project folder\ncd /cluster/tufts/bio/tools/training/intro-to-rnaseq/users/\n\n# make a new directory using your username for your practice, and enter that directory\nmkdir YOUR_USERNAME\ncd YOUR_USERNAME\n# copy course material to your directory, unzip it and enter the course material directory\ncp /cluster/tufts/bio/tools/training/intro-to-rnaseq/intro-to-RNA-seq-May-2020.tar.gz ./\ntar -xvzf intro-to-RNA-seq-May-2020.tar.gz\ncd intro-to-RNA-seq\n\n# load fastqc module\nmodule load fastqc/0.11.8\nmkdir fastqc\n\n# Run fastqc on raw sequencing reads. Each fastq file will be analyzed individually. * is a wild card. \nfastqc raw_data/WT/*.fastq.gz -o fastqc --extract\nfastqc raw_data/SNF2/*.fastq.gz -o fastqc --extract\n\n# run multiqc to compile individual fastqc files, this helps visualization of fastqc reports\nmodule load multiqc/1.7.0\nmkdir multiqc\nmultiqc fastqc/ -o multiqc\n\n# Read alignment Step 1: prepare reference genomes. Your input will be genome.fa.\nmodule load STAR/2.6.1d\nmkdir genome\nSTAR --runMode genomeGenerate --genomeDir ./genome --genomeFastaFiles /cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/Sequence/WholeGenomeFasta/genome.fa --runThreadN 4\n\n# Read alignment Step 2: align. You will need your own annotation file in gtf format. You will run this step for individual samples. Here we are using ERR458493.fastq.gz as an example.\nmkdir STAR\nSTAR --genomeDir ./genome \\\n--readFilesIn raw_data/WT/ERR458493.fastq.gz \\\n--readFilesCommand zcat \\\n--outFileNamePrefix STAR/ \\\n--outFilterMultimapNmax 1 \\\n--outSAMtype BAM SortedByCoordinate \\\n--runThreadN 4 \\\n--alignIntronMin 1 \\\n--alignIntronMax 2500 \\\n--sjdbGTFfile annotation.gtf \\\n--sjdbOverhang 49\n\n# Read alignment Step 3: generate bam index. The output.bam is output file from step 2. You will run this for individual samples following Step 2.\nmodule load samtools/1.2\nsamtools index STAR/output.bam\n\n\n# Before moving on to next step, make sure your STAR folder contains 14 bam files, one for each replicate.\n# You can do so by running\nsh ./scripts/star_align_individual.sh\n\n\n# Gene quantification using featureCounts - This step compiles all alignment results together. This is done after alignment is finished for all samples.\n# Gene quantification step 1: load subread module\nmodule load subread/1.6.3\n# Gene quantification step 2: create output directory\nmkdir featurecounts\n# Gene quantification step 3: Run featurecounts\nfeatureCounts \\\n-a /cluster/tufts/bio/data/genomes/Saccharomyces_cerevisiae/UCSC/sacCer3/Annotation/Genes/sacCer3.gtf \\\n-o featurecounts/featurecounts_results.txt \\\nSTAR/*.bam\n\n\n# Check feature count results\ncat featurecounts/featurecounts_results.txt.summary\nhead featurecounts/featurecounts_results.txt\n# Clean the column names in featurecounts_results.txt\ncat featurecounts/featurecounts_results.txt |sed \"2s/STAR\\///g\" | sed \"2s/\\_Aligned.sortedByCoord.out.bam//g\" &gt; featurecounts/featurecounts_results.mod.txt\n\n# now you are ready to move on to R scripts.\n\n\n\n# Optional step 1. Visualize number of mapped reads v.s. unmapped reads in all samples using barplot. This code will generate a pdf file named Mapping_stat.pdf.\nmodule load R/3.5.0\nRscript ./scripts/mapping_percentage.R\n\n# Optional step 2. Visualize number of assigned reads in all samples using barplot. This code will generate a pdf file named Featurecount_stat.pdf.\nmodule load R/3.5.0\nRscript ./scripts/featurecount_stat.R\n</code></pre>"},{"location":"omics/intro-to-rnaseq/09_R_scripts/","title":"09 R scripts","text":""},{"location":"omics/intro-to-rnaseq/09_R_scripts/#r-scripts-for-differential-expression","title":"R scripts for differential expression","text":""},{"location":"omics/intro-to-rnaseq/09_R_scripts/#these-scripts-are-used-to-calculate-differential-expression-using-featurecounts-data","title":"These scripts are used to calculate differential expression using featurecounts data","text":"<pre><code># set to work directory: make sure to set directory to project folder. Change whuo01 to your own username below\nsetwd(\"/cluster/tufts/bio/tools/training/intro-to-rnaseq/users/whuo01/intro-to-RNA-seq/\")\n\n# load shared Tufts bio library path; if you don't have access to tufts HPC, skip this step\n.libPaths('/cluster/tufts/bio/tools/R_libs/3.5')\n\n# load required libraries\nlibrary(DESeq2)\nlibrary(vsn)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(DEGreport)\nlibrary(pheatmap)\n\n\n# load data.\nmeta &lt;- read.table(\"raw_data/sample_info.txt\", header=TRUE)\nfeature_count &lt;- read.table(\"./featurecounts/featurecounts_results.mod.txt\",\n                            header=TRUE, row.names = 1)\n# remove first 6 columns by select the column 6 to 19\ndata &lt;- feature_count[,6:19]\n\n# check to make sure that all rows labels in meta are columns in data\nall(colnames(data) == rownames(meta))\n\n# Differntial expression using DESeq2\ndds &lt;- DESeqDataSetFromMatrix(countData = data, colData = meta, design = ~ condition)\ndds &lt;- DESeq(dds)\n# Creating contrasts and running a Wald test\ncontrast &lt;- c(\"condition\", \"SNF2\", \"WT\")\nres_unshrunken &lt;- results(dds, contrast=contrast)\nsummary(res_unshrunken)\n# Shrinkage of the log2 fold changes\nres &lt;- lfcShrink(dds, contrast=contrast, res=res_unshrunken)\nsummary(res)\n\n# Quality control: Principle Components Analysis\n# regularized log transformation (rlog)\nrld &lt;- rlog(dds, blind=TRUE)\nplotPCA(rld, intgroup=\"condition\") + geom_text(aes(label=name))\n\n# Filtering to find significant genes using FDR cutoff of 0.05\npadj.cutoff &lt;- 0.05 # False Discovery Rate cutoff\nsignificant_results &lt;- res[which(res$padj &lt; padj.cutoff),]\n# save results using customized file_name\nfile_name = 'significant_padj_0.05.txt'\nwrite.table(significant_results, file_name, quote=FALSE)\n\n\n\n# Visualization\n\n# simple plot for a single gene YOR290C (SNF2)\nplotCounts(dds, gene=\"YOR290C\", intgroup=\"condition\")\n\n# heatmap  \n# plot top 50 genes in a heatmap: top 50 with most significant padj value\nsignificant_results_sorted &lt;- significant_results[order(significant_results$padj), ]\nsignificant_genes_50 &lt;- rownames(significant_results_sorted[1:50, ])\n# extract the counts from the rlog transformed object\nrld_counts &lt;- assay(rld)\n# select by row name using the list of genes:\nrld_counts_sig &lt;- rld_counts[significant_genes_50, ]\n# Plot multiple genes in a heatmap:\npheatmap(rld_counts_sig,\n         cluster_rows = T,\n         show_rownames = T,\n         annotation = meta,\n         border_color = NA,\n         fontsize = 10,\n         scale = \"row\",\n         fontsize_row = 8,\n         height = 20)\n\n\n# volcano plot\n# load necessary library ggplot2\nlibrary(ggplot2)\n# add another column in the results table to label the significant genes using threshold of padj&lt;0.05 and absolute value of log2foldchange &gt;=1\nres_table &lt;- res %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") %&gt;%\n  as_tibble()\nres_table &lt;- res_table %&gt;%\n  mutate(threshold_OE =  padj &lt; 0.05 &amp; abs(log2FoldChange) &gt;= 1)\n# you can view the modified table\nview(res_table)\n# make volcano plot, the significant genes will be labeled in red\nggplot(res_table) +\n  geom_point(aes(x = log2FoldChange, y = -log10(padj), colour = threshold_OE)) +\n  scale_color_manual(values=c(\"black\", \"red\")) +  # black v.s. red dots\n  ggtitle(\"SNF2 against WT\") +                       # this line defines the title of the plot\n  xlab(\"log2 fold change\") +                      # this line defines the name of the x-axis\n  ylab(\"-log10 adjusted p-value\") +               # name of y-axis\n  scale_x_continuous(limits = c(-7.5,7.5)) +      # the axis range is set to be from -7.5 to 7.5\n  theme(legend.position = \"none\", #c(0.9, 0.9),\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))\n\n\n\n\n# functional analysis using clusterprofiler\n# load library\nlibrary(org.Sc.sgd.db)\nlibrary(clusterProfiler)\n\n## Run GO enrichment analysis for the top 500 genes\nsignificant_results_sorted &lt;- res[order(res$padj), ]\nsignificant_genes_500 &lt;- rownames(significant_results_sorted[1:500, ])\nego &lt;- enrichGO(gene = significant_genes_500,\n                         keyType = \"ENSEMBL\",\n                         OrgDb = org.Sc.sgd.db)\n\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\n## Dotplot\ndotplot(ego, showCategory=50)\n## Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms\nemapplot(ego, showCategory = 50)\n</code></pre>"},{"location":"omics/intro-to-rnaseq/background/","title":"Background","text":""},{"location":"omics/intro-to-rnaseq/background/#background","title":"Background","text":"<p>Sequencing data analysis typically focuses on either assessing DNA or RNA. As a reminder here is the interplay between DNA, RNA, and protein:</p> <p></p>"},{"location":"omics/intro-to-rnaseq/background/#dna-sequencing","title":"DNA Sequencing","text":"<ul> <li>Fixed copy of a gene per cell </li> <li>Analysis goal: Variant calling and interpretation</li> </ul>"},{"location":"omics/intro-to-rnaseq/background/#rna-sequencing","title":"RNA Sequencing","text":"<ul> <li>Copy of a transcript per cell depends on gene expression</li> <li>Analysis goal: Differential expression and interpretation</li> </ul> <p>Note</p> <p>Here we are working with RNA sequencing</p>"},{"location":"omics/intro-to-rnaseq/background/#next-generation-sequencing","title":"Next Generation Sequencing","text":"<p>Here we will analyze a DNA sequence using next generation sequencing data. Here are the steps to get that data:</p> <ul> <li>Library Preparation: DNA is fragmented and adapters are added to these fragments</li> </ul> <p></p> <ul> <li>Cluster Amplification: This library is loaded onto a flow cell, where the adapters help hybridize the fragments to the flow cell. Each fragment is then amplified to form a clonal cluster</li> </ul> <p></p> <ul> <li>Sequencing: Fluorescently labelled nucleotides are added to this flow cell and each time a base in the fragment bonds a light signal is emmitted telling the sequencer which base is which in the sequence.</li> </ul> <p></p> <ul> <li>Alignment &amp; Data Analysis: These sequenced fragments, or reads, can then be aligned to a reference sequence to determine differences.</li> </ul> <p></p>"},{"location":"omics/intro-to-rnaseq/background/#singe-end-v-paired-end-data","title":"Singe End v. Paired End Data","text":"<ul> <li>single-end sequence each DNA fragement from one end only</li> <li>paired-end sequence each DNA fragement from both sides. Paired-end data is useful when sequencing highly repetitive sequences.</li> </ul>"},{"location":"omics/ngsTipsAndTricks/fastq/fastq/","title":"Fastq Manipulation","text":""},{"location":"omics/ngsTipsAndTricks/fastq/fastq/#ngs-tips-tricks","title":"NGS Tips &amp; Tricks","text":"<p>NGS data is widely used to assess RNA/DNA by providing insight into genome sequences, RNA transcription activity, and analyze epigenetic activity -  just to name a few applications. Manipulating this data can be difficult, so here we provide a few tips and tricks for manipulating Fastq data. </p>"},{"location":"omics/ngsTipsAndTricks/fastq/fastq/#counting-sequences-in-a-fastq-file","title":"Counting Sequences in a Fastq File","text":"<ul> <li>Counting sequences in a non-compressed fastq file:</li> </ul> <pre><code>cat Mock_12hr_rep1.fastq | echo \"$((`wc -l` / 4))\"\n</code></pre> <ul> <li>Counting sequences in a compressed fastq file:</li> </ul> <pre><code>zcat Mock_12hr_rep1.fastq.gz | echo \"$((`wc -l` / 4))\"\n</code></pre>"},{"location":"omics/ngsTipsAndTricks/fastq/fastq/#subsampling-a-fastq-file","title":"Subsampling a Fastq File","text":"<ul> <li>Subsampling 10,000 sequences from paired fastq files using the same random seed (<code>-s100</code>) so that the same sequences are grabbed from each file:</li> </ul> <pre><code># load the anaconda module\nmodule load anaconda/2021.11\n\n# activate the conda environment\nsource activate /cluster/tufts/bio/tools/conda_envs/seqtk/1.3\n\n# subsample the fastq file\nseqtk sample -s100 fullData_R1.fastq.gz 10000 &gt; subsampledData_R1.fastq.gz\nseqtk sample -s100 fullData_R2.fastq.gz 10000 &gt; subsampledData_R2.fastq.gz\n</code></pre>"},{"location":"omics/ngsTipsAndTricks/fastq/fastq/#quality-control","title":"Quality Control","text":"<ul> <li>Perform FastQC on your files and then MulitQC to aggregate these reports:</li> </ul> <pre><code># make directory for output\nmkdir fastqc\n\n# load the fastqc module\nmodule load fastqc\n\n# run fastqc on all fastq files in this directory and output them to the fastqc directory\nfastqc *fastq.gz -o fastqc\n</code></pre> <ul> <li>To collate these reports into one report:</li> </ul> <pre><code># make multiqc directory\nmkdir multiqc\n\n# run multiqc on fastqc results\nmultiqc fastqc/ -o multiqc/\n</code></pre>"},{"location":"omics/ngsTipsAndTricks/fastq/fastq/#convert-fastq-to-fasta","title":"Convert Fastq to Fasta","text":"<pre><code># load the anaconda module\nmodule load anaconda/2021.11\n\n# activate the conda environment\nsource activate /cluster/tufts/bio/tools/conda_envs/seqtk/1.3\n\n# convert fastq to fasta\nseqtk seq -a someData.fastq.gz &gt; someData.fasta\n</code></pre>"},{"location":"omics/ngsTipsAndTricks/fastq/fastq/#extract-sequences-in-regions-from-a-bed-file","title":"Extract Sequences in Regions from a BED File","text":"<pre><code># load the anaconda module\nmodule load anaconda/2021.11\n\n# activate the conda environment\nsource activate /cluster/tufts/bio/tools/conda_envs/seqtk/1.3\n\n# extract sequences from bed file\nseqtk subseq fullData.fasta filterdBed.bed &gt; filteredData.fasta\n</code></pre>"},{"location":"stats/binomial-test/","title":"Binomial Test","text":""},{"location":"stats/binomial-test/#estimating-proportions","title":"Estimating Proportions","text":"<p>When we estimate proportions using a sample, this proportion is also subject to sampling error. And just like the mean, we can estimate it's standard error with:</p> \\[SE_p = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] <p>Term Definitions</p> <ul> <li>\\(SE_p\\) standard error of the proportion</li> <li>\\(\\hat{p}\\) sample proportion</li> <li>\\(n\\) number of observations</li> <li>\\(X\\) number of successes out of \\(n\\) observations (down below)</li> </ul> <p>However, there is an issue with this standard error estimate: When either the \\(n\\) is small or our \\(\\hat{p}\\) is extreme (close to 0 or 1) the estimate is not reliable. To remedy this we use the Agresti-Coull interval:</p> \\[p^\\prime = \\frac{X + 2}{n + 4}\\] <p>So that the confidence interval is:</p> \\[1.96 \\pm \\sqrt{\\frac{p^\\prime(1-p^\\prime)}{n + 4}}\\] <p>So let's use this formula to estimate the proportion of males in our sample:</p> <pre><code>library(tidyverse)\nlibrary(binom)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n#take our sample\nsex &lt;- sample(meta$SEX,20)\n\n# define number of successes and number observations\nn = length(sex[!(is.na(sex))])\nmales = length(sex[sex==\"Male\"])\n\n#calculate our confidence interval\nsex.confit &lt;- binom.confint(x = males, n = n, conf.level = 0.95, methods = \"ac\")\n\nsex.confit\n</code></pre> <pre><code>         method  x  n mean     lower     upper\n1 agresti-coull 13 20 0.65 0.4315888 0.8200736\n</code></pre>"},{"location":"stats/binomial-test/#binomial-distribution","title":"Binomial Distribution","text":"<p>What we have just calculated a confidence interval for a binary variable. Above we measured the interval we believe our true population proportion estimate of males to be between. Now let's switch gears and discuss the probability of \\(X\\) number of males in \\(n\\) independent observations would be:</p> \\[P[X] = \\binom{n}{X} p^X (1-p)^{N-X}\\] <p>Where:</p> \\[\\binom{n}{X} = \\frac{n!}{X!(n-X)!}\\] <p>Term Definitions</p> <ul> <li>\\(n\\) number of trials/observations</li> <li>\\(X\\) number of successes</li> <li>\\(p\\) probability of success with each trial/observation</li> </ul> <p>We can see that with different numbers of successes come different probabilities. We can plot these differences in probabilities to get an idea of how probability changes when you change the number of successes:</p> <pre><code>#20 observations/trials\n# with a probabilty of 0.65\nsex.exact.prob &lt;- data.frame(\nX = 0:20,\nprobs = dbinom(x = 0:20, size = 20, prob = 0.65)\n)\n\nggplot(sex.exact.prob, aes(x=X,y=probs)) + geom_bar(fill=\"lightpink\",stat = \"identity\")+\ntheme_bw()+\nlabs(\nx=\"X number of Successes\",\ny=\"Probability of X Successes\",\ntitle=\"Probability of X Successes v. X number of Successes\"\n)\n</code></pre> <p></p> <p>Note</p> <p>You'll note that we use <code>dbinom</code> (for the probability density function) - but there is also the option for <code>pbinom</code> (for the cumulative density function ). What do these options mean? - probability density function probability of exactly X successes - cumulative density function probability of less than or equal to X successes</p> <p>Here we are trying to determine the probability of exactly X successes which is why we use <code>dbinom</code></p>"},{"location":"stats/binomial-test/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>So we've determined the probability of \\(X\\) successes in \\(N\\) trials with a \\(p\\) probability of success per trial. What if we wanted to test a hypothesis that this is or is not the behavior of the underlying population? First let's cover some hypothesis testing terms:</p> <ul> <li>\\(H_0\\) or null hypothesis states that any oberved differences are due to chance</li> <li>\\(H_1\\) or alternative hypohthesis that any oberved differences are not due to chance</li> <li>\\(\\alpha\\) or probability where we reject the null (a.k.a hypothesis where any oberved differences are due to chance)</li> <li>test statistic or numeric summary to help distinguish between the the null and the alternative hypothesis</li> </ul> <p>Here we will ask, is this proportion of males evidence that males are over-represented in glioblastoma patients? So if we wanted to test for this overrepresentation, we could specify the number of males in our samples, the total number of observations, and the probability under the null hypothesis(that there is a 50% probability of male patients and a 50% probability of female patients).</p> <pre><code>binom.res &lt;- binom.test(x = 13, n = 20, p = 0.5, alternative = \"two.sided\")\nbinom.res\n</code></pre> <pre><code>Exact binomial test\n\ndata:  13 and 20\nnumber of successes = 13, number of trials = 20, p-value = 0.2632\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4078115 0.8460908\nsample estimates:\nprobability of success \n                  0.65 \n</code></pre> <p>Explanation</p> <p>So here we see that our hypothesis is that:</p> <ul> <li>we have 13 successes and 20 trials</li> <li>our hypothesis is that our true probability of success is not equal to 0.5</li> <li>our p-value is above 0.05; so we do not have enough evidence to reject the null (a.k.a that our observed probability is due to chance)</li> <li>our sample probability of success is 0.65</li> </ul> <p>Reportable Confidence Interval</p> <p>Remember to use the Agresti-Coull confidence interval, and not the confidence interval reported by <code>binom.test()</code></p>"},{"location":"stats/binomial-test/#references","title":"References","text":"<ol> <li>Wilson score and Agresti-Coull intervals for binomial proportions</li> <li>BIOL - 202: Analyzing a single categorical variable</li> <li>A Guide to dbinom, pbinom, qbinom, and rbinom in R</li> <li>Test Statistic</li> </ol>"},{"location":"stats/biostats/","title":"Introduction to Biostatistics","text":"<p>Biostatistics attempts to use statiscal methods to solve biological problems. This involves data so for the purpose of our biostatistics tutorials we will need to do some setup.</p>"},{"location":"stats/biostats/#setup","title":"Setup","text":"<p>For the following machine learning tutorials we will be using glioblastoma data from cBioPortal. Before getting started you will need:</p> <ul> <li>Account on Tufts HPC</li> <li>VPN if accessing the HPC from off campus</li> </ul>"},{"location":"stats/biostats/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p>"},{"location":"stats/biostats/#setting-up-a-project-space","title":"Setting Up A Project Space","text":"<p>We are going to open an interactive app:</p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>32GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Default</code></li> <li><code>Load Supporting Modules</code>: <code>curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6</code></li> </ul> <p>We will now need to create our project that we will work out of:</p> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. Now Create a new project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>machine-learning</code>)</li> <li><code>Create Project</code></li> </ol> <p>In terminal, start setting up your directories:</p> <pre><code>mkdir data\nmkdir scripts\nmkdir results\n</code></pre> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>cd data\nwget https://cbioportal-datahub.s3.amazonaws.com/gbm_cptac_2021.tar.gz\ntar -xvf gbm_cptac_2021.tar.gz cd ..\n</code></pre>"},{"location":"stats/chi-square/","title":"Chi-Square Test","text":""},{"location":"stats/chi-square/#chi-square-test","title":"Chi-Square Test","text":"<p>When comparing categorical variables, your data won't always have 2x2 dimensions as in the Fisher's Test Topic Note.  For tables with greater dimensions we can use the Chi-square test to test hypotheses of association.  The Chi-Square or \\(\\chi^2\\) test is defined by:</p> \\[\\chi^2 = \\sum_i{\\frac{(O_i-E_i)^2}{E_i}}\\] <p>with:</p> \\[d.f. = (r-1)(c-1)\\] <p>Explanation of Terms</p> <ul> <li>\\(\\chi^2\\) chi-square statistic</li> <li>\\(O_i\\) observed value at cell i</li> <li>\\(E_i\\) expected value at cell i</li> <li>\\(d.f.\\) degrees of freedom</li> <li>\\(r\\) number of rows</li> <li>\\(c\\) number of columns</li> </ul>"},{"location":"stats/chi-square/#chi-square-distribution","title":"Chi-Square Distribution","text":"<p>This \\(\\chi^2\\) value is then compared to a probability distriubtion. In our Binomial Test Topic Note, we described that  probability distributions help us determine the probability of an event given some parameter. In the Binomial Test Topic Note, that parameter was the number of successes, but here it is our degrees of freedom. We won't cover the math of this probability distribution function here, but if your curious check out the wikipedia page on chi-sqare distributions.  Let's use R to examine how probability changes with varying degrees of freedom and \\(\\chi^2\\) values:</p> <pre><code># examine the chi square cdf\nchi.sq.df &lt;- data.frame(\nprobability = 0,\nchi_square = 0,\ndf = 0\n)\nfor(i in 2:9){\nchi.sq.df.i &lt;- data.frame(\nprobability = dchisq(1:10,i),\nchi_square = 1:10,\ndf = rep(as.character(i),10)\n)\nchi.sq.df &lt;- rbind(chi.sq.df,\nchi.sq.df.i)\n}\nchi.sq.df &lt;- chi.sq.df[-1,]\n\n\nggplot(chi.sq.df,            aes(x = chi_square,\ny = probability,\ncolor = df)) +  geom_line()+\ntheme_bw()\n</code></pre> <p></p> <p>Now, what you might have guessed from the equation above, but we are testing the following hypotheses:</p> <ul> <li>\\(H_0\\) or null hypothesis that there is no association between the two categorical values</li> <li>\\(H_a\\) or alternative hypothesis that there is an association between the two categorical values</li> </ul>"},{"location":"stats/chi-square/#visualizing-data","title":"Visualizing Data","text":"<p>Here we will test the association between losing a patient to follow up and their country of origin. Let's try visualizing this first:</p> <pre><code># load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n\n# generate a table of losing a patient to follow up\n# by their country of origin\n# removing zero count countries\ntable &lt;- as.data.frame.matrix(\ntable(meta$LOST_TO_FOLLOW_UP,meta$COUNTRY_OF_ORIGIN)\n) %&gt;%\nselect(c(China,Poland,Russia, `United States`))\n\ntable\n</code></pre> <pre><code>    China Poland Russia United States\nNo     21     14     16            16\nYes     9      2      3             5\n</code></pre> <p>Now let's see this plotted!</p> <pre><code>#plot our data\ntable %&gt;% t() %&gt;% reshape2::melt() %&gt;%\nggplot(.,aes(x=Var1,y=value,fill=Var2))+\ngeom_bar(stat = \"identity\",position=\"fill\") +\nscale_y_continuous(labels = scales::percent)+\ntheme_bw()+\nscale_fill_manual(values=c(\"aquamarine3\",\"lightpink3\")) +\nlabs(\nx=\"\",\ny=\"Frequency\",\nfill=\"Lost To Follow Up?\"\n)\n</code></pre> <p></p>"},{"location":"stats/chi-square/#running-the-chi-square-test","title":"Running the Chi-Square Test","text":"<pre><code># run the chi-square test\nchisq.test(table)\n</code></pre> <pre><code>    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 2.4197, df = 3, p-value = 0.49\n\nWarning message:\nIn chisq.test(table) : Chi-squared approximation may be incorrect\n</code></pre> <p>Explanation of Results</p> <p>Here we note that:</p> <ul> <li>Our chi-square test statistic is <code>2.4197</code></li> <li>The degrees of freedom is <code>3</code></li> <li>Our p-value is <code>0.49</code> and is greater than 0.05:<ul> <li>indicating there is not enough evidence to reject the null hypothesis (a.k.a there is no association between the two categorical values)</li> </ul> </li> <li>Our test assumptions might not be correct!</li> </ul>"},{"location":"stats/chi-square/#assumptions","title":"Assumptions","text":"<p>The Chi-Square test is not free of assumptions:</p> <ul> <li>categories shouldn't have an expected frequency less than one</li> <li>20% of the categories should not have an expected frequency less than 5</li> </ul> <p>In the table above we note that some frequency values are indeed less than 5! Now how does our test change if our values are increased ten-fold?</p> <pre><code>chisq.test(table*10)\n</code></pre> <pre><code>Pearson's Chi-squared test\n\ndata:  table * 10\nX-squared = 24.197, df = 3, p-value = 2.272e-05\n</code></pre> <p>Here we see that not only did the warning disappear but our p-value is below the canonical 0.05! So be aware that the Chi-Square test is very dependent on sample size. </p>"},{"location":"stats/chi-square/#references","title":"References","text":"<ol> <li>BIOL 202</li> <li>Chi-Square Distribution</li> </ol>"},{"location":"stats/confidence-interval/","title":"Confidence Intervals","text":""},{"location":"stats/confidence-interval/#confidence-intervals","title":"Confidence Intervals","text":"<p>Estimating the mean from a sample is going to have some fluctuation defined by the standard error. We can define a range or confidence interval  which we expect to contain the true mean. Often we report a 95% confidence interval. This interval is defined by plus or minus 1.96 times the standard error:</p> \\[ -1.96\\frac{\\sigma}{\\sqrt{N}} \\le \\mu  \\le +1.96\\frac{\\sigma}{\\sqrt{N}}\\] <p>Term Definitions</p> <ul> <li>\\(\\sigma\\) Standard deviation of the sample</li> <li>\\(N\\) Number of observations in the sample</li> <li>\\(\\mu\\) sample mean</li> </ul> <p>Let's try this with our sample:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n## defined some population of ages\nages &lt;- sample(meta$AGE,20)\n\n## data frame of results\nsummary &lt;- data.frame(\nSample_Mean=mean(ages,na.rm = T),\nStandard_Error=sd(ages,na.rm = T)/sqrt(length(ages[!is.na(ages)])),\nLower_Bound_CI = mean(ages,na.rm = T) - 1.96*(sd(ages,na.rm = T)/sqrt(length(ages[!is.na(ages)]))),\nUpper_Bound_CI = mean(ages,na.rm = T) + 1.96*(sd(ages,na.rm = T)/sqrt(length(ages[!is.na(ages)])))\n)\n\nsummary\n</code></pre> <pre><code>  Sample_Mean Standard_Error Lower_Bound_CI Upper_Bound_CI\n1       61.35       2.316162       56.81032       65.88968\n</code></pre> <p>Note</p> <p>So we are 95% confident that the true mean lies somewhere between <code>56.81032</code> and <code>65.88968</code></p>"},{"location":"stats/confidence-interval/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"stats/correlation/","title":"Correlation","text":""},{"location":"stats/correlation/#pearson-correlation","title":"Pearson Correlation","text":"<p>So far we have compared two groups within the same numeric variable. But what about comparing two different numeric variables? We often accomplish this  by assessing the correlation between the two numeric variables. First we will discuss Pearson Correlation which can be calculated by:</p> \\[ r = \\frac{\\sum{(x - \\mu_x)(y - \\mu_y)}}{\\sqrt{\\sum{(x - \\mu_x)^2} \\sum{(y - \\mu_y)^2}}}  \\] <p>Explanation of Terms</p> <ul> <li>\\(r\\) : correlation coefficient</li> <li>\\(x\\) : variable x</li> <li>\\(y\\) : variable y</li> <li>\\(\\mu_x\\) : mean of variable of x</li> <li>\\(\\mu_y\\) : mean of variable of y</li> </ul> <p>Pearson Correlation is called parametric correlation as it depends on the distribution of your data. This type of correlation is assessing the linear dependence of variables x and y. We can test the signifcance of this association using the t statistic for pearson correlation:</p> \\[t = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n - 2}\\] <p>Explanation of Terms</p> <ul> <li>\\(r\\) : correlation coefficient</li> <li>\\(n\\) : number of observations</li> </ul>"},{"location":"stats/correlation/#calculating-pearson-correlation","title":"Calculating Pearson Correlation","text":"<p>Using our glioblastoma data, let's determine the correlation between height and weight in our patients. However, before we do this,  let's plot height  versus weight:</p> <pre><code># let's plot height and weight\nggplot(meta,aes(x=HEIGHT,y=WEIGHT))+\ngeom_point()+\ntheme_bw()\n</code></pre> <p></p> <p>Here we see there is a positive relationship between height and weight. Let's test the significance of this relationship:</p> <pre><code># test the correlation between height and weight\ncor.test(\nx = meta$HEIGHT,\ny = meta$WEIGHT,\nmethod = \"pearson\"\n)\n</code></pre> <pre><code>    Pearson's product-moment correlation\n\ndata:  meta$HEIGHT and meta$WEIGHT\nt = 9.5779, df = 97, p-value = 1.094e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5795233 0.7863597 \nsample estimates:\n     cor \n0.697175 \n</code></pre> <p>Explanation of Results</p> <ul> <li>our test statistic is <code>9.5779</code></li> <li>our d.f. is <code>97</code></li> <li>our p-value is <code>1.094e-15</code> giving us enough evidence to reject the null hypothesis that the correlation is equal to 0</li> <li>the alternative hypothesis is that the true correlation is not equal to 0 appears supported</li> <li>the 95% confidence interval around our correlation coefficient is <code>0.5795233</code> and <code>0.7863597</code></li> <li>the correlation coefficient is <code>0.697175</code> indicating a positive association</li> </ul> <p>Correlation Coefficient Interpretation</p> <ul> <li>correlation coefficient = 0 : no association</li> <li>correlation coefficient = 1 : positive assocation</li> <li>correlation coefficient. = -1 : negative associtaion</li> </ul>"},{"location":"stats/correlation/#pearson-correlation-assumptions","title":"Pearson Correlation Assumptions","text":"<p>Now that we have conducted our test we must test our assumptions:</p> <ul> <li>the relationship is linear</li> <li>is each variable normally distributed?</li> </ul> <p>We saw from our plot that the relationship does appear linear, meaning that as one variable increases/decreases the other does too. To test the normality of each variable we will use the Shapiro-Wilk Test:</p> <pre><code># test the normality of height and weight\nshapiro.test(meta$HEIGHT)\nshapiro.test(meta$WEIGHT)\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  meta$HEIGHT\nW = 0.98735, p-value = 0.4689\n\n    Shapiro-Wilk normality test\n\ndata:  meta$WEIGHT\nW = 0.91388, p-value = 7.436e-06\n</code></pre> <p>Given our p-value for the Shapiro-Wilk Test on height is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. However, we note the opposite case for weight. The p-value is below 0.05 which indicates the data are not  normally distributed.</p>"},{"location":"stats/correlation/#spearman-rank-correlation","title":"Spearman Rank Correlation","text":"<p>Above we saw that the normality assumption was violated in our data. What are we to do! Another option we could pursue is non-parametric correlation  which does not make the assumptions we discussed above. This correlation coefficient is referred to as Spearman Rank Correlation which can be  calculated like so:</p> \\[ r = \\frac{\\sum{(x\\prime - \\mu_{x\\prime} )(y\\prime  - \\mu_{y\\prime} )}}{\\sqrt{\\sum{(x\\prime  - \\mu_{x\\prime} )^2} \\sum{(y\\prime  - \\mu_{y\\prime} )^2}}}  \\] <p>Explanation of Terms</p> <ul> <li>\\(r\\) : spearman rank correlation coefficient</li> <li>\\(x\\) : ranked variable x</li> <li>\\(y\\) : ranked variable y</li> <li>\\(\\mu_x\\) : mean of variable of x</li> <li>\\(\\mu_y\\) : mean of variable of y</li> </ul> <p>Essentially, the only difference is that the values in the variables are ranked and then the correlation coefficient is calculated. Let's try this in R!</p> <pre><code># test the spearman correlation between height and weight\ncor.test(\nx = meta$HEIGHT,\ny = meta$WEIGHT,\nmethod = \"spearman\",\nexact = FALSE\n)\n</code></pre> <pre><code>    Spearman's rank correlation rho\n\ndata:  meta$HEIGHT and meta$WEIGHT\nS = 46676, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7113395 \n</code></pre> <p>Explanation of Results</p> <ul> <li>the p-value is less than <code>2.2e-16</code></li> <li>the p-value is below 0.05 giving us enough evidence to reject the null hypothesis, that the true rho is equal to 0</li> <li>the spearman correlation coefficient is <code>0.7113395</code> indicative of a positive association</li> </ul>"},{"location":"stats/correlation/#references","title":"References","text":"<ol> <li>Correlation Test Between Two Variables in R</li> <li>BIOL 202 - Pearson correlation analysis</li> </ol>"},{"location":"stats/distributions/","title":"Probability Distributions","text":""},{"location":"stats/distributions/#the-variable-the-probability-and-the-distriubtion","title":"The Variable, The Probability, And The Distriubtion","text":"<p>Probability can be used to assess the likelihood of getting a value. Multiple values make up a variable, like a set of biomarker values, temperature values, etc.. This variable is related to the probability of getting the value by the probability distribution. Before we get to probability distributions we will talk a bit more about variables, specifically random variables!</p> <p> </p>"},{"location":"stats/distributions/#random-variables","title":"Random Variables","text":"<p>Random variables are some quantity derived from a random process. Think about drawing some random value from a bag of possible values. That set of possible values can change depending on what you are measuring:</p> <ul> <li>If you are measuring a variable that exists on a continuous spectrum it is called a continuous random variable. Think rainfall, where the values are continous (1,1.1, 1.005,2, etc.)</li> <li>If you are measure a variable that is limited to integers, the variable is called a discrete random variable. Think of count data like number of shoes, where you cannot have values in between values (2,4,5, etc.)</li> <li>If you are measuring a variable that is limited to true or false values, the variable is called a boolean random variable. Think of marital status, where the values are limited to yes/no or true/false (True, False)</li> </ul> <p> </p>"},{"location":"stats/distributions/#probability-distributions","title":"Probability Distributions","text":"<p>For each value in a random variable there is some probability of getting that value. A set of these probabilities is known as the probability distribution! These probablities can be calculated using different functions depending on your variable type. Broadly speaking, there are two kinds of probability fucntions:</p> <ul> <li>Probability Mass Function - What is the probabilty of getting that exact value?</li> <li>Cumulative Distribution Function - What is the probability of getting a value less than or equal to that value?</li> </ul> <p> </p>"},{"location":"stats/distributions/#references","title":"References","text":"<ol> <li>A Gentle Introduction to Probability Distributions</li> <li>Connecting the CDF and the PDF</li> </ol>"},{"location":"stats/fisher-test/","title":"Fisher's Exact Test","text":""},{"location":"stats/fisher-test/#fishers-exact-test","title":"Fisher's Exact Test","text":"<p>In the odds ratio topic note we noted that the odds ratio could help us determine whether the odds  were greater in one group versus another. We can test the strength of association of the group and event with Fisher's Exact Test.  Fisher's Exact Test has the following hypotheses:</p> <ul> <li>\\(H_0\\) or null hypothesis: there is no association between the group and event (Odds ratio = 1)</li> <li>\\(H_a\\) or alternative hypothesis: there is an association between the group and event (Odds ratio != 1)</li> </ul> <p>We can calculate the probability given the following contingency table with:</p> group 1 group 2 Event a b No Event c d \\[p = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!(a+b+c+d)!}\\] <p>Explanation of Terms</p> <ul> <li>\\(a\\) number of members in group 1 with event</li> <li>\\(b\\) number of members in group 2 with event</li> <li>\\(c\\) number of members in group 1 without event</li> <li>\\(d\\) number of members in group 2 without event</li> </ul> <p>Let's assess the relationship between pathiet sex and losing patients to follow up:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n# What is the frequency distribution of losing males/females to follow up\ntable &lt;- as.data.frame.matrix(\ntable(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n)\n\ntable\n</code></pre> <pre><code>       No Yes\nFemale 33  10\nMale   44  10\n</code></pre> <p>Before we continue, we need to make this table match the contingency table above. With the rows being the event and the columns being the group:</p> <pre><code># Reorder so that we are assessing the odds ratio of losing patients to follow up\ntable &lt;- as.data.frame.matrix(\ntable(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n) %&gt;%\nselect(c(Yes,No)) %&gt;%\nt()\n\ntable\n</code></pre> <pre><code>    Female Male\nYes     10   10\nNo      33   44\n</code></pre> <p>Now let's conduct our hypothesis test:</p> <pre><code>#apply our test\nfisher.test(table)\n</code></pre> <pre><code>    Fisher's Exact Test for Count Data\n\ndata:  table\np-value = 0.6191\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.4395952 4.0274603\nsample estimates:\nodds ratio \n   1.32933 \n</code></pre> <p>Explanation of Results</p> <p>Here we note:</p> <ul> <li>our p-value is above 0.05 and thus not strong enough to reject the null (a.k.a the true odds ratio is equal to 1)</li> <li>the 95% confidence interval reveals that our true odds ratio is somewhere between <code>0.4395952</code> and <code>4.0274603</code></li> <li>our odds ratio that patient sex is associated with losing the patient to follow up is about <code>1.3</code></li> </ul>"},{"location":"stats/fisher-test/#references","title":"References","text":"<ol> <li>BIOL 202</li> <li>Fisher's Exact Test</li> </ol>"},{"location":"stats/odds-ratio-risk/","title":"Risk/Odds Ratio","text":""},{"location":"stats/odds-ratio-risk/#risk","title":"Risk","text":"<p>We have defined odds as the probability of that event happening divided by the probability of that event not happening.  In our topic note on odds we asked what were the odds of losing a male patient to follow up. This is different from risk. Risk can be defined as:</p> \\[Risk = \\frac{n_i}{N}\\] <p>Explanation of Terms</p> <ul> <li>\\(n_i\\) number of times event happened</li> <li>\\(N\\) total number of events</li> </ul> <p>Here we see this is just a proportion or the number of times an event happened divided by the total number of events! Let's assess the risk of  losing a male patient to follow up:</p> <pre><code>#risk\nlibrary(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n# what are the odds of being a female non-smoker in our dataset?\ntable &lt;- as.data.frame.matrix(\ntable(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n) %&gt;%\nmutate(row_totals = apply(.,1,sum)) %&gt;%\nrbind(t(data.frame(column_totals=apply(., 2, sum))))\n\ntable\n\n\nmale &lt;- table %&gt;%\nfilter(rownames(.) == \"Male\") %&gt;%\nmutate(\nrisk_male_lost = Yes/row_totals,\nrisk_male_not_lost = No/row_totals)\n\nmale\n</code></pre> <pre><code>     No Yes row_totals risk_male_lost risk_male_not_lost\nMale 44  10         54      0.1851852          0.8148148\n</code></pre> <p>We see that the risk of losing a male patient to follow up is about 18%. </p>"},{"location":"stats/odds-ratio-risk/#relative-risk","title":"Relative Risk","text":"<p>You might hear about two risks how relate to each other - also called the relative risk. Relative risk can be calculated by:</p> \\[Relative Risk = \\frac{Risk_i}{Risk_j}\\] <p>Where :</p> \\[Risk_i = \\frac{n_i}{N_i}\\] \\[Risk_j = \\frac{n_j}{N_j}\\] <p>Explanation of Terms</p> <ul> <li>\\(n_i\\) number of times event happens in group i</li> <li>\\(N_i\\) total number of group i members</li> <li>\\(n_j\\) number of times event happens in group j</li> <li>\\(N_j\\) total number of group j members</li> </ul> <p>We will calculate the relative risk of losing a female patient to follow up versus a male patient. However, before we do so we will visualize  our data:</p> <pre><code># visualize risks:\n\nas.data.frame.matrix(\ntable(meta$SEX,meta$LOST_TO_FOLLOW_UP)) %&gt;%\nt() %&gt;%\nreshape2::melt() %&gt;%\nggplot(.,aes(x=Var2,y=value,fill=Var1))+\ngeom_bar(stat = \"identity\",position=\"fill\") +\nscale_y_continuous(labels = scales::percent)+\ntheme_bw()+\nscale_fill_manual(values=c(\"aquamarine3\",\"lightpink3\")) +\nlabs(\nx=\"\",\ny=\"Frequency\",\nfill=\"Lost To Follow Up?\"\n)\n</code></pre> <p></p> <p>Here we see that females are slighly more prone to being lost to follow up. How does this translate to relative risk?</p> <pre><code>risks &lt;- table %&gt;%\nfilter(rownames(.) != \"column_totals\") %&gt;%\nmutate(\nrisk_lost = Yes/row_totals,\nrisk_not_lost = No/row_totals)\n\nrisks\n</code></pre> <pre><code>       No Yes row_totals risk_lost risk_not_lost\nFemale 33  10         43 0.2325581     0.7674419\nMale   44  10         54 0.1851852     0.8148148\n</code></pre> <p>Here we can eyeball that the risk of losing a female patient to follow up is greater than losing a  male patient. Let's see what the relative risk is:</p> <pre><code>relative_risk &lt;- risks$risk_lost[\"Female\"]/risks$risk_lost[\"Male\"]\n\nrelative_risk\n</code></pre> <pre><code>  Female \n1.255814 \n</code></pre> <p>Here we can guage from the relative risk that being a female patient increases the risk of losing the patient to follow up. </p>"},{"location":"stats/odds-ratio-risk/#odds-ratio","title":"Odds Ratio","text":"<p>The odds ratio of two events is also used as a measure to compare events between groups. However, the odds ratio is the ratio of odds of an event in one group versus another and is defined by:</p> \\[Odds\\ Ratio = \\frac{Odds_i}{Odds_j}\\] <p>Where:</p> \\[Odds_i = \\frac{n_i}{n_{not\\ i}}\\] \\[Odds_j = \\frac{n_j}{n_{not\\ j}}\\] <p>Explanation of Terms</p> <ul> <li>\\(Odds_i\\) odds event happens in group i</li> <li>\\(Odds_j\\) odds event happens in group j</li> <li>\\(n_i\\) number of times event happens in group i</li> <li>\\(n_{not\\ i}\\) number of times event doesn't happen in group i</li> <li>\\(n_j\\) number of times event happens in group j</li> <li>\\(n_{not\\ j}\\) number of times event doesn't happen in group j</li> </ul> <p>So let's calculate the odds ratio of losing a female patients to follow up, versus male patients to follow up:</p> <pre><code>odds &lt;- table %&gt;%\nfilter(rownames(.) != \"column_totals\") %&gt;%\nmutate(\nodds = Yes/No)\nodds\n</code></pre> <pre><code>       No Yes row_totals odds_lost\nFemale 33  10         43 0.3030303\nMale   44  10         54 0.2272727\n</code></pre> <p>Here we note that the odds of losing a female patient to follow up are higher than the odds of losing a female. The odds ratio would then be:</p> <pre><code>odds_ratio &lt;- odds$odds[rownames(odds)==\"Female\"]/odds$odds[rownames(odds)==\"Male\"]\nodds_ratio\n</code></pre> <pre><code>[1] 1.333333\n</code></pre> <p>Here we note that the odds of losing a female patient versus a male patient to follow up are 1.3 to 1. We can see that the odds ratio is similar to the relative risk, but the values are nonetheless different. </p> <p>Relative Risk/Odds Ratio Interpretation</p> <ul> <li>Odds Ratio/Relative Risk = 1 the factor does not affect the event</li> <li>Odds Ratio/Relative Risk &lt; 1 the factor decreases the risk of the event (protective factor)</li> <li>Odds Ratio/Relative Risk &gt; 1 the factor increases the risk of the event (risk factor)</li> </ul> <ol> <li>Relative Risk</li> <li>Odds Ratio</li> <li>BIOL 202</li> <li>Common pitfalls in statistical analysis: Odds versus risk</li> </ol>"},{"location":"stats/odds/","title":"Odds","text":"<p>We often hear about the odds of something happening, but what does this mean? Well, the odds would be:</p> \\[O = \\frac{p}{1-p}\\] <p>Explanation of Terms</p> <ul> <li>\\(O\\) the odds of something happening</li> <li>\\(p\\) the probability that thing happens</li> </ul> <p>So colloquially, we see that the odds of an event is the probability of that event happening divided by the  probability of that event not happening. We are going to assess something called lost to follow up. In the setting of a clinical  trial this term describes losing patients due to various reasons. Let's try using our data to calculate the odds of losing a male patient to  follow up:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n# what are the odds of being a female non-smoker in our dataset?\ntable = as.data.frame.matrix(\ntable(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n) %&gt;%\nmutate(row_totals = apply(.,1,sum)) %&gt;%\nrbind(t(data.frame(column_totals=apply(., 2, sum))))\n\ntable\n</code></pre> <pre><code>              No Yes row_totals\nFemale        33  10         43\nMale          44  10         54\ncolumn_totals 77  20         97\n</code></pre> <p>Here we can have created what is called a contingency table or table that describes the frequency distribution of variables. We see that more patients are not lost to follow up. Let's calculate the odds now!</p> <pre><code>male &lt;- table %&gt;%\nfilter(rownames(.) == \"Male\") %&gt;%\nmutate(\nprob_male_lost = Yes/row_totals,\nprob_male_not_lost = No/row_totals,\nodds_male_lost = prob_male_lost/(1-prob_male_lost),\nodds_male_not_lost = prob_male_not_lost/(1-prob_male_not_lost))\n\nmale\n</code></pre> <pre><code>     No Yes row_totals prob_male_lost prob_male_not_lost odds_male_lost odds_male_not_lost\nMale 44  10         54      0.1851852          0.8148148      0.2272727                4.4\n</code></pre> <p>So here we see that the odds of losing a male to follow up are 0.23 to 1. An alternative way of looking at this is that the odds of not losing a male to follow up are 4.4 to 1.</p>"},{"location":"stats/odds/#references","title":"References","text":"<ol> <li>BIOL - 202: Analyzing a single categorical variable</li> <li>Lost To Follow Up</li> <li>Contingency Table</li> </ol>"},{"location":"stats/one-t-test/","title":"One Sample T-Test","text":""},{"location":"stats/one-t-test/#one-sample-t-test","title":"One-Sample T-Test","text":"<p>When we deal with numeric variables we often examine the mean of that variable. The one-sample t-test can help answer the following questions about our  numeric variable:</p> <ul> <li>Does the mean of our sample, \\(\\mu\\), equal the theoretical mean of our population, \\(\\mu_0\\)?<ul> <li>\\(H_0: \\mu = \\mu_0\\)</li> <li>\\(H_a: \\mu \\neq \\mu_0\\)</li> </ul> </li> <li>Is the mean of our sample, \\(\\mu\\), less than the theoretical mean of our population, \\(\\mu_0\\)?<ul> <li>\\(H_0: \\mu \\le \\mu_0\\)</li> <li>\\(H_a: \\mu &gt; \\mu_0\\)</li> </ul> </li> <li>Is the mean of our sample, \\(\\mu\\), greater than the theoretical mean of our population, \\(\\mu_0\\)?<ul> <li>\\(H_0: \\mu \\ge \\mu_0\\)</li> <li>\\(H_a: \\mu &lt; \\mu_0\\)</li> </ul> </li> </ul> <p>Tip</p> <p>When we ask if the sample mean is equal to the population mean we are conducting a two-sided test. When we ask if the sample mean is less than  or greater than the population mean, we are conducting a one-sided test.</p>"},{"location":"stats/one-t-test/#test-statistic","title":"Test Statistic","text":"<p>Our one-sample t-test statistic can be calculated by:</p> \\[t = \\frac{\\mu - \\mu_0}{\\sigma / \\sqrt{n}} \\] \\[d.f. = n - 1\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu\\) : sample mean</li> <li>\\(\\mu_0\\) : theoretical population mean</li> <li>\\(\\sigma\\) : standard deviation of our sample</li> <li>\\(n\\) : sample size</li> <li>\\(d.f.\\) : degrees of freedom</li> </ul>"},{"location":"stats/one-t-test/#normal-distribution","title":"Normal Distribution","text":"<p>Using our glioblastoma data, we are going to ask: Does the mean age of our patients equal the theoretical mean of the U.S. population (Let's say the avearage age is 32)? Before we do so, we need to ask; what probability function are we comparing our test statistic to? For a numeric variable we often compare our test statistic to a Gaussian or normal distribution. The probability density function for the normal distribution has the following formula:</p> \\[f(x) = \\frac{1}{(\\sigma\\sqrt{2 \\pi})} e^{-(\\frac{(x - \\mu)^2}{2 \\sigma^2})}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) : standard deviation</li> <li>\\(\\mu\\) : mean</li> </ul>"},{"location":"stats/one-t-test/#confidence-interval","title":"Confidence Interval","text":"<p>Just like our proportion tests, we also have a confidence interval around our sample parameter, in this case the sample mean. So for a test statistic, \\(t\\), at an \\(\\alpha\\) level of 0.05, our confidence interval would be:</p> \\[\\mu \\pm t \\frac{\\sigma}{\\sqrt{n}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu\\) : sample mean</li> <li>\\(t\\) : test statistic for an \\(\\alpha\\) of 0.05</li> <li>\\(\\sigma\\) : sample standard deviation</li> <li>\\(n\\) :  sample size</li> </ul>"},{"location":"stats/one-t-test/#running-the-one-sample-t-test","title":"Running the One-Sample T-Test","text":"<p>Putting this all together let's test whether or not the mean of our sample is equal to the theoretical mean of our population, 32:</p> <pre><code>#one-sample t-test\nlibrary(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n# run the one-sample t-test to determine if the\n# mean of our sample is equal to 32\nt.test(x = meta$AGE,\nmu = 32,\nalternative = \"two.sided\")\n</code></pre> <pre><code>    One Sample t-test\n\ndata:  meta$AGE\nt = 20.621, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 32\n95 percent confidence interval:\n 55.39750 60.38028\nsample estimates:\nmean of x \n 57.88889 \n</code></pre> <p>Explanation</p> <ul> <li>our test statistic is <code>20.621</code></li> <li>our d.f. is <code>98</code></li> <li>The pvalue is below <code>2.2e-16</code></li> <li>our alternative hypothesis is that the true mean is not equal to 32</li> <li>our sample mean is <code>57.88889</code> </li> <li>the 95% confidence interval for our mean is <code>55.39750</code> to <code>60.38028</code></li> <li>So we see that we have enough evidence to reject the null hypothesis that the true mean of our sample is equal to 32</li> </ul>"},{"location":"stats/one-t-test/#assumptions","title":"Assumptions","text":"<p>So now that we have conducted our test, we should assess the test's assumptions:</p> <ul> <li>the values are independent of one another</li> <li>the numeric variable is a continuous numeric variable</li> <li>there are no signficant outliers</li> <li>the data are normally distributed</li> </ul> <p>Our data are age, which is indeed a continuous variable. The data are also independent of one another (the age of our patients should not be dependent on the age of another patient). We can identify outliers using the rosner test in the <code>EnvStats</code> package. But to use this test we need to identify how many outliers we think there are. Let's estimate this visually by plotting our data:</p> <pre><code># plot our data and see if we notice any outliers\nggplot(meta, aes(x=AGE)) +\ngeom_histogram(fill =\"lightpink\") +\ntheme_bw()\n</code></pre> <p></p> <p>Given we don't see any drastic outliers let's say we might have 5 outliers:</p> <pre><code># run the rosner test and identify whether or not\n# we do have outliers\nlibrary(EnvStats)\nros.test &lt;- rosnerTest(x = meta$AGE, k = 5)\nros.test$all.stats\n</code></pre> <pre><code>  i   Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier\n1 0 57.88889 12.49154    24      86 2.712947   3.380651   FALSE\n2 1 58.23469 12.07007    88      81 2.466042   3.377176   FALSE\n3 2 57.92784 11.74224    30      99 2.378408   3.373658   FALSE\n4 3 58.21875 11.44709    31      92 2.377788   3.370097   FALSE\n5 4 58.50526 11.15641    34      34 2.196519   3.366490   FALSE\n</code></pre> <p>Here we see that the last column in the output above indicates that our 5 possible outliers are probably not outliers! Now let's check if our data are normally distributed using the Shapiro-Wilk Test:</p> <pre><code># run the shapiro-wilk test on our data\nshapiro.test(meta$AGE)\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  meta$AGE\nW = 0.98784, p-value = 0.5038\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed.</p>"},{"location":"stats/one-t-test/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the one-sample Wilcoxon signed rank test:</p> <pre><code># run the non-parametric alternative to the one-sample\n# t-test the one sample Wilcoxon signed rank test\nwilcox.test(meta$AGE,\nmu = 32,\nalternative = \"two.sided\")\n</code></pre> <pre><code>    Wilcoxon signed rank test with continuity correction\n\ndata:  meta$AGE\nV = 4936, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 32\n</code></pre>"},{"location":"stats/one-t-test/#references","title":"References","text":"<ol> <li>BIOL 202 - One-Sample T-Test</li> <li>One-Sample T-test in R</li> <li>Normal Distribution</li> <li>One-Sample T-Test using SPSS Statistics</li> <li>Nonparametric statistics</li> <li>One-Sample Wilcoxon Signed Rank Test in R</li> </ol>"},{"location":"stats/one-way-anova/","title":"One-Way ANOVA","text":""},{"location":"stats/one-way-anova/#one-way-anova-hypothesis","title":"One-Way ANOVA Hypothesis","text":"<p>In the paired t-test topic note, and the two-sample t-test topic note we discussed how to compare the means between two groups. What if you'd like to compare the means of two or more groups? This problem can be solved using a one-way ANOVA. The hypothesis of the One-Way ANOVA states:</p> <ul> <li>\\(H_0\\) : there is no difference between the means of each group</li> <li>\\(H_a\\) : One or more of the group means is different from the other group means</li> </ul>"},{"location":"stats/one-way-anova/#between-group-sum-of-squares","title":"Between Group Sum of Squares","text":"<p>To determine this difference we first calculate the between group sum of squares:</p> \\[ SS_{between} =  \\sum_a^z{n_a(x_a - \\overline{X})} \\] <p>Explanation of Terms</p> <ul> <li>\\(SS_between\\) : between group sum of squares</li> <li>\\(a\\) : group a</li> <li>\\(z\\) : last group</li> <li>\\(x_a\\) : mean of group a</li> <li>\\(n_a\\) : sample size of group a</li> <li>\\(\\overline{X}\\) : mean of all groups</li> </ul> <p>So here we see that for each group we will subtract the total mean from the group mean and multiply that value by sample size of that group. We then take  all those values for each group and add them together to get the sum of squares. </p>"},{"location":"stats/one-way-anova/#within-group-sum-of-squares","title":"Within Group Sum of Squares","text":"<p>Now we will need to calculate the within group sum of squares:</p> \\[ SS_{within} = \\sum_a^z{(x_{ia} - \\overline{x_a})}\\] <p>Explanation of Terms</p> <ul> <li>\\(SS_{within}\\) : within group sum of squares</li> <li>\\(a\\) : group a</li> <li>\\(z\\) : last group</li> <li>\\(x_ia\\) : value i in group a</li> <li>\\(\\overline{x_a}\\) : mean of group a</li> </ul> <p>Here we note that for each group we sum the difference from the mean for each value and add them together. Then when we are done adding these differences  for each group we add those values together to get the within group sum of squares.</p>"},{"location":"stats/one-way-anova/#one-way-anova-f-statistic","title":"One-Way ANOVA F-Statistic","text":"<p>Now to get our F-statistic we need to calculate the between group mean square value and the within group mean square value:</p> \\[ MS_{between} = \\frac{SS_{between}}{k - 1}\\] \\[ MS_{within} = \\frac{SS_{within}}{t - k}\\] \\[ F = \\frac{MS_{between}}{MS_{within}}\\] <p>Explanation of Terms</p> <ul> <li>\\(F\\) : F-statistic</li> <li>\\(MS_{between}\\) : between group mean square value</li> <li>\\(MS_{within}\\) : within group mean square value</li> <li>\\(k\\) : number of groups</li> <li>\\(t\\) : total number of observations between all groups</li> <li>\\(SS_{between}\\) : between group sum of squares</li> </ul>"},{"location":"stats/one-way-anova/#visualizing-our-data","title":"Visualizing our data","text":"<p>As you can see this is pretty laborious. Luckily, R can do these calculations for us and we will use R to determine if there is any significant  difference in age between the patient's country of origin. Here we will filter out countries that have less than 5 observations. First let's try a  visual inspection of our data:</p> <pre><code># one-way ANOVA\nlibrary(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n# isolate countries and ages\ncountries_ages &lt;- meta %&gt;%\nfilter(COUNTRY_OF_ORIGIN %in% c(\"China\",\"Poland\",\"Russia\",\"United States\"))\n\n# plot our data\nggplot(countries_ages,aes(x=COUNTRY_OF_ORIGIN,y=AGE,fill=COUNTRY_OF_ORIGIN)) +\ngeom_boxplot()+\ntheme_bw()+\nlabs(\nx=\"\",\ny=\"AGE\",\nfill=\"Country of Origin\"\n)\n</code></pre> <p></p> <p>Here we can visually see that the mean age of patients from China is lower than the other groups.</p>"},{"location":"stats/one-way-anova/#one-way-anova-in-r","title":"One-Way ANOVA in R","text":"<p>Let's now apply a One-Way ANOVA in R and output the summary of results:</p> <pre><code># calculate our one-way ANOVA\nanova_res &lt;- aov(AGE ~ COUNTRY_OF_ORIGIN, data = countries_ages)\n\n# print out a summary of results\nsummary(anova_res)\n</code></pre> <pre><code>                  Df Sum Sq Mean Sq F value Pr(&gt;F)  \nCOUNTRY_OF_ORIGIN  3   1583   527.6   3.824 0.0128 * \nResiduals         84  11591   138.0                 \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n</code></pre> <p>Explanation of Results</p> <ul> <li>Here we see that our between sum of squares is <code>1583</code> and our within sum of squares is <code>11591</code></li> <li>We also note that our between group mean square value is <code>527.6</code> and our within mean square valueis <code>138.0</code></li> <li>By dividing our between group mean square value by the within mean square value we get an F-statistic of <code>3.824</code></li> <li>We notice a p-value of <code>0.0128</code>, below 0.05, giving us enough evidence to reject the null:<ul> <li>that there is not a difference between group means</li> </ul> </li> </ul>"},{"location":"stats/one-way-anova/#assumptions","title":"Assumptions","text":"<p>Like any statistical test, we make assumptions. The assumptions made by the one-way ANOVA are:</p> <ul> <li>The residuls are normally distributed</li> <li>The variances of each group are equal (Homoscedasticity)</li> </ul> <p>Note</p> <p>The residuals for a One-Way ANOVA are calculated by taking each value and subtracting the mean of the group that value belongs to</p> <p>To test the normality of the residuals we will use the Shapiro-Wilk test:</p> <pre><code># use the shapiro-wilk test to determine the normality of the residuals\nshapiro.test(x = residuals(anova_res))\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  residuals(anova_res)\nW = 0.98732, p-value = 0.5527\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed. To determine the homoscedasticity of our data we will use the Levene test:</p> <pre><code># apply the levene test to our data\nlibrary(car)\nleveneTest(AGE ~ COUNTRY_OF_ORIGIN, data = countries_ages)\n</code></pre> <pre><code>Levene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.1514 0.9285\n      84  \n</code></pre> <p>Here we note that the p-value is above 0.05, indicating we don't have enough evidence to say that there is a significant difference in the variances between groups.</p>"},{"location":"stats/one-way-anova/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the Kruskal-Wallis rank sum test:</p> <pre><code># run the non-parametric alternative to the one-way ANOVA\n# the Kruskal-Wallis rank sum test\nkruskal.test(AGE ~ COUNTRY_OF_ORIGIN, data = countries_ages)\n</code></pre> <pre><code>    Kruskal-Wallis rank sum test\n\ndata:  AGE by COUNTRY_OF_ORIGIN\nKruskal-Wallis chi-squared = 9.1676, df = 3, p-value = 0.02714\n</code></pre>"},{"location":"stats/one-way-anova/#references","title":"References","text":"<ol> <li>One-Way ANOVA Test in R</li> <li>How to Perform a One-Way ANOVA by Hand</li> <li>Nonparametric statistics</li> </ol>"},{"location":"stats/paired-t-test/","title":"Paired T-Test","text":""},{"location":"stats/paired-t-test/#paired-t-test","title":"Paired T-Test","text":"<p>We have seen how to compare our sample mean to a theory of the underlying population. Here we will ask how do we compare the means of paired observations? We can use the paired t-test to answer any one of the following questions:</p> <ul> <li>Does the mean difference, \\(m\\), equal to 0?<ul> <li>\\(H_0: m = 0\\)</li> <li>\\(H_a: m \\neq 0\\)</li> </ul> </li> <li>Is the mean difference, \\(m\\), less than 0?<ul> <li>\\(H_0: m \\le 0\\)</li> <li>\\(H_a: m &gt; 0\\)</li> </ul> </li> <li>Is the mean difference, \\(m\\), greater than 0?<ul> <li>\\(H_0: m \\ge 0\\)</li> <li>\\(H_a: m &lt; 0\\)</li> </ul> </li> </ul> <p>Tip</p> <p>When we ask if the mean difference is equal to 0 we are conducting a two-sided test. When we ask if the sample mean is less than  or greater than 0, we are conducting a one-sided test.</p>"},{"location":"stats/paired-t-test/#test-statistic","title":"Test Statistic","text":"<p>Our paired t-test statistic can be calculated by:</p> \\[t = \\frac{m}{\\sigma / \\sqrt{n}} \\] \\[d.f. = n - 1\\] <p>Explanation of Terms</p> <ul> <li>\\(m\\) : mean difference</li> <li>\\(\\sigma\\) : standard deviation of our sample</li> <li>\\(n\\) : sample size</li> <li>\\(d.f.\\) : degrees of freedom</li> </ul>"},{"location":"stats/paired-t-test/#normal-distribution","title":"Normal Distribution","text":"<p>Just like in the one-sample t-test topic note we will be comparing our test statistic to the normal distribution with a  probability density function of:</p> \\[f(x) = \\frac{1}{(\\sigma\\sqrt{2 \\pi})} e^{-(\\frac{(x - \\mu)^2}{2 \\sigma^2})}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) : standard deviation</li> <li>\\(\\mu\\) : mean</li> </ul>"},{"location":"stats/paired-t-test/#confidence-interval","title":"Confidence Interval","text":"<p>Similarily to our one-sample t-test topic note our confidence interval will be defined as:</p> \\[\\mu \\pm t \\frac{\\sigma}{\\sqrt{n}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu\\) : sample mean difference</li> <li>\\(t\\) : test statistic for an \\(\\alpha\\) of 0.05</li> <li>\\(\\sigma\\) : standard deviation of the differences</li> <li>\\(n\\) :  sample size</li> </ul>"},{"location":"stats/paired-t-test/#running-the-paired-t-test","title":"Running the Paired T-Test","text":"<p>Seeing as our glioblastoma data does not have a paired observation (essentially a before-after comparison), we are going to need to generate some  dummy data to apply our paired-test to:</p> <pre><code># paired t-test\nlibrary(tidyverse)\n# create our dummy data\ndf &lt;- data.frame(\ncholesterol = c(sample(70:120,20),sample(110:220,20)),\ngroup = c(rep(\"before\",20),rep(\"after\",20))\n)\n\n# what is mean of each group\ndf %&gt;%\ngroup_by(group) %&gt;%\nsummarise(\nn=n(),\nstandard_deviation=sd(cholesterol),\nmean=mean(cholesterol)\n)\n</code></pre> <pre><code># A tibble: 2 \u00d7 4\n  group      n standard_deviation  mean\n  &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 after     20               32.4 160. \n2 before    20               14.2  94.8\n</code></pre> <p>Here we note that the before group has much lower cholesterol levels than the after group. Let's now apply the paired t-test to determine the significance of this difference. We will do this by asking is the difference between groups equal 0?</p> <pre><code># run the paired t-test to determine if the \n# difference between groups is equal to 0\nt.test(\ncholesterol ~ group, data = df, paired = TRUE)\n</code></pre> <pre><code>    Paired t-test\n\ndata:  cholesterol by group\nt = 8.6603, df = 19, p-value = 5.061e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 49.59408 81.20592\nsample estimates:\nmean of the differences \n                   65.4\n</code></pre> <p>Explanation</p> <ul> <li>our test statistic is <code>8.6603</code></li> <li>The pvalue is <code>5.061e-08</code></li> <li>our alternative hypothesis is that the true difference in means is not equal to 0</li> <li>our mean of the differences is <code>65.4</code></li> <li>the 95% confidence interval for our mean of differences is <code>49.59408</code> to <code>81.20592</code></li> <li>So we see that we have enough evidence to reject the null hypothesis that the true difference in means is equal to 0</li> </ul>"},{"location":"stats/paired-t-test/#assumptions","title":"Assumptions","text":"<p>So now that we have conducted our test, we should assess the test's assumptions:</p> <ul> <li>the values are paired</li> <li>the differences of the pairs are normally distributed</li> </ul> <p>The values in our dummy data are intended to be paired. Meaning that the first observation in the before group is the same patient as the first observation in the after group. Now to check if the differences of the pairs are normally distributed we can use the Shapiro-Wilk test like we did in the  one-sample t-test topic note:</p> <pre><code># differences betweeen the paired observations\ndifference &lt;- df$cholesterol[df$group == \"before\"] - df$cholesterol[df$group == \"after\"]\n\n# Shapiro-Wilk test to determine are they normally distributed\nshapiro.test(difference) </code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  difference\nW = 0.94613, p-value = 0.3121\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed.</p>"},{"location":"stats/paired-t-test/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the Wilcoxon signed rank test:</p> <pre><code># run the non-parametric alternative to the paired\n# t-test the Wilcoxon signed rank test\nwilcox.test(cholesterol ~ group, data = df, paired = TRUE,\nalternative = \"two.sided\")\n</code></pre> <pre><code>    Wilcoxon signed rank exact test\n\ndata:  cholesterol by group\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n</code></pre>"},{"location":"stats/paired-t-test/#references","title":"References","text":"<ol> <li>BIOL 202 - Paired T-Test</li> <li>Paired Samples T-test in R</li> <li>Normal Distribution</li> <li>Paired Samples Wilcoxon Test in R</li> <li>Nonparametric statistics</li> </ol>"},{"location":"stats/qual/","title":"Qualitative Variables","text":"<p>Qualitative variables can be thought of as categories: so variables like eye color, gender, and race.  When assessing qualitative variables it is useful to consider proportions:</p> \\[\\frac{n_i}{N}\\] <p>Explanation of Terms</p> <ul> <li>\\(n_i\\) number in category of interest</li> <li>\\(N\\) total number of observations</li> </ul> <p>So let's calculate some proportions in R!</p> <pre><code>library(tidyverse)\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\ncountry_sum &lt;- meta %&gt;%\ncount(COUNTRY_OF_ORIGIN, sort = TRUE) %&gt;% mutate(proportion = n / sum(n)) %&gt;%\nmutate(COUNTRY_OF_ORIGIN = replace_na(COUNTRY_OF_ORIGIN,\"NA\"))\n\ncountry_sum\n</code></pre> <pre><code>  COUNTRY_OF_ORIGIN  n proportion\n1             China 30 0.30303030\n2     United States 21 0.21212121\n3            Russia 19 0.19191919\n4            Poland 18 0.18181818\n5              &lt;NA&gt;  6 0.06060606\n6          Bulgaria  2 0.02020202\n7           Croatia  1 0.01010101\n8            Mexico  1 0.01010101\n9       Phillipines  1 0.01010101\n</code></pre> <p>Note</p> <p>You'll see that we do have an <code>NA</code> value here and that it's proportion in our variable is counted too! Since the <code>NA</code> value in R has special properties we ensure it is a character and not an NA value using the <code>replace_na()</code> function. </p> <p>Qualitative variables can be visualized using a bar plot:</p> <pre><code>ggplot(country_sum, aes(x=proportion,y=reorder(COUNTRY_OF_ORIGIN,+proportion))) + geom_bar(fill=\"lightpink\",stat = \"identity\")+\ntheme_bw()+\nlabs(\nx=\"Proportion\",\ny=\"Country of Origin\",\ntitle=\"Country of Origin Barplot\"\n)\n</code></pre> <p></p> <p>Tip</p> <p>Here we ensure that we reorder our countries with the <code>reorder()</code> function as <code>ggplot2</code> will not order our data for us.</p>"},{"location":"stats/qual/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"stats/quant/","title":"Quantitative Variables","text":"<p>Quantitaive variables are numerical data: so variables such as height, weight, age. We can describe these variables with the following terms:</p> <p>Explanation of Terms</p> <ul> <li>Minimum : smallest value in your variable</li> <li>Median : middle value in your variable</li> <li>Mean : average value of your variable</li> <li>Max : largest value in your variable</li> <li>Count : how many values are in your variable</li> <li>Standard deviation : measure of the spread of your variable</li> </ul> <p>Let's see how to do this in our code:</p> <pre><code>library(tidyverse)\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\nheight.sum &lt;- meta %&gt;%\nsummarise( minimum = min(HEIGHT, na.rm = T),\nmedian = median(HEIGHT, na.rm = T),\nmean = mean(HEIGHT, na.rm = T),\nmax = max(HEIGHT, na.rm = T),\ncount = length(HEIGHT[!is.na(HEIGHT)]),\nSD = sd(HEIGHT, na.rm = T))\n\nheight.sum\n</code></pre> <pre><code>  minimum median     mean max count       SD\n1     150    170 169.6768 196    99 9.875581\n</code></pre> <p>Note</p> <p>You'll note here that we explicitly remove <code>NA</code> values to calculate these descriptive statistics.</p> <p>Now that we know how to calculate our descriptive statistics, let's try and visualize our numeric data:</p> <pre><code>ggplot(meta, aes(x=HEIGHT)) + geom_histogram(fill=\"lightpink\")+\ntheme_bw()+\nlabs(\nx=\"Height\",\ny=\"Frequency\",\ntitle=\" Histogram of Height\"\n)\n</code></pre> <p></p>"},{"location":"stats/quant/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"stats/sampling/","title":"Sampling","text":"<p>When we try to assess an underlying population we often take samples of that population. Let's try and take a sample using the <code>sample()</code> function in R:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\nheader = T,\nsep=\"\\t\")\n\n## defined some population of ages\nages &lt;- sample(meta$AGE,20)\n</code></pre> <p>If we wanted to take the same random sample we could use the <code>set.seed()</code> function:</p> <pre><code>## grab the same sample\nset.seed(123)\nages1 &lt;- sample(meta$AGE,20)\nages2 &lt;- sample(meta$AGE,20)\n</code></pre>"},{"location":"stats/sampling/#sampling-error","title":"Sampling Error","text":"<p>Not every sample is going to be a true approximation of the underline population. This difference is known as the sampling error. What's assess our sample and see how it stacks up against our population:</p> <pre><code>data.frame(\nSample_Mean=mean(ages,na.rm = T),\nPopulation_Mean=mean(meta$AGE,na.rm = T)\n)\n</code></pre> <pre><code>  Sample_Mean Population_Mean\n1        57.4        57.88889\n</code></pre> <p>Here we note that while similar to our true meta data mean, it is not exact. When we don't know the actual population mean we can get a whole range (or distribution) of means. The standard error of the mean is the measure of that sampling distribution:</p> \\[\\frac{\\sigma}{\\sqrt{N}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) Standard deviation of the sample</li> <li>\\(N\\) Number of observations in the sample</li> </ul> <p>Math Tip</p> <p>We can see that increasing the size of the sample, decreases the standard error of the mean.</p>"},{"location":"stats/sampling/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"stats/stats/","title":"Introduction To Biostatistics","text":""},{"location":"stats/stats/#variables-and-sampling","title":"Variables and Sampling","text":"<ul> <li>Quantitative Variables</li> <li>Qualitative Variables</li> <li>Sampling</li> <li>Confidence Intervals</li> <li>Probability Distributions</li> </ul>"},{"location":"stats/stats/#analyzing-one-categorial-variable","title":"Analyzing One Categorial Variable","text":"<ul> <li>Binomial Test</li> </ul>"},{"location":"stats/stats/#analyzing-two-categorical-variables","title":"Analyzing Two Categorical Variables","text":"<ul> <li>Odds</li> <li>Risk/Odds Ratio</li> <li>Fisher's Exact Test</li> <li>Chi-Square Test</li> </ul>"},{"location":"stats/stats/#analyzing-one-numeric-variable","title":"Analyzing One Numeric Variable","text":"<ul> <li>One Sample T-Test</li> </ul>"},{"location":"stats/stats/#analyzing-numeric-variable-with-two-groups","title":"Analyzing Numeric Variable With Two Groups","text":"<ul> <li>Paired T-Test</li> <li>Two Sample T-Test</li> </ul>"},{"location":"stats/stats/#analyzing-two-numeric-variables","title":"Analyzing Two Numeric Variables","text":"<ul> <li>Correlation</li> </ul>"},{"location":"stats/stats/#analyzing-two-or-more-groups","title":"Analyzing Two Or More Groups","text":"<ul> <li>One-Way ANOVA</li> </ul>"},{"location":"stats/two-t-test/","title":"Two Sample T-Test","text":""},{"location":"stats/two-t-test/#two-sample-t-test","title":"Two-Sample T-Test","text":"<p>In the paired t-test topic note we examined the difference in means between paired data. To compare the difference in means between two unpaired groups we will use the two-sample t-test. The two-sample t-test can be used to ask the following:</p> <ul> <li>Does the mean of group 1, \\(\\mu_1\\), equal the mean of our group 2, \\(\\mu_2\\)?<ul> <li>\\(H_0: \\mu_1 = \\mu_2\\)</li> <li>\\(H_a: \\mu_1 \\neq \\mu_2\\)</li> </ul> </li> <li>Is the mean of group 1, \\(\\mu_1\\), less than the mean of group 2, \\(\\mu_2\\)?<ul> <li>\\(H_0: \\mu_1 \\le \\mu_2\\)</li> <li>\\(H_a: \\mu_1 &gt; \\mu_2\\)</li> </ul> </li> <li>Is the mean of group 1, \\(\\mu_1\\), greater than the mean of group 2, \\(\\mu_2\\)?<ul> <li>\\(H_0: \\mu_1 \\ge \\mu_2\\)</li> <li>\\(H_a: \\mu_1 &lt; \\mu_2\\)</li> </ul> </li> </ul> <p>Tip</p> <p>When we ask if mean of group 1 is equal to the mean of group 2 we are conducting a two-sided test. When we ask if the mean of group 1 is less than or greater than the mean of group 2, we are conducting a one-sided test.</p>"},{"location":"stats/two-t-test/#test-statistic","title":"Test Statistic","text":"<p>Our one-sample t-test statistic can be calculated by:</p> \\[t = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\frac{\\sigma_1}{n_1} + \\frac{\\sigma_2}{n_2}}} \\] \\[d.f. = n_1 + n_2 - 2\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu_1\\) : mean of group 1</li> <li>\\(\\mu_2\\) : mean of group 2</li> <li>\\(\\sigma_1\\) : standard deviation of group 1</li> <li>\\(\\sigma_2\\) : standard deviation of group 2</li> <li>\\(n_1\\) : size of group 1</li> <li>\\(n_2\\) : size of group 2</li> <li>\\(d.f.\\) : degrees of freedom</li> </ul>"},{"location":"stats/two-t-test/#normal-distribution","title":"Normal Distribution","text":"<p>Just like in the one-sample t-test topic note we will be comparing our test statistic to the normal distribution with a probability density function of:</p> \\[f(x) = \\frac{1}{(\\sigma\\sqrt{2 \\pi})} e^{-(\\frac{(x - \\mu)^2}{2 \\sigma^2})}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) : standard deviation</li> <li>\\(\\mu\\) : mean</li> </ul>"},{"location":"stats/two-t-test/#confidence-interval","title":"Confidence Interval","text":"<p>Just like our proportion tests, we also have a confidence interval around our sample parameter, in this case the sample mean. So for a test statistic, \\(t\\), at an \\(\\alpha\\) level of 0.05, our confidence interval would be:</p> \\[(\\mu_1 - \\mu_2) \\pm t * \\sqrt{\\frac{\\sigma_1}{n_1} + \\frac{\\sigma_2}{n_2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu_1\\) : mean of group 1</li> <li>\\(\\mu_2\\) : mean of group 2</li> <li>\\(t\\) : test statistic for an \\(\\alpha\\) of 0.05</li> <li>\\(\\sigma_1\\) : standard deviation of group 1</li> <li>\\(\\sigma_2\\) : standard deviation of group 2</li> <li>\\(n_1\\) : size of group 1</li> <li>\\(n_2\\) : size of group 2</li> </ul>"},{"location":"stats/two-t-test/#running-the-two-sample-t-test","title":"Running the Two-Sample T-Test","text":"<p>Putting this all together let's ask: Is mean age of males equal to the mean age of females in our glioblastoma data? We will start by manually calculating the mean age and standard deviation of males and females:</p> <pre><code># what is mean of each group\nmeta %&gt;%\ngroup_by(SEX) %&gt;%\nsummarise(\nn=n(),\nstandard_deviation=sd(AGE),\nmean=mean(AGE)\n)\n</code></pre> <pre><code># A tibble: 2 \u00d7 4\n  SEX        n standard_deviation  mean\n  &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 Female    44               13.7  57.9\n2 Male      55               11.6  57.9\n</code></pre> <p>Remarkably we notice that the mean age of males and the mean age of females are the same! We will now use the two-sample t-test to ask if the mean age of males equals the mean age of females:</p> <pre><code># run the two-sample t-test to determine if the\n# mean age of males is equal to the mean\n# age of females\nt.test(\nAGE ~ SEX, data = meta, method = \"two.sided\",\nvar.equal = FALSE)\n</code></pre> <pre><code>    Welch Two Sample t-test\n\ndata:  AGE by SEX\nt = 0.014062, df = 84.559, p-value = 0.9888\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.105621  5.178348 \nsample estimates:\nmean in group Female   mean in group Male \n            57.90909             57.87273 \n</code></pre> <p>Explanation</p> <ul> <li>our test statistic is <code>0.014062</code></li> <li>The pvalue is below <code>0.9888</code></li> <li>our alternative hypothesis is that the true difference in means is not equal to 0</li> <li>the mean female is is <code>57.90909</code> and the mean male age is <code>57.87273</code></li> <li>the 95% confidence interval for our difference in means is <code>-5.105621</code> to <code>5.178348</code></li> <li>So we see that we do not have enough evidence to reject the null hypothesis that the true difference in means is equal to 0</li> </ul>"},{"location":"stats/two-t-test/#assumptions","title":"Assumptions","text":"<p>So now that we have conducted our test, we should assess the test's assumptions:</p> <ul> <li>the values are independent of one another</li> <li>the variances in each group are equal</li> <li>the data in each group normally distributed</li> </ul> <p>Here we note that our age values should be independent of one another (the age of one patient should not affect the age of another). Now for the assumption that variances of each group are equal - we avoided this by setting the <code>var.equal</code> argument equal to false. If we were to set it to true, we could check our variances with the F-Test:</p> <pre><code># use the f test to determine if the variance in \n# group 1 is the same as the variance in group 2\nvar.test(\nAGE ~ SEX, data = meta\n)\n</code></pre> <pre><code>    F test to compare two variances\n\ndata:  AGE by SEX\nF = 1.3849, num df = 43, denom df = 54, p-value = 0.2559\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.7879316 2.4804317\nsample estimates:\nratio of variances \n          1.384904 \n</code></pre> <p>Here we will just note that the p-value is above 0.05 and thus there isn't a significant difference in the variance of group 1 and the variance of group 2. Now to check if the data in each group are normally distributed we will use the Shapiro-Wilk test:</p> <pre><code># check the normality of male ages and female ages\nshapiro.test(meta$AGE[meta$SEX == \"Male\"])\nshapiro.test(meta$AGE[meta$SEX == \"Female\"])\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  meta$AGE[meta$SEX == \"Male\"]\nW = 0.98809, p-value = 0.8604\n\n    Shapiro-Wilk normality test\n\ndata:  meta$AGE[meta$SEX == \"Female\"]\nW = 0.95578, p-value = 0.09048\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed. However, it should be noted that the Shapiro-Wilk test p-value for female ages is rather close to 0.05 so there does seem to be some skey in female ages. </p>"},{"location":"stats/two-t-test/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the Wilcoxon signed rank test:</p> <pre><code># run the non-parametric alternative to the unpaired\n# t-test the Wilcoxon signed rank test\nwilcox.test(AGE ~ SEX, data = meta, alternative = \"two.sided\")\n</code></pre> <pre><code>    Wilcoxon rank sum test with continuity correction\n\ndata:  AGE by SEX\nW = 1278.5, p-value = 0.6318\nalternative hypothesis: true location shift is not equal to 0\n</code></pre>"},{"location":"stats/two-t-test/#references","title":"References","text":"<ol> <li>BIOL 202 - Two-Sample T-Test</li> <li>Unpaired Two-Samples T-test in R</li> <li>Normal Distribution</li> <li>2 sample test of mean</li> <li>Unpaired Two-Samples Wilcoxon Test in R</li> <li>Nonparametric statistics</li> </ol>"},{"location":"tools/all_hpc_tools/","title":"Available HPC Tools","text":""},{"location":"tools/all_hpc_tools/#available-tufts-hpc-tools","title":"Available Tufts HPC Tools","text":"<p>There are a number of modules available on the Tufts HPC. Here we link out to the NIH's documentation for these tools, which will provide descriptions and how to create a batch script to use that tool. </p> <p>Note about NIH HPC Documentation</p> <p>Note that this will not perfectly line up with how to use this tool on the HPC as we have different reservation, node, and partition names. However, it is a good start to get you going on how to use the tool in a batch script. </p> <p>For more information on how to create a batch script to work with the Tufts Cluster, check out the following tutorial: How to Create a Batch Script</p> Link To Documentation Versions ABySS https://hpc.nih.gov/apps/abyss.html ABySS/1.5.2 afni https://hpc.nih.gov/apps/afni.html afni/2014-01-28(default) amber https://hpc.nih.gov/apps/AMBER.html amber/12(default), amber/12.gpu, amber/14, amber/22-cuda, amber/22-mpi, amber/22-omp, amber/22-serial anaconda https://hpc.nih.gov/apps/python.html anaconda/2, anaconda/3, anaconda/2020.02, anaconda/2021.05, anaconda/2021.11, anaconda/bio35 ancestrymap https://hpc.nih.gov/apps/AncestryMap.html ancestrymap/6210 ant https://hpc.nih.gov/apps/ANTs.html ant/1.8.2 aspera https://hpc.nih.gov/docs/transfer.html aspera/3.5.4 ATLAS https://hpc.nih.gov/development/ATLAS.html ATLAS/3.9.78 autoconf https://hpc.nih.gov/development/autotools.html autoconf/2.69 bcftools https://hpc.nih.gov/apps/samtools.html bcftools/1.2, bcftools/1.12 bcl2fastq https://hpc.nih.gov/apps/bcl2fastq.html bcl2fastq/2.19 beagle https://hpc.nih.gov/apps/Beagle.html beagle/1.0-cuda, beagle/3.1.2 BEAST https://hpc.nih.gov/apps/BEAST.html BEAST/1.10.1(default), BEAST/1.10.4, BEAST/1.8.4 bedtools https://hpc.nih.gov/apps/bedtools.html bedtools/2.17.0, bedtools/2.19.1, bedtools/2.26.0(default) binutils https://hpc.nih.gov/apps/python.html binutils/2.29.1 blacs https://hpc.nih.gov/docs/svis.html blacs/1.1 BLAS https://hpc.nih.gov/apps/Blast.html BLAS/1.0, BLAS/3.5.0, OpenBLAS/0.2.19 blast https://hpc.nih.gov/apps/Blast.html blast/2.2.24, blast/2.2.31, blast/2.3.0, blast/2.8.1, blast-plus/2.11.0, ncbi-magicblast/1.5.0 blat https://hpc.nih.gov/apps/blat.html blat/20140708(default) bowtie https://hpc.nih.gov/apps/bowtie.html bowtie/0.12.7(default), bowtie/1.0.1, bowtie/1.1.2, bowtie/2.1.0, bowtie2/2.2.3(default) bowtie2 https://hpc.nih.gov/apps/bowtie2.html bowtie2/2.2.3(default) bwa https://hpc.nih.gov/apps/bwa.html bwa/0.7.17, bwa/0.7.9a caffe https://hpc.nih.gov/development/caffe.html caffe/20161026, caffe/20170201, caffe/20170227, caffe_unet/16.04.10.7, matcaffe/1.0 cellranger https://hpc.nih.gov/apps/cellranger.html cellranger/2.1.1, cellranger/3.0.2, cellranger-atac/1.2.0 chimera64 https://hpc.nih.gov/apps/Chimera.html chimera64/1.6.2, chimera64-osmesa/1.4.2577 chimera64-osmesa https://hpc.nih.gov/apps/Chimera.html chimera64-osmesa/1.4.2577 CirSeq https://hpc.nih.gov/apps/VIRTUS.html CirSeq/3 comsol https://hpc.nih.gov/apps/Comsol.html comsol/4.3b1, comsol/4.4, comsol/5.0, comsol/5.1, comsol/5.2(default), comsol/5.2a, comsol/5.3, comsol/5.3a, comsol/5.4, comsol/5.5, comsol/5.6, comsol/6.0 cuda https://hpc.nih.gov/docs/deep_learning.html beagle/1.0-cuda, cuda/10.0, cuda/10.2, cuda/11.0, cuda/4.2.9, cuda/5.0.35, cuda/6.5.14, cuda/7.5.18, cuda/8.0.44, cuda/9.0, amber/22-cuda, cuda/11.6, cuda/11.7 cudnn https://hpc.nih.gov/development/cuDNN.html cudnn/5.1, cudnn/7.1, cudnn/8.0.4-11.0 cufflinks https://hpc.nih.gov/apps/cufflinks.html cufflinks/0.8.3, cufflinks/2.0.0, cufflinks/2.0.2(default), cufflinks/2.1.1, cufflinks/2.2.1 Cytoscape https://hpc.nih.gov/apps/Cytoscape.html Cytoscape/2.8.3 eclipse https://hpc.nih.gov/apps/solar.html eclipse/1.4.1, eclipse/1.4.1-x86_64, eclipse/4.6.2-cpp, eclipse/4.6.2-java eigen https://hpc.nih.gov/apps/eigensoft.html eigen/3.2.2(default), eigensoft/3.0, eigensoft/4.2, eigen/3.4.0 eigensoft https://hpc.nih.gov/apps/eigensoft.html eigensoft/3.0, eigensoft/4.2 espresso https://hpc.nih.gov/apps/express.html espresso/3.1.1, espresso/3.3.1 exonerate https://hpc.nih.gov/apps/exonerate.html exonerate/2.2.0(default) fastqc https://hpc.nih.gov/apps/fastqc.html fastqc/0.11.5, fastqc/0.11.8, fastqc/0.11.9 fastx https://hpc.nih.gov/apps/fastxtoolkit.html fastx/0.0.13 ffmpeg https://hpc.nih.gov/apps/python.html ffmpeg/3.2.2 fftw https://hpc.nih.gov/apps/modules.html fftw/3.3.2(default), fftw/3.3.2-double, fftw/3.3.3, fftw/3.3.3-double, fftw/3.3.10 freebayes https://hpc.nih.gov/apps/freebayes.html freebayes/1.1.0 freesurfer https://hpc.nih.gov/apps/freesurfer.html freesurfer/5.3.0(default) fsl https://hpc.nih.gov/apps/fsl.html fsl/5.0.6(default), fsl/5.0.9, fsl/6.0.5 gamess https://hpc.nih.gov/apps/GAMESS.html gamess/2013-03-01 GATK https://hpc.nih.gov/apps/GATK.html GATK/3.1-1(default), GATK/3.7, GATK/4.2.6.1 gaussian https://hpc.nih.gov/apps/Gaussian.html gaussian/g09, gaussian/g16avx, gaussian/g16avx2, gaussian/g16sse4_2 gcc https://hpc.nih.gov/development/compilers.html gcc/4.7.0, gcc/4.9.2, gcc/5.3.0, gcc/7.3.0, gcc/9.3.0, gcc/8.4.0, netcdf/4.0.0-gcc, intel-oneapi-mkl/2021.1.1-gcc-9.3.0, lhapdf5/5.9.1-gcc-4.8.5, intel-oneapi-tbb/2021.1.1-gcc-9.3.0, log4cpp/1.1.3-gcc-4.8.5, perl/5.34.0_gcc7.3.0, pythia6/6.4.28-gcc-4.8.5 gcta https://hpc.nih.gov/apps/GCTA.html gcta/1.25.2 glibc https://hpc.nih.gov/apps/csd.html glibc/2.14 glog https://hpc.nih.gov/apps/python.html glog/0.3.4, glog/0.4.0 gnu-parallel https://hpc.nih.gov/apps/parallel.html gnu-parallel/20150122 alphafold https://hpc.nih.gov/apps/alphafold2.html alphafold/2.1.1, alphafold/2.2.0 annovar https://hpc.nih.gov/apps/ANNOVAR.html annovar/202106 aria2 https://hpc.nih.gov/apps/aria2.html aria2/1.35.0 automake https://hpc.nih.gov/development/autotools.html automake/1.16.3 biobakery https://hpc.nih.gov/apps/biobakery_workflows.html biobakery/workflows blast-plus https://hpc.nih.gov/apps/Blast.html blast-plus/2.11.0 blender https://hpc.nih.gov/apps/blender.html blender/3.4.1 cactus https://hpc.nih.gov/apps/cactus.html cactus/gpu cellprofiler https://hpc.nih.gov/apps/cellprofiler.html cellprofiler/3.1.9, cellprofiler/4.0.6, cellprofiler/4.1.3, cellprofiler/4.2.1 cellranger-atac https://hpc.nih.gov/apps/cellranger-atac.html cellranger-atac/1.2.0 chimera https://hpc.nih.gov/apps/Chimera.html chimera64/1.6.2, chimera64-osmesa/1.4.2577, chimera/1.15 condaenv https://hpc.nih.gov/apps/python.html condaenv/bracken ctffind https://hpc.nih.gov/apps/ctffind.html ctffind/4.1.8 dssp https://hpc.nih.gov/apps/DSSP.html dssp/3.1.4 emacs https://hpc.nih.gov/apps/Editors.html emacs/26.3, emacs/27.1_ccgpu, emacs/X26.3 eman2 https://hpc.nih.gov/apps/EMAN2.html eman2/2.91 fastjar https://hpc.nih.gov/apps/R.html fastjar/0.98 fasttree https://hpc.nih.gov/apps/FastTree.html fasttree/2.1.10 Fiji https://hpc.nih.gov/apps/Fiji.html Fiji/1.53t, Fiji/ImageJ flash https://hpc.nih.gov/systems/hardware.html flash/1.2.11 fontconfig https://hpc.nih.gov/apps/python.html fontconfig/2.13.94 freetype https://hpc.nih.gov/apps/python.html freetype/2.11.0 hail https://hpc.nih.gov/apps/Hail.html hail/202106 gstreamer https://hpc.nih.gov/apps/python.html gstreamer/0.10 imagej https://hpc.nih.gov/apps/Fiji.html imagej/1.52a, imagej/1.5.2 hdf5 https://hpc.nih.gov/apps/ont-fast5-api.html hdf5/1.10.4, hdf5/1.12.1 hmmer https://hpc.nih.gov/apps/hmmer.html hmmer/3.3, hmmer/3.1b2 htslib https://hpc.nih.gov/apps/samtools.html htslib/1.9, htslib/1.2.1 libuuid https://hpc.nih.gov/apps/python.html libuuid/1.0.3 gem https://hpc.nih.gov/apps/gem.html gem/1.4.3, imagemagick/7.1.0 harfbuzz https://hpc.nih.gov/apps/python.html harfbuzz/2.9.1, harfbuzz/5.1.0 intel https://hpc.nih.gov/systems/hardware.html intel/mpi_2021.5.0, intel/compilers_2022.0.1, intel-oneapi-mkl/2021.1.1-gcc-9.3.0, intel-oneapi-tbb/2021.1.1-gcc-9.3.0, intel/2013_sp1, intel/tbb433 lftp https://hpc.nih.gov/docs/transfer.html lftp/4.8.1 fribidi https://hpc.nih.gov/apps/python.html fribidi/1.0.5 lastz https://hpc.nih.gov/apps/LASTZ.html lastz/1.03 netcdf https://hpc.nih.gov/apps/python.html netcdf/4.0.0-gcc, netcdf/4.0.0-pgi, netcdf/4.0.0, netcdf/c-4.8.1, netcdf/fortran-4.5.2, netcdf/fortran-4.5.3, netcdf/4.7.4 java https://hpc.nih.gov/development/java.html eclipse/4.6.2-java, java/11.0.2, java/15.0.2, java/1.5.0_64bit, java/1.6.0_64bit, java/1.5.0, java/1.4.2, java/1.6.0, java/1.6.0_25, java/1.6.0_25_64bit, java/1.8.0_60, java/1.7.0_51, structure-java/2.3.4 lapack https://hpc.nih.gov/development/LAPACK.html lapack/3.9.0, lapack/3.4.0, lapack/3.2.1, lapack/3.5.0, scalapack/1.8.0 gromacs https://hpc.nih.gov/apps/Gromacs.html gromacs/4.6.5, gromacs/4.6.5d, gromacs/5.0.1, gromacs/4.5.5(default), gromacs/4.6.1, gromacs/4.6.5-gls, gromacs/4.6.1-slurm, gromacs/4.6.7, gromacs/4.6.7-gpu, gromacs/5.1.0, gromacs/5.1.4-gpu, gromacs-plumed-libmatheval/2014-06-17(default), gromacs/4.6.5+plumed, gromacs/5.1.4 matlab https://hpc.nih.gov/apps/Matlab.html matlab/2020a, matlab/2019a, matlab/2019b, matlab/2020b, matlab/2021a, matlab/2022a, matlab/2016a, matlab/2016b, matlab/2017b, matlab/2018a, matlab/2017a(default) mpfr https://hpc.nih.gov/apps/python.html mpfr/2.4.2(default), mpfr/3.1.2 julia https://hpc.nih.gov/apps/julia.html julia/1.6.0, julia/1.5.3, julia/0.3 gcam https://hpc.nih.gov/apps/gautomatch.html gcam/5.3, gcam/5.4 giflib https://hpc.nih.gov/apps/python.html giflib/5.1.4 gobject-introspection https://hpc.nih.gov/apps/python.html gobject-introspection/1.56.1 interproscan https://hpc.nih.gov/apps/interproscan.html interproscan/5.20 glib https://hpc.nih.gov/apps/python.html glibc/2.14, glib/2.56.1, glib/2.66.7 lammps https://hpc.nih.gov/apps/lammps.html lammps/20210310, lammps/27Aug13(default), lammps/10Aug15, lammps/16Mar18 libgmp https://hpc.nih.gov/apps/R.html libgmp/4.1.4 libicu https://hpc.nih.gov/apps/python.html libicu/4.2.1 imod https://hpc.nih.gov/apps/IMOD.html imod/4.11.18, imod/4.9.9, imod/4.9.12 ld_lib https://hpc.nih.gov/apps/modules.html ld_lib/2.2.21 libiconv https://hpc.nih.gov/apps/python.html libiconv/1.16 libtiff https://hpc.nih.gov/apps/python.html libtiff/4.2.0 gurobi https://hpc.nih.gov/apps/gurobi.html gurobi/9.5.0, gurobi/6.5.1, gurobi/7.5.1, gurobi/8.1.0 metal https://hpc.nih.gov/apps/metal.html metal/2010-02-08, metal/2011-03-25(default), metal/2010-08-01, metal/2009-10-10 mirdeep2 https://hpc.nih.gov/apps/mirdeep.html mirdeep2/2.0.0.5(default) openmpi https://hpc.nih.gov/development/MPI.html openmpi/1.8.6, openmpi/1.8.2(default), openmpi/2.1.2, openmpi/4.0.4, openmpi/4.0.5, openmpi/1.10.1, openmpi/1.10.2, openmpi/1.6.3-gnu, openmpi/4.1.2, openmpi/3.1.2, openmpi/4.1.1, openmpi/3.1.6, openmpi/3.1.6_slurm, openmpi/4.1.2_slurm, openmpi/4.1.4 hisat https://hpc.nih.gov/apps/hisat.html hisat/2.0.5, hisat/2.1.0 namd https://hpc.nih.gov/apps/NAMD.html namd/2014-07-10, namd/2.12-mpi, namd/2.8(default) openmm https://hpc.nih.gov/apps/charmm/older_charmm.html openmm/4.1.1(default), openmm/5.1 PEET https://hpc.nih.gov/apps/PEET.html PEET/1_13.0, PEET/1_12.0 NCAR https://hpc.nih.gov/docs/deep_learning.html NCAR/5.2.1 opencv https://hpc.nih.gov/apps/python.html opencv/3.1.0, opencv/4.5.1 IGV https://hpc.nih.gov/apps/IGV.html IGV/1.5.30 MCR https://hpc.nih.gov/apps/dynamo.html MCR/R2012b, MCR/R2012a MDAnalysis https://hpc.nih.gov/apps/mdtraj.html MDAnalysis/0.8.1 multinest https://hpc.nih.gov/policies/multinode.html multinest/201911 icc https://hpc.nih.gov/apps/R.html icc/10.1.017 jupyter https://hpc.nih.gov/apps/jupyter.html jupyter/3.0, jupyter/1.0(default) kallisto https://hpc.nih.gov/apps/kallisto.html kallisto/0.45.0 mothur https://hpc.nih.gov/apps/mothur.html mothur/1.25.1, mothur/1.29.1, mothur/1.36.1(default), mothur/1.48, mothur/1.44, mothur/1.47 graphviz https://hpc.nih.gov/apps/modules.html graphviz/2.40.1 HTSeq https://hpc.nih.gov/apps/htseq.html HTSeq/0.5.4p5(default), HTSeq/0.6.1p1 misopy https://hpc.nih.gov/apps/misopy.html misopy/0.5.2(default) mrbayes https://hpc.nih.gov/apps/mrbayes.html mrbayes/3.1.2 nccl https://hpc.nih.gov/docs/deeplearning/multinode_DL.html nccl/2.7.8-1 impute https://hpc.nih.gov/apps/IMPUTE.html impute/2.0.3 mach https://hpc.nih.gov/apps/mash.html mach/1.0.16 MACS https://hpc.nih.gov/apps/macs.html MACS/1.4.2-1 merlin https://hpc.nih.gov/apps/merlin.html merlin/1.1.2 mvapich2 https://hpc.nih.gov/development/MPI.html mvapich2/1.6-1 ont-guppy https://hpc.nih.gov/apps/guppy.html ont-guppy/3.3.0-gpu, ont-guppy/3.3.0-cpu, ont-guppy/4.5.2-cpu, ont-guppy/4.5.2-gpu, ont-guppy/5.0.11-gpu, ont-guppy/5.0.11-cpu gpaw-mpi https://hpc.nih.gov/development/MPI.html gpaw-mpi/0.4.2627 idl https://hpc.nih.gov/apps/idl.html idl/8.4 libffi https://hpc.nih.gov/apps/python.html libffi/3.2.1 maq https://hpc.nih.gov/docs/helixdrive.html maq/0.7.1 mathematica https://hpc.nih.gov/apps/mathematica.html mathematica/13.1, mathematica/13.2.1, mathematica/10.4.0, mathematica/10.4.1, mathematica/11.0.0, mathematica/11.3.0, mathematica/12.0, mathematica/13.0, mathematica/12.1(default), mathematica/12.2, mathematica/12.3 muscle https://hpc.nih.gov/apps/muscle.html muscle/3.8.31 gromacs-plumed-libmatheval https://hpc.nih.gov/apps/Gromacs.html gromacs-plumed-libmatheval/2014-06-17(default) MAnorm https://hpc.nih.gov/apps/manorm.html MAnorm/2014-04-03 multiqc https://hpc.nih.gov/apps/multiqc.html multiqc/1.7.0 paraview https://hpc.nih.gov/apps/paraview.html paraview/3.8.1, paraview/4.4 povray https://hpc.nih.gov/apps/POVRay/index.html povray/3.6.1 haploview https://hpc.nih.gov/apps/plink.html haploview/4.1 pcre https://hpc.nih.gov/apps/python.html pcre/8.38 mafft https://hpc.nih.gov/apps/mafft.html mafft/7.481 metaphlan https://hpc.nih.gov/apps/metaphlan.html metaphlan/2.6.0 nextflow https://hpc.nih.gov/apps/nextflow.html nextflow/21.10.1, nextflow/21.10.6 ORFfinder https://hpc.nih.gov/apps/ORFfinder.html ORFfinder/0.4.3 orthofinder https://hpc.nih.gov/apps/orthofinder.html orthofinder/2.5.4 libxml2 https://hpc.nih.gov/apps/python.html libxml2/2.9.10, libxml2/2.9.12 libxpm https://hpc.nih.gov/apps/python.html libxpm/3.5.12 mamba https://hpc.nih.gov/apps/python.html mamba/22.9.0-2 minimap2 https://hpc.nih.gov/apps/minimap2.html minimap2/2.15 orca https://hpc.nih.gov/apps/python.html orca/5.0.3, orca/5.0.4 structure-java https://hpc.nih.gov/development/java.html structure-java/2.3.4 salmon https://hpc.nih.gov/apps/salmon.html salmon/1.6.0, salmon/0.13.1 openssl https://hpc.nih.gov/apps/python.html openssl/1.1.1 ninja https://hpc.nih.gov/apps/python.html ninja/1.10.1 tcltk https://hpc.nih.gov/apps/R.html tcltk/08.5.11-rhel6, tcltk/8.5.11 rstudio https://hpc.nih.gov/apps/RStudio.html rstudio/0.96.331, rstudio/1.1.447, rstudio/1.1.383, rstudio/0.99.903 tophat https://hpc.nih.gov/apps/tophat.html tophat/1.0.14, tophat/2.0.10, tophat/2.0.9(default), tophat/2.0.13 picard https://hpc.nih.gov/apps/picard.html picard/1.139, picard/2.8.0 protobuf https://hpc.nih.gov/apps/python.html protobuf/3.18, protobuf/2.6 R https://hpc.nih.gov/apps/R.html abaqus/V6R2017, abaqus/V6R2018, NCAR/5.2.1, MCR/R2012b, mirPRo/1.1.4, MCR/R2012a, ORFfinder/0.4.3, sf/R_4.0.0, R/3.2.2, R/3.4.1, R/4.0.0, R/3.2.5, PyRosetta/r55194, R/3.6.3, RepeatMasker/4.0.7, STAR/2.7.0a, R/3.3.3, R/3.4, R/4.2.2, tecplot/2013R1, R/3.4_dulla, R/3.0.3, R/3.5.0, RetroSeq/20130716(default), STAR/2.6.1d, R/3.0.2, R/3.1.0, ViennaRNA/2.1.6(default), PyRosetta/r55193, R/3.0.1, R/3.3.2, R/3.4.3(default), R/4.1.1, STAR/2.3.0e, R/2.15.3, STAR/2.5.2b(default), RAxML/8.2.12, R/4.2.0 rosetta https://hpc.nih.gov/apps/rosetta.html rosetta/3.4, rosetta/3.13_mpi, rosetta/3.7, rosetta/3.13 usearch https://hpc.nih.gov/apps/usearch.html usearch/8.1.1861, usearch/10.0.240, usearch/7.0.1001(default) ncbi-magicblast https://hpc.nih.gov/apps/magicblast.html ncbi-magicblast/1.5.0 QIIME https://hpc.nih.gov/apps/QIIME.html QIIME/1.9.0(default), QIIME/1.7.0, QIIME/1.8.0, QIIME/1.5.0, QIIME/1.6.0 samtools https://hpc.nih.gov/apps/samtools.html samtools/0.1.18(default), samtools/0.1.19, samtools/1.9, samtools/1.2 plink https://hpc.nih.gov/apps/plink.html plink/1.90b5, plink2/2.0, plink/1.06, plinkseq/0.10 sra https://hpc.nih.gov/apps/sratoolkit.html sra/2.5.0, sra/2.10.8, sra/2.9.2 root https://hpc.nih.gov/apps/singularity.html root/6.22.08_pythia6, root/6.24.06, root/5.34.18, proot/5.1.0, root/6.22.08 python https://hpc.nih.gov/apps/python.html boost/1.63.0-python3, python/2.7.4, python/3.8.8, python/3.6.0, tensorflow/11-python3.5, python/2.7.6, python/3.5.0, tensorflow/11-python2.7, python/2.6.5, python/2.7.3(default) rsem https://hpc.nih.gov/apps/rsem.html rsem/1.3, rsem/1.3.3, rsem/1.2.14(default), rsem/1.3.1 singularity https://hpc.nih.gov/apps/singularity.html singularity/3.6.1, singularity/2.6.1(default), singularity/3.1.0, singularity/3.5.3, singularity/3.8.4 subread https://hpc.nih.gov/apps/subread.html subread/1.6.3, subread/1.5.1 vmd https://hpc.nih.gov/apps/VMD.html vmd/1.9.4, vmd/1.9 pytorchgpu https://hpc.nih.gov/docs/deep_learning.html pytorchgpu/1 perl https://hpc.nih.gov/apps/modules.html bioperl/1.7.6, perl/5.34.0, perl/5.34.0_gcc7.3.0, superlu/4.0, perl/5.32.1 PyRosetta https://hpc.nih.gov/apps/PyRosetta.html PyRosetta/r55194, PyRosetta/r55193 trinity https://hpc.nih.gov/apps/trinity.html trinity/10.15.15, trinity/7.17.14 plink2 https://hpc.nih.gov/apps/plink.html plink2/2.0 RepeatMasker https://hpc.nih.gov/apps/repeatmasker.html RepeatMasker/4.0.7 STAR https://hpc.nih.gov/apps/STAR.html STAR/2.7.0a, STAR/2.6.1d, STAR/2.3.0e, STAR/2.5.2b(default) rclone https://hpc.nih.gov/apps/rclone.html rclone/1.51.0, rclone/1.59.0 rsync https://hpc.nih.gov/docs/transfer.html rsync/3.2.2 pythia https://hpc.nih.gov/apps/python.html root/6.22.08_pythia6, pythia/8235, pythia/8219, pythia6/6.4.28-gcc-4.8.5, pythia6/6.4.28 trinotate https://hpc.nih.gov/apps/trinotate.html trinotate/2.0.2 stacks https://hpc.nih.gov/apps/R.html stacks/2.53, stacks/1.12(default) plinkseq https://hpc.nih.gov/apps/plinkseq.html plinkseq/0.10 spades https://hpc.nih.gov/apps/spades.html spades/3.12.0, spades/3.15.4 zlib https://hpc.nih.gov/apps/R.html zlib/1.2.8, zlib/1.2.11 phred https://hpc.nih.gov/apps/modphred.html phred/0.0 rnaseqmut https://hpc.nih.gov/apps/VIRTUS.html rnaseqmut/1.0 pandoc https://hpc.nih.gov/apps/python.html pandoc/2.14 valgrind https://hpc.nih.gov/development/debugging.html valgrind/3.16.1 pilon https://hpc.nih.gov/apps/pilon.html pilon/1.22 qiime2 https://hpc.nih.gov/apps/QIIME.html qiime2/2017.7, qiime2/2021.11, qiime2/2018.8 SAS https://hpc.nih.gov/apps/SAS.html SAS/9.3, SAS/9.4(default) solar https://hpc.nih.gov/apps/solar.html solar/4.2.7 squid https://hpc.nih.gov/docs/transfer.html squid/1.9g(default) relion https://hpc.nih.gov/apps/RELION/index.html relion/4.0 tabix https://hpc.nih.gov/apps/samtools.html tabix/20131216 proj4 https://hpc.nih.gov/apps/R.html proj4/4.4 sniffles https://hpc.nih.gov/apps/sniffles.html sniffles/1.0.10 SuiteSparse https://hpc.nih.gov/apps/python.html SuiteSparse/3.6.1 tensorflow https://hpc.nih.gov/docs/deep_learning.html tensorflow/11-python3.5, tensorflow/11-python2.7 vcftools https://hpc.nih.gov/apps/vcftools.html vcftools/0.1.13(default), vcftools/0.1.12b phenix https://hpc.nih.gov/apps/Phenix.html phenix/1.13 Qt https://hpc.nih.gov/apps/RStudio.html Qt/4.8.4 RetroSeq https://hpc.nih.gov/apps/VIRTUS.html RetroSeq/20130716(default) tinker https://hpc.nih.gov/apps/mdtraj.html tinker/7.1 trimmomatic https://hpc.nih.gov/apps/trimmomatic.html trimmomatic/0.38 tmhmm https://hpc.nih.gov/apps/trinotate.html tmhmm/2.0c vim https://hpc.nih.gov/apps/Editors.html vim/8.1 randfold https://hpc.nih.gov/apps/randfold.html randfold/2.0(default) spark https://hpc.nih.gov/apps/spark.html spark/2.0.1, spark/2.3.0(default) ViennaRNA https://hpc.nih.gov/apps/viennarna.html ViennaRNA/2.1.6(default) readline https://hpc.nih.gov/apps/python.html readline/8.1 proot https://hpc.nih.gov/docs/biowulf_tools.html proot/5.1.0 Schrodinger https://hpc.nih.gov/apps/schrodinger/index.html Schrodinger/2015-4, Schrodinger/2012 sqlite3 https://hpc.nih.gov/apps/sqlite.html sqlite3/3.33.0 trim-galore https://hpc.nih.gov/apps/trimgalore.html trim-galore/0.6.4_dev zmq https://hpc.nih.gov/apps/python.html zmq/4.8.1 RAxML https://hpc.nih.gov/apps/raxml.html RAxML/8.2.12 transdecoder https://hpc.nih.gov/apps/TransDecoder.html transdecoder/2.0"},{"location":"tools/api/","title":"API (Application Programming Interface)","text":""},{"location":"tools/api/#api-queries","title":"API Queries","text":"<p>An API, or Application Programming Interface, is a way of accessing data directly from a website. In this way we can pull data from a website without having to deal with parsing HTML content. An API request occurs between a client and a server:</p> <p> </p> Image by DATAQUEST <p>Essentiall, we (the client) reach out to the server and request data. In return we get the data and a response code telling us how the request went. Sometimes we don't get the data and the response code can give us a hint as to why:</p> <p> </p> Image by WhiteHat <p>Each website (with an available API) should have more specific documentation on these codes and how to structure your request. Here we are going to cover how to use the STRINGDB API using Python and R. </p>"},{"location":"tools/api/#api-request","title":"API Request","text":"RPython <ul> <li>We will first need to load the R packages necessary to handle API requests:</li> </ul> <pre><code>library(httr)\nlibrary(jsonlite)\n</code></pre> <ul> <li>Now we will need to take a look at the API documentation on the STRINGDB website. Typically, we have a base url that we pull from, and in this case is is:</li> </ul> <p>https://string-db.org/</p> <ul> <li>We then need to plug in the information we would like to pull by adding to the url. So to get an image of the  information we will add the following to the url:</li> </ul> <pre><code>api/json/interaction_partners?\n</code></pre> <ul> <li>To point to specific genes, say PTCH1, we will add the following:</li> </ul> <pre><code>identifiers=PTCH1\n</code></pre> <ul> <li>Now the full url will be:</li> </ul> <pre><code>https://string-db.org/api/json/interaction_partners?identifiers=PTCH1\n</code></pre> <ul> <li>We can now plug this url into the <code>GET</code> function!</li> </ul> <pre><code>res &lt;- GET(\"https://string-db.org/api/json/interaction_partners?identifiers=PTCH1\")\nres\n</code></pre> <p>output</p> <pre><code>Response [https://string-db.org/api/json/interaction_partners?identifiers=PTCH1]\n  Date: 2023-01-10 19:37\n  Status: 200\n  Content-Type: text/json; charset=utf-8\n  Size: 2.72 kB\n</code></pre> <ul> <li>Here we see that our request did come through. However, the data is in json format. We can convert this json data to tabular data with:</li> </ul> <pre><code>data = fromJSON(rawToChar(res$content))\nhead(data)\n</code></pre> <p>output</p> <pre><code>           stringId_A           stringId_B preferredName_A preferredName_B\n1 9606.ENSP00000332353 9606.ENSP00000295731           PTCH1             IHH\n2 9606.ENSP00000332353 9606.ENSP00000297261           PTCH1             SHH\n3 9606.ENSP00000332353 9606.ENSP00000266991           PTCH1             DHH\n4 9606.ENSP00000332353 9606.ENSP00000256442           PTCH1           CCNB1\n5 9606.ENSP00000332353 9606.ENSP00000249373           PTCH1             SMO\n6 9606.ENSP00000332353 9606.ENSP00000376458           PTCH1            CDON\n</code></pre> <ul> <li>Congratulations! You have pulled data using an API!</li> </ul>"},{"location":"tools/api/#references","title":"References","text":"<ol> <li>https://www.dataquest.io/blog/r-api-tutorial/</li> <li>https://www.dataquest.io/blog/python-api-tutorial/</li> <li>https://apidocs.whitehatsec.com/whs/docs/error-handling</li> </ol>"},{"location":"tools/containers/","title":"Containers","text":""},{"location":"tools/containers/#containers","title":"Containers","text":"<p>Containers are a way of sharing software across different systems. On the Tufts HPC Cluster we can use the Singularity module (now called Apptainer) to build containers on the cluster. </p>"},{"location":"tools/containers/#building-an-outside-container-on-the-cluster","title":"Building An Outside Container On The Cluster","text":"<ul> <li>To begin you will need to load the following modules:</li> </ul> <pre><code>module load singularity/3.6.1\nmodule load squashfs\n</code></pre> <ul> <li>Now, search docker hub for the tool of your choice. You will be using this image to build a singularity container or sif file. In this case we will be demonstrating how to download the biobakery workflows docker image:</li> </ul> <pre><code>singularity build bioBakery.sif docker://biobakery/workflows\n</code></pre>"},{"location":"tools/containers/#using-the-container","title":"Using The Container","text":"<ul> <li>To use this tool you will need to reference the sif file you created. So to run the humann3 command you would use the following:</li> </ul> <pre><code>singularity exec bioBakery.sif humann3 --help\n</code></pre>"},{"location":"tools/containers/#references","title":"References","text":"<ol> <li>CHPC - Research Computing and Data Support for the University - Singularity</li> </ol>"},{"location":"tools/dbgap/","title":"dbGAP Downloads","text":""},{"location":"tools/dbgap/#downloading-fastq-data-using-dbgap","title":"Downloading Fastq Data Using dbGAP","text":"<p>dbGAP is a repository of data assessing the connection between genotypes and phenotypes. Here we discuss how to access this data using the Tufts HPC.</p> <ol> <li>Obtain your dbGaP repository key by logging into dgGAP and clicking \"get dbGAP repository key\"</li> </ol> <p>dbGAP Download Guide</p> <p></p> <ol> <li>Now, navigate to the dbGAP SRA RUN Selector, login with your credentials, select the files you'd like to download, and click Accession List:</li> </ol> <p></p> <ol> <li> <p>Upload this ngc file and the accession list to the desired directory on the Tufts HPC cluster. For more information on how to login to the cluster visit: Navigate To The Cluster</p> </li> <li> <p>Now you will need to load the tools needed to download your data:</p> </li> </ol> <pre><code>module load sra/2.10.8\n</code></pre> <ol> <li>Now you will need to configure the sratoolkit:</li> </ol> <p><pre><code>vdb-config --interactive\n</code></pre> 5. Hit \"X\"</p> <ol> <li>Now set up the following batch script:</li> </ol> Using ParallelNot Using Parallel <p>dbGAP_download.sh</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=dbGap\n#SBATCH --time=07-00:00:00\n#SBATCH --partition=largemem\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=110Gb\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=Your.Email@tufts.edu\n\nmodule load sra/2.10.8 parallel\n\n# using parallel\nparallel --jobs 4 \"fastq-dump -X 9999999999999 --ngc /path/to/projectNgcFile.ngc --split-files --gzip {}\" &lt; /path/to/accessionList.txt\n</code></pre> <p>dbGAP_download.sh</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=dbGap\n#SBATCH --time=07-00:00:00\n#SBATCH --partition=largemem\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=110Gb\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=Your.Email@tufts.edu\n\nmodule load sra/2.10.8 # not using parallel\nfastq-dump -X 9999999999999 --ngc /path/to/projectNgcFile.ngc --gzip $(&lt;/path/to/accessionList.txt)\n</code></pre> <ol> <li>To run your script, enter the following:</li> </ol> <pre><code>sbatch dbGAP_download.sh\n</code></pre> <ol> <li>To check on the status of your job, enter the following:</li> </ol> <pre><code>squeue -u $USER\n</code></pre> <ol> <li>dbGAP repositories can contain a lot of data, so if you need your job extended reach out to tts-research@tufts.edu</li> </ol>"},{"location":"tools/dbgap/#downloading-other-dbgap-data","title":"Downloading Other dbGAP Data","text":"<ol> <li>Obtain your dbGaP repository key by logging into dgGAP and clicking \"get dbGAP repository key\"</li> </ol> <p>dbGAP Download Guide</p> <p></p> <ol> <li>Now, navigate to the dbGAP SRA RUN Selector, login with your credentials, select the files you'd like to download, and click Cart File:</li> </ol> <p></p> <ol> <li> <p>Upload this ngc file and the accession list to the desired directory on the Tufts HPC cluster. For more information on how to login to the cluster visit: Navigate To The Cluster</p> </li> <li> <p>Now you will need to load the tools needed to download your data:</p> </li> </ol> <pre><code>module load sra/2.10.8\n</code></pre> <ol> <li>Now you will need to configure the sratoolkit:</li> </ol> <p><pre><code>vdb-config --interactive\n</code></pre> 5. Hit \"X\"</p> <ol> <li>Now set up the following batch script:</li> </ol> <p>dbGAP_download.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=dbGap\n#SBATCH --time=07-00:00:00\n#SBATCH --partition=largemem\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=110Gb\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=Your.Email@tufts.edu\n\nmodule load sra/2.10.8\nprefetch -X 9999999999999 --ngc your_file.ngc cart_prj#####_###.krt\nvdb-decrypt --ngc your_file.ngc enc_file.xml\n</code></pre> </p> <p>Note</p> <p>Note that we add in the option <code>-X 9999999999999</code>. This allows for files larger than 20GB, and without this option larger files will not download.</p> <ol> <li>To run your script, enter the following:</li> </ol> <pre><code>sbatch dbGAP_download.sh\n</code></pre> <ol> <li>To check on the status of your job, enter the following:</li> </ol> <pre><code>squeue -u $USER\n</code></pre> <ol> <li>dbGAP repositories can contain a lot of data, so if you need your job extended reach out to tts-research@tufts.edu</li> </ol>"},{"location":"tools/dbgap/#references","title":"References","text":"<ol> <li>dbGAP Download Guide</li> </ol>"},{"location":"tools/github/","title":"GitHub","text":""},{"location":"tools/github/#introduction-to-github","title":"Introduction to GitHub","text":"<p>The power in GitHub lies in version control. Code is often changed and in doing so previous versions of files can easily be lost. GitHub saves changes to files so that one can go back and restore previous versions if needed. Additionally, there is a plethora of functionality to: update code collaboratively, publish static website pages, report issues with code, etc..</p>"},{"location":"tools/github/#creating-a-git-repository","title":"Creating a Git Repository","text":"<ul> <li>To create a Git Repository by:</li> </ul> <pre><code># change into your project directory\ncd /path/to/your/project\n\n# initialize the repository\ngit init\n</code></pre>"},{"location":"tools/github/#addcommit-files","title":"Add/Commit Files","text":"<ul> <li>To add files to be tracked:</li> </ul> <pre><code># add files to be tracked\ngit add main.py input.txt \n</code></pre> <ul> <li>To commit these files to your Git Repository:</li> </ul> <pre><code># commit the files to the repository, creating the first snapshot\ngit commit -m \"Initial Commit\"\n</code></pre>"},{"location":"tools/github/#configure-your-credentials","title":"Configure Your Credentials","text":"<ul> <li>To configure your credentials:</li> </ul> <pre><code># configure your user name/email\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"[email address]\"\n</code></pre>"},{"location":"tools/github/#push-to-github","title":"Push To GitHub","text":"<ul> <li>Currently, these files are not on github. To add them to Github:</li> </ul> <pre><code>## push files to github\ngit remote add origin git@github.com:user_name/my_new_repo.git\ngit push -u origin master\n</code></pre>"},{"location":"tools/github/#going-further","title":"Going Further","text":"<ul> <li>GitHub has much more functionality than what has been described here. To learn more, check out:</li> </ul> <p>GitHub Docs</p>"},{"location":"tools/github/#references","title":"References","text":"<ol> <li>GitHub Docs</li> </ol>"},{"location":"tools/nextflow/","title":"Nextflow","text":""},{"location":"tools/nextflow/#nextflow","title":"Nextflow","text":"<p>Nextflow is a type of workflow manager, designed to be portable and reproducible. Nextflow can be used on local, HPC schedulers, AWS Batch, Google Cloud Life Sciences, and Kubernetes. Nextflow can access tool dependencies through Conda, Spack, Docker, Podman, Singularity, Modules and more.</p>"},{"location":"tools/nextflow/#how-do-i-access-nextflow-on-the-cluster","title":"How Do I Access Nextflow on the Cluster?","text":"<p>Nextflow can be accessed on the Tufts HPC Cluster with the following modules:</p> <pre><code>module load gcc nextflow/21.10.1 java/15.0.2 anaconda/2021.11\n</code></pre> <p>You might also have noticed that we include <code>anaconda/2021.11</code> as well. This is because many nextflow pipelines you might import will use Anaconda for pipeline tool dependencies. </p>"},{"location":"tools/nextflow/#running-nf-core-pipelines","title":"Running nf-core Pipelines","text":"<p>Speaking of importing Nextflow pipelines, it is worth mentioning nf-core pipelines. These are community developed pipelines that are available for your use! There are a number of pipelines you can use:</p> <p>Nextflow nf-core Pipelines</p> <p>Danger</p> <p>When Nextflow is running tasks it will create many temp files within: the <code>./work</code> directory. Be very careful when running these pipelines and always be sure to clear out files in this folder to avoid running out of file storage. </p> <p>We will demonstrate how to run the RNA-seq pipeline:</p> <pre><code>nextflow run nf-core/rnaseq \\\n--input samplesheet.csv \\\n--outdir &lt;OUTDIR&gt; \\\n--genome GRCh37 \\\n-profile docker\n</code></pre> <p>Where your <code>samplesheet.csv</code> will include:</p> <p>samplesheet.csv</p> <pre><code>sample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\n</code></pre> <p>However, this is a multi-faceted pipeline, with many additional parameters you can tweak for your needs. To better understand those parameters, always check the <code>Usage docs</code> and <code>Parameters</code> sections of each pipeline's documentation.</p>"},{"location":"tools/nextflow/#building-your-own-nextflow-pipeline","title":"Building your own Nextflow Pipeline","text":""},{"location":"tools/nextflow/#references","title":"References","text":"<ol> <li>Nextflow Github</li> <li>RNA-Seq Nextflow</li> </ol>"},{"location":"tools/parallel/","title":"Introduction to Parallel","text":"<p>GNU Parallel is a shell tool that allows for independent jobs to be run in parallel over multiple compute resources. </p>"},{"location":"tools/parallel/#bash-loop-using-parallel","title":"Bash Loop Using Parallel","text":"<p>GNU Parallel can greatly speed up a task given that it can leverage multiple compute resources at once. Let's examine the case of the following bash loop (example taken from Yale Center For Research Computing - Parallel):</p> <pre><code>for letter in {a..f};\ndo\necho $letter\ndone\n</code></pre> <p>output</p> <pre><code>a\nb\nc\nd\ne\nf\n</code></pre> <p>To parallelize this task we can use the <code>parallel</code> module and ask for multiple CPUs per task:</p> <pre><code>salloc -c 4\nmodule load parallel\nparallel -j 4 \"echo {}\" ::: {a..f}\n</code></pre> <p>output</p> <pre><code>a\nb\nc\nd\ne\nf\n</code></pre>"},{"location":"tools/parallel/#parallel-in-a-bash-script","title":"Parallel In A Bash Script","text":"<p>Additionally, we can leverage the <code>parallel</code> module in a batch script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=runParallel\n#SBATCH --time=01-00:00:00\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=4G\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n\n# load modules\nmodule load parallel\nmodule load fastqc\n\n# make an output directory\nmkdir fastqc_output\n\n# find all fastq files and run fastqc them\nls *.fastq.gz | parallel -j ${SLURM_CPUS_PER_TASK} \"fastqc {} -o fastqc_output\"\n</code></pre> <p>Here we load the <code>parallel</code> and <code>fastqc</code> modules. We then create an output directory (<code>fastqc_output</code>). In our command we list all our fastq files (<code>ls *.fastq.gz</code>), then use the parallel command to run fastqc on each file (<code>fastqc {} -o fastqc_output</code>). We reference each fastq file with the curly brackets <code>{}</code>. You'll also notice that we specify how many compute resources are available with <code>-j ${SLURM_CPUS_PER_TASK}</code>.</p>"},{"location":"tools/parallel/#references","title":"References","text":"<ol> <li>https://www.gnu.org/software/parallel/</li> <li>https://docs.ycrc.yale.edu/clusters-at-yale/guides/parallel/</li> </ol>"},{"location":"tools/python-conda/","title":"Conda Environments","text":""},{"location":"tools/python-conda/#conda-on-the-hpc","title":"Conda on the HPC","text":"<p>It is often desirable to download your own python packages into a conda environment. We will quickly go through how to create one and add packages to that conda environment.</p> <ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>Start an interactive session on the Tufts HPC Cluster to work on a compute node. To learn more about how to set up an interactive session visit:</li> </ul> <p>Start an Interative Session</p> <ul> <li>Load relevant modules:</li> </ul> <pre><code>module load anaconda/2021.11\n</code></pre> <p>Info</p> <p>You may need to load other modules, such as <code>cuda</code> if you plan to utilize GPUs:</p> <pre><code>module load cuda/11.0\n</code></pre>"},{"location":"tools/python-conda/#create-your-conda-environment","title":"Create your conda environment","text":"<ul> <li>Now you can create your own conda env:</li> </ul> <pre><code>cd /cluster/tufts/XXXXlab/$USER/condaenv/\nconda create -p yourenvname\n</code></pre> <ul> <li>Or if you have a specific version of python you need to use, e.g. 3.8 (Recommended!):</li> </ul> <pre><code>conda create -p yourenvname python=3.8 \n</code></pre> <p>Note</p> <p>you will need to have python and pip installed inside the env to pip install packages inside the env.</p> <ul> <li>Activate the environment (needs to be executed whenever you need to use the conda env you have created)</li> </ul> <pre><code>source activate yourenvname\n</code></pre> <ul> <li>If you are using system installed conda, please DO NOT use conda activate to activate your environment Install yourpackage in the conda env</li> </ul> <pre><code>conda install yourpackage\n</code></pre> <ul> <li>Or if you have python (comes with pip) installed</li> </ul> <pre><code>pip install yourpackage\n</code></pre> <ul> <li>Or follow the instruction on package website. Check what's installed in your conda environment:</li> </ul> <pre><code>conda list\n</code></pre> <ul> <li>When you are done, deactivate the environment:</li> </ul> <pre><code>conda deactivate\n</code></pre>"},{"location":"tools/python-conda/#additional-information-for-jupyter-users-run-conda-env-as-a-kernel-in-jupyter","title":"Additional Information for Jupyter Users: Run conda env as a kernel in Jupyter","text":"<ul> <li>If you would like to use JupyterNotebook or JupyterLab from OnDemand, you can follow the instructions below and run your conda env as a kernel in Jupyter.</li> <li>Make sure with python 3.7+ and make sure you load cluster's anaconda module (this only works with py3.7+)</li> <li>Activate your conda env from terminal. Install ipykernel with:</li> </ul> <pre><code>pip install ipykernel \n</code></pre> <p>Note</p> <p>this assumes you installed python and pip in your env, otherwise, use \"--user\" flag</p> <ul> <li>Add your env to jupyter with:</li> </ul> <pre><code>python -m ipykernel install --user --name=myenvname \n</code></pre> <ul> <li>Restart Jupyter from OnDemand </li> </ul>"},{"location":"tools/python-jupyter/","title":"JupyterLab","text":"<p>Often times you like to test your python code using an interactive development environment or IDE. We offer the python IDE, JupyterLab, to do just that. Here's how to request a Jupiter lab session OnDemand:</p> <ul> <li>Go to:</li> </ul> <p>OnDemand</p> <ul> <li>Navigate to \"Interactive Apps\"</li> <li>Scroll down and click on \"JupyterLab\"</li> <li> <p>Select:</p> <ul> <li>how long of a session you would like</li> <li>the number of cores, the memory</li> <li>your python version</li> <li>and your reservation</li> </ul> </li> </ul> <p></p> <ul> <li>Click \"Launch\" </li> <li>Once your session is ready, click on the \"Connect to JupyterLab\"</li> </ul>"},{"location":"tools/python-modules/","title":"Python Interactive Session","text":""},{"location":"tools/python-modules/#python-interactive-session","title":"Python Interactive Session","text":"<ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>From the login node, load the python module </li> </ul> <pre><code>module load python/3.8.8\n</code></pre> <ul> <li>To check out different python modules enter the command:</li> </ul> <pre><code>module av python\n</code></pre> <ul> <li>Allocate computing resources. Start an interactive session with your desired number of cores and memory, here we are using 2 cores with 4GB of memory: </li> </ul> <pre><code>srun -p interactive -n 2 --mem=4g --pty bash\n</code></pre> <ul> <li> <p>The Interactive partition has a default 4-hour time limit </p> </li> <li> <p>For more information on how to allocate resources on Tufts HPC cluster, check out:</p> </li> </ul> <p>Compute Resources</p>"},{"location":"tools/r-batch/","title":"R Batch Jobs","text":""},{"location":"tools/r-batch/#r-batch-jobs","title":"R batch jobs","text":"<p>Sometimes an R script will take to long to either run via an interactive session or RStudio. In these cases we can submit the R script as a batch job.</p> <ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li> <p>Upload your R script to the HPC cluster</p> </li> <li> <p>Go to the directory/folder which contains your R script</p> </li> <li> <p>Open your favorite text editor and write a slurm submission script similar to the following one <code>batchjob.sh</code> (name your own)</p> </li> </ul> <p>batchjob.sh</p> <pre><code>#!/bin/bash\n#SBATCH -J myRjob  #job name\n#SBATCH --time=00-00:20:00 #requested time\n#SBATCH -p batch  #running on \"batch\" partition/queue\n#SBATCH -n 2  #2 cores total\n#SBATCH --mem=2g #requesting 2GB of RAM total\n#SBATCH --output=myRjob.%j.out #saving standard output to file\n#SBATCH --error=myRjob.%j.err  #saving standard error to file\n#SBATCH --mail-type=ALL  #email optitions\n#SBATCH --mail-user=Your_Tufts_Email @tufts.edu\nmodule load R/4.0.0\nRscript --no-save your_rscript_name.R\n</code></pre> <ul> <li>Submit it with: </li> </ul> <pre><code>sbatch batchjob.sh\n</code></pre> <ul> <li>If you are submitting multiple batch jobs to run the same script on different datasets, please make sure they are saving results to different files inside of your R script.</li> </ul>"},{"location":"tools/r-interactive/","title":"R Interactive Session","text":""},{"location":"tools/r-interactive/#r-interactive-session","title":"R Interactive Session","text":"<ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>From the login node, load R module and associated modules</li> </ul> <pre><code>module load R/4.0.0 boost/1.63.0-python3 java/1.8.0_60 gsl/2.6\n</code></pre> <ul> <li> <p>Additional modules may need to be loaded, such as <code>sf/R_4.0.0</code> </p> </li> <li> <p>Allocate computing resources. Start an interactive session with your desired number of cores and memory, here we are using 2 cores with 4GB of memory: </p> </li> </ul> <pre><code>srun -p interactive -n 2 --mem=4g --pty bash\n</code></pre> <ul> <li> <p>The Interactive partition has a default 4-hour time limit. </p> </li> <li> <p>For more information on how to allocate resources on Tufts HPC cluster, check out:</p> </li> </ul> <p>Compute Resources</p> <ul> <li>Within the interactive session, you can start R </li> </ul> <pre><code>R\n</code></pre>"},{"location":"tools/r-interactive/#installing-r-packages","title":"Installing R packages","text":"<ul> <li>In R, you can install the packages you need in your home directory with:</li> </ul> <pre><code>install.packages(\"XXX\")\n</code></pre> <ul> <li>You can also use the packages installed in HPC Tools R package repo:</li> </ul> <pre><code>LIB='/cluster/tufts/hpc/tools/R/4.0.0' .libPaths(c(\"\",LIB))\n</code></pre> <ul> <li>You can also use packages installed in BioTools R package repo:</li> </ul> <pre><code>LIB='/cluster/tufts/bio/tools/R_libs/4.0.0' .libPaths(c(\"\",LIB)) </code></pre> <ul> <li> <p>If you are having trouble installing the packages you need, please contact tts-research@tufts.edu.</p> </li> <li> <p>To exit from R command line interface:</p> </li> </ul> <p><pre><code>q()\n</code></pre> - To terminate interactive session </p> <pre><code>exit\n</code></pre>"},{"location":"tools/r-interactive/#r-package-installation-troubleshooting","title":"R Package Installation Troubleshooting","text":"<p>Suggestion 1: Try installing the R package from command line instead of RStudio OnDemand</p> <ul> <li>The RStudio OnDemand interface is not perfect and can store things like different libPaths between sessions. To be safe, it is always best to install new R package from the command line when on the Tufts HPC.</li> </ul> <p>Suggestion 2: You may need to load more modules</p> <ul> <li>On the Tufts HPC you load modules of software that might already be installed on your machine. This is why it can be easier to install R packages on your local machine rather than the Tufts HPC. If you are unsuccessful at installing an R package, try loading the following modules:</li> </ul> <pre><code>module load curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6 R/4.0.0\n</code></pre> <p>Suggestion 3: Take a look at the last few lines of the error message</p> <ul> <li>The error message will give you a clue as to what is going wrong. For example a common example:</li> </ul> <pre><code>Error in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : \n  there is no package called \u2018Rcpp\u2019\nError: package or namespace load failed for \u2018ggplot2\u2019\n</code></pre> <ul> <li>Here you might need to install a dependency beforehand with either:</li> </ul> <pre><code>install.packages(\"Rcpp\")\n</code></pre> <p>or:</p> <pre><code>install.packages(\"ggplot2\",dependencies = TRUE)\n</code></pre> <p>Suggestion 4: Install the package from the Repository</p> <ul> <li>Dependant on the R module you are loading, you may be working with an older version of a package manager, like BiocManager. As such some of the packages might have dependencies that are deprecated. For example, when installing <code>APAlyzer</code>:</li> </ul> <pre><code>ERROR: dependency \u2018DESeq\u2019 is not available for package \u2018APAlyzer\u2019\n* removing \u2018/cluster/home/user/R/x86_64-pc-linux-gnu-library/4.0/APAlyzer\u2019\n\nThe downloaded source packages are in\n    \u2018/tmp/Rtmpraf0Ix/downloaded_packages\u2019\nInstallation paths not writeable, unable to update packages\n  path: /opt/shared/R/4.0.0/lib64/R/library\n  packages:\n    boot, class, cluster, codetools, foreign, KernSmooth, lattice, MASS,\n    Matrix, mgcv, nlme, nnet, rpart, spatial, survival\nOld packages: 'openssl', 'Seurat'\nUpdate all/some/none? [a/s/n]: n\nWarning message:\nIn install.packages(...) :\n  installation of package \u2018APAlyzer\u2019 had non-zero exit status\n</code></pre> <ul> <li>We can try installing <code>APAlyzer</code> like so:</li> </ul> <pre><code>BiocManager::install('RJWANGbioinfo/APAlyzer')\n</code></pre> <p>Suggestion 5: Try updating the packages it asks to update</p> <ul> <li>R usually tries to tells you what it needs to proceed. Oftentimes when you install a package, you will be prompted to update the packages you have:</li> </ul> <pre><code>These packages have more recent versions available.\nIt is recommended to update all of them.\nWhich would you like to update?\n\n1: All                                           2: CRAN packages only                            3: None                                          4: sitmo        (2.0.1      -&gt; 2.0.2     ) [CRAN]\n5: BH           (1.75.0-0   -&gt; 1.81.0-1  ) [CRAN]\n6: dqrng        (0.2.1      -&gt; 0.3.0     ) [CRAN]\n7: irlba        (2.3.3      -&gt; 2.3.5.1   ) [CRAN]\n...\n...\n...\nEnter one or more numbers, or an empty line to skip updates: 1\n</code></pre> <ul> <li>Some packages require specific/updated versions of the packages you have. Package installation issues can be circumvented when you enter <code>1</code> to update all packkages. </li> </ul> <p>Suggestion 6: Restarting R</p> <ul> <li>The old IT addage of turning it on and off again is not just all talk. The way you set your libPath or the packages you already have loaded may interrupt your ability to install packages. You can restart R to wipe the proverbial slate clean by going to <code>Session &gt; Restart R</code>. Now try to install your package! </li> </ul>"},{"location":"tools/r-rstudio/","title":"RStudio OnDemand","text":""},{"location":"tools/r-rstudio/#rstudio-interactive-app-on-ondemand","title":"RStudio Interactive App on OnDemand","text":"<ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>Go to \"Interactive Apps\" tab </li> <li>Select \"RStudio\"</li> <li> <p>Select the:</p> <ul> <li>the time needed on the app</li> <li>number of cores </li> <li>CPU memory you need</li> <li>the version of R you wish to run. </li> <li> <p>the modules needed for your pacakges to run </p> <ul> <li>typically the following are sufficient: </li> </ul> <pre><code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6\n</code></pre> </li> </ul> </li> </ul> <p>Note</p> <ul> <li>Each user can only start one OnDemand RStudio session on one compute node at a time. If you need to start multiple RStudio sessions, please make sure you select a different nodename from your current running session. </li> </ul> <p></p> <ul> <li>Click \"Launch\"</li> <li>Click on \"Connect to RStudio\"</li> <li> <p>When you are finished:</p> <ul> <li>exit RStudio properly <code>q()</code></li> <li>close the RStudio tab</li> <li>Go back to the main OnDemand page </li> <li>Click \"Delete\" to end the session</li> </ul> </li> </ul>"},{"location":"tools/rclone/","title":"Rclone","text":""},{"location":"tools/rclone/#rclone","title":"Rclone","text":"<p>Rclone is a tool to manage files stored on cloud storage. Some examples of this are Box, Google Drive, Amazon, etc.. To connect Rclone to the cluster,  you will need to install it on your local machine to set up your access token and then configure your Rclone on the cluster using this token.</p>"},{"location":"tools/rclone/#rclone-installation","title":"Rclone Installation","text":"<ul> <li>To set up Rclone on your local machine follow the instructions for your machine type:</li> </ul> <p>Rclone Installation</p>"},{"location":"tools/rclone/#rclone-configuration","title":"Rclone Configuration","text":"<ul> <li>On your local machine navigate to your terminal/command prompt and enter the following command to authorize the connection to the clound storage of your choice. Here we will demonstrate how to connect to Box:</li> </ul> <pre><code>rclone authorize \"box\"\n</code></pre> <ul> <li>Now the following text will pop up along with a web browser with instructions to enter your credentials. Follow the instructions and take the SECRET_TOKEN they provide and paste it into your terminal/command prompt whent prompeted:</li> </ul> <pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth\nLog in and authorize rclone for access\nWaiting for code...\nGot code\nPaste the following into your remote machine ---&gt;\nSECRET_TOKEN\n&lt;---End paste\n</code></pre> <ul> <li>Now on navigate to the cluster (if you do not know how check out these instructions) and enter the following command:</li> </ul> <pre><code>module load rclone\n</code></pre> <ul> <li>Now we will start creating a remote file:</li> </ul> <pre><code>rclone config\n</code></pre> <ul> <li>You will be prompted to create a remote file:</li> </ul> <pre><code>No remotes found, make a new one?\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\nname&gt; remote\nType of storage to configure.\nChoose a number from below, or type in your own value\n[snip]\nXX / Box\n   \\ \"box\"\n[snip]\nStorage&gt; box\nBox App Client Id - leave blank normally.\nclient_id&gt; \nBox App Client Secret - leave blank normally.\nclient_secret&gt;\nBox App config.json location\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nbox_config_file&gt;\nBox App Primary Access Token\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\naccess_token&gt;\n\nEnter a string value. Press Enter for the default (\"user\").\nChoose a number from below, or type in your own value\n 1 / Rclone should act on behalf of a user\n   \\ \"user\"\n 2 / Rclone should act on behalf of a service account\n   \\ \"enterprise\"\nbox_sub_type&gt;\nRemote config\nUse web browser to automatically authenticate rclone with remote?\n * Say Y if the machine running rclone has a web browser you can use\n * Say N if running rclone on a (remote) machine without web browser access\nIf not sure try Y. If Y failed, try N.\ny) Yes\nn) No\ny/n&gt; n\nFor this to work, you will need rclone available on a machine that has\na web browser available.\n\nFor more help and alternate methods see: https://rclone.org/remote_setup/\n\nExecute the following on the machine with the web browser (same rclone\nversion recommended):\n\n    rclone authorize \"box\"\n\nThen paste the result below:\nresult&gt;\n</code></pre> <ul> <li>Now, in the result field enter the SECRET_TOKEN you got on your local machine:</li> </ul> <pre><code>result&gt; SECRET_TOKEN\n--------------------\n[acd12]\nclient_id = \nclient_secret = \ntoken = SECRET_TOKEN\n--------------------\ny) Yes this is OK\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt;\n</code></pre> <p>Success</p> <p>Congrats! You have Rclone configured on the Tufts HPC Cluster!</p>"},{"location":"tools/rclone/#rclone-cluster-use","title":"Rclone Cluster Use","text":"<ul> <li>To examine files in your remote storage you can use the following command (assuming the name of your remote is <code>box</code>):</li> </ul> <pre><code>rclone ls box:/\n</code></pre> <ul> <li>To copy those files to your location on the cluster you can use (assuming the name of your remote is <code>box</code>):</li> </ul> <pre><code># to download a file to the cluster\nrclone copy box:/path/to/file .\n\n# to upload a file from the cluster to the cloud storage\nrclone copy filename box:/path/to/desitination/\n</code></pre> <ul> <li>To learn more about the different types of Rclone commands use the help option:</li> </ul> <pre><code>rclone help\n</code></pre>"},{"location":"tools/rclone/#references","title":"References","text":"<ol> <li>Rclone</li> </ol>"},{"location":"tools/vdi/","title":"Tufts VDI","text":""},{"location":"tools/vdi/#tufts-virtual-lab-vdi-desktop-client","title":"Tufts Virtual Lab (VDI) Desktop Client","text":"<p>Aside from the Tufts HPC, Tufts maintains a Virtual Lab (VDI) Desktop Client with a number of software, tools, and shared data already configured. </p>"},{"location":"tools/vdi/#vdi-installation","title":"VDI Installation","text":"WindowsMac <ol> <li>Navigate to https://vdi.it.tufts.edu </li> <li>Click Install VMWare Horizon Client.</li> <li>From the list of clients, locate the VMware Horizon Client for Windows, select the appropriate row for your system, then, click the Go to Downloads link on the far right to download the client.</li> <li>Start the installation.</li> <li>Once completed, restart your computer.</li> <li>Open the VMWare client.</li> <li>The only customization required for install is the selection of IPv4 as the IP protocol; leave everything else as the default.</li> <li>Click the plus icon.</li> <li>Input vdi.it.tufts.edu as the server to connect to.</li> <li>Input your Tufts Username and Tufts Password to sign in. Make sure the Domain says TUFTS.</li> <li>Open the available desktop or application.</li> </ol> <ol> <li>Navigate to https://vdi.it.tufts.edu</li> <li>Click \"Click Here to Download VMWare Horizon Client\"</li> <li>Click Here is at the bottom of the screen</li> <li>You will be taken to the VMWare Horizon website with a list of clients, locate the correct row for you (VMware Horizon Client for Mac or VMware Horizon Client for Linux); then, click the Go to Downloads link on the far right to download the client.<ul> <li>Tip: If you have a Linux machine, it is most likely 64-bit. If you'd like to check, follow these \"Check if your linux system is 32bit or 64bit\" instructions.</li> </ul> </li> <li>Start the installation.</li> <li>Once completed, locate your new application and open.</li> <li>Click the Add Server + icon.</li> <li>The plus is under new server</li> <li>Input vdi.it.tufts.edu as the Connection Server.</li> <li>Connection Server is the only field in the popup box</li> <li>Click Connect.</li> <li>A Tufts login screen may open in a browser window. Authenticate with your Tufts username and password.</li> <li>In the VDI panel, input your Tufts Username and Tufts Password to sign in. Make sure the Domain says TUFTS.</li> <li>Login is underneath the login fields</li> <li>Click Login.</li> </ol>"},{"location":"tools/vdi/#connecting-your-local-machine-to","title":"Connecting your local Machine to","text":"WindowsMac <ol> <li>Click on the settings icon at the top of the window:</li> <li>Click \u201cDrive and Network Sharing\u201d </li> <li>Select the local drive you\u2019d like to add</li> <li>Now click \u201cTTS Virtual Lab\u201c to start your session</li> <li>To find your local machine, go to \u201cThis PC\u201c &gt; \u201cNetwork Drive \u201c</li> </ol> <ol> <li>Click \u201cVMware Horizon Client\u201d at the top of your MAC, Then click \u201cPreferences\u201d</li> <li>Click \u201cDrive Sharing\u201d</li> <li>Select the local drive you\u2019d like to add</li> <li>Now click \u201cTTS Virtual Lab\u201c to start your session</li> <li>To find your local machine, go to \u201cThis PC\u201c &gt; \u201cNetwork Drive \u201c</li> </ol>"},{"location":"tools/vdi/#software-on-the-vdi","title":"Software on the VDI","text":"<p>The following software are on the Tufts VDI:</p> <ul> <li>7-zip</li> <li>Adobe Flash Player</li> <li>Adobe Acrobat Reader</li> <li>ArcGIS</li> <li>Audacity</li> <li>Box Edit and Box for Office</li> <li>Brainstorm</li> <li>CAST</li> <li>Cisco Jabber</li> <li>DNRGPS</li> <li>Endnote x8</li> <li>ENVI</li> <li>EPI Data and EPI Info</li> <li>FME Desktop</li> <li>GDAL</li> <li>GeoDa</li> <li>JASP</li> <li>JMP</li> <li>KNIME   </li> <li>Mathematica</li> <li>Matlab R2017a</li> <li>Mega</li> <li>Mendeley Desktop</li> <li>NotePad ++</li> <li>PyCharm Edu</li> <li>Python</li> <li>QGIS</li> <li>R and Rstudio</li> <li>SAS</li> <li>SNAP</li> <li>Skype</li> <li>SPSS</li> <li>STATA</li> <li>Tableau Desktop (Lab license)</li> <li>VLC Media Player</li> <li>VMware Tools</li> <li>WebEx</li> <li>WinSCP</li> <li>Wolfram Extras</li> <li>Write-N-Cite</li> <li>Zotero</li> </ul>"}]}